<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink 1.1-SNAPSHOT Documentation: Amazon Web Services (AWS)</title>
    <link rel="shortcut icon" href="/page/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/page/favicon.ico" type="image/x-icon">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
    <link rel="stylesheet" href="/page/css/flink.css">
    <link rel="stylesheet" href="/page/css/syntax.css">
    <link rel="stylesheet" href="/page/css/codetabs.css">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    
    





    <!-- Top navbar. -->
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <!-- The logo. -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <div class="navbar-logo">
            <a href="http://flink.apache.org"><img alt="Apache Flink" src="/page/img/navbar-brand-logo.jpg"></a>
          </div>
        </div><!-- /.navbar-header -->

        <!-- The navigation links. -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav">
            <li class="hidden-sm "><a href="/">Docs v1.1</a></li>

            <li class=""><a href="/concepts/concepts.html">Concepts</a></li>

            <!-- Setup -->
            <li class="dropdown active">
              <a href="/setup" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Setup <span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                
                
                <li class=""><a href="/setup/building.html">Build Flink from Source</a></li>
                
                <li class=""><a href="/setup/config.html">Configuration</a></li>
                

                <li class="divider"></li>
                <li role="presentation" class="dropdown-header"><strong>Quickstart</strong></li>

                <!-- Quickstart -->
                
                
                <li class=""><a href="/quickstart/setup_quickstart.html">Setup & Run Example</a></li>
                
                <li class=""><a href="/quickstart/java_api_quickstart.html">Java API</a></li>
                
                <li class=""><a href="/quickstart/scala_api_quickstart.html">Scala API</a></li>
                

                <li class="divider"></li>
                <li role="presentation" class="dropdown-header"><strong>Deployment</strong></li>
                
                
                <li class=""><a href="/setup/local_setup.html">Local</a></li>
                
                <li class=""><a href="/setup/cluster_setup.html">Cluster (Standalone)</a></li>
                
                <li class=""><a href="/setup/yarn_setup.html">YARN</a></li>
                
                <li class=""><a href="/setup/gce_setup.html">Google Compute Engine</a></li>
                
                <li class="active"><a href="/setup/aws.html">AWS</a></li>
                
                <li class=""><a href="/setup/jobmanager_high_availability.html">High Availability</a></li>
                
              </ul>
            </li>

            <!-- Programming Guides -->
            <li class="dropdown">
              <a href="/apis" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Programming Guides <span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                
                
                <li class=""><a href="/apis/common/index.html"><strong>Basic API Concepts</strong></a></li>
                
                <li class=""><a href="/apis/streaming/index.html"><strong>Streaming Guide</strong> (DataStream API)</a></li>
                
                <li class=""><a href="/apis/batch/index.html"><strong>Batch Guide</strong> (DataSet API)</a></li>
                
                <li class=""><a href="/apis/best_practices.html">Best Practices</a></li>
                
                <li class=""><a href="/apis/table.html">Table API and SQL</a></li>
                
                <li class=""><a href="/apis/cli.html">Command-Line Interface</a></li>
                
                <li class=""><a href="/apis/local_execution.html">Local Execution</a></li>
                
                <li class=""><a href="/apis/cluster_execution.html">Cluster Execution</a></li>
                
                <li class=""><a href="/apis/scala_shell.html">Scala Shell</a></li>
                
                <li class=""><a href="/apis/scala_api_extensions.html">Scala API Extensions</a></li>
                
                <li class=""><a href="/apis/java8.html">Java 8</a></li>
                
              </ul>
            </li>

            <!-- Libraries -->
            <li class="dropdown">
              <a href="/libs" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Libraries <span class="caret"></span></a>
                <ul class="dropdown-menu" role="menu">
                  
                  
                  <li class=""><a href="/apis/batch/libs/gelly.html">Graphs: Gelly</a></li>
                  
                  <li class=""><a href="/apis/streaming/libs/cep.html">CEP</a></li>
                  
                  <li class=""><a href="/apis/batch/libs/ml/index.html">Machine Learning</a></li>
                  
              </ul>
            </li>

            <!-- Internals -->
            <li class="dropdown">
              <a href="/internals" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Internals <span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li role="presentation" class="dropdown-header"><strong>Contribute</strong></li>
                <li><a href="http://flink.apache.org/how-to-contribute.html"><small><span class="glyphicon glyphicon-new-window"></span></small> How to Contribute</a></li>
                <li><a href="http://flink.apache.org/contribute-code.html#coding-guidelines"><small><span class="glyphicon glyphicon-new-window"></span></small> Coding Guidelines</a></li>
                
                
                <li class=""><a href="/internals/ide_setup.html">IDE Setup</a></li>
                
                <li class=""><a href="/internals/logging.html">Logging</a></li>
                
                <li class=""><a href="/internals/general_arch.html">Architecture and Process Model</a></li>
                
                <li class=""><a href="/internals/stream_checkpointing.html">Fault Tolerance for Data Streaming</a></li>
                
                <li class=""><a href="/internals/types_serialization.html">Type Extraction and Serialization</a></li>
                
                <li class=""><a href="/internals/monitoring_rest_api.html">Monitoring REST API</a></li>
                
                <li class=""><a href="/internals/job_scheduling.html">Jobs and Scheduling</a></li>
                
                <li class=""><a href="/internals/add_operator.html">How-To: Add an Operator</a></li>
                
                <li class=""><a href="/internals/back_pressure_monitoring.html">Back Pressure Monitoring</a></li>
                
              </ul>
            </li>
          </ul>
          <form class="navbar-form navbar-right hidden-sm hidden-md" role="search" action="/search-results.html">
            <div class="form-group">
              <input type="text" class="form-control" size="16px" name="q" placeholder="Search all pages">
            </div>
            <button type="submit" class="btn btn-default">Search</button>
          </form>
        </div><!-- /.navbar-collapse -->
      </div><!-- /.container -->
    </nav>


    

    <!-- Main content. -->
    <div class="container">
      
      
<div class="row">

  
  <div class="col-md-8 col-md-offset-2 text">
    <!-- Artifact name change warning. Remove for the 1.0 release. -->
    <div class="panel panel-default">
      <div class="panel-body"><strong>Important</strong>: Maven artifacts which depend on Scala are now suffixed with the Scala major version, e.g. "2.10" or "2.11". Please consult the <a href="https://cwiki.apache.org/confluence/display/FLINK/Maven+artifact+names+suffixed+with+Scala+version">migration guide on the project Wiki</a>.</div>
    </div>

    <h1>Amazon Web Services (AWS)</h1>


<p>Amazon Web Services offers cloud computing services on which you can run Flink.</p>

<ul id="markdown-toc">
  <li><a href="#emr-elastic-mapreduce" id="markdown-toc-emr-elastic-mapreduce">EMR: Elastic MapReduce</a>    <ul>
      <li><a href="#create-emr-cluster" id="markdown-toc-create-emr-cluster">Create EMR Cluster</a></li>
      <li><a href="#install-flink-on-emr-cluster" id="markdown-toc-install-flink-on-emr-cluster">Install Flink on EMR Cluster</a></li>
    </ul>
  </li>
  <li><a href="#s3-simple-storage-service" id="markdown-toc-s3-simple-storage-service">S3: Simple Storage Service</a>    <ul>
      <li><a href="#set-s3-filesystem" id="markdown-toc-set-s3-filesystem">Set S3 FileSystem</a></li>
      <li><a href="#configure-access-credentials" id="markdown-toc-configure-access-credentials">Configure Access Credentials</a></li>
      <li><a href="#provide-s3-filesystem-dependency" id="markdown-toc-provide-s3-filesystem-dependency">Provide S3 FileSystem Dependency</a></li>
    </ul>
  </li>
  <li><a href="#common-issues" id="markdown-toc-common-issues">Common Issues</a>    <ul>
      <li><a href="#missing-s3-filesystem-configuration" id="markdown-toc-missing-s3-filesystem-configuration">Missing S3 FileSystem Configuration</a></li>
      <li><a href="#aws-access-key-id-and-secret-access-key-not-specified" id="markdown-toc-aws-access-key-id-and-secret-access-key-not-specified">AWS Access Key ID and Secret Access Key Not Specified</a></li>
      <li><a href="#classnotfoundexception-natives3filesystems3afilesystem-not-found" id="markdown-toc-classnotfoundexception-natives3filesystems3afilesystem-not-found">ClassNotFoundException: NativeS3FileSystem/S3AFileSystem Not Found</a></li>
      <li><a href="#ioexception-400-bad-request" id="markdown-toc-ioexception-400-bad-request">IOException: <code>400: Bad Request</code></a></li>
      <li><a href="#nullpointerexception-at-orgapachehadoopfslocaldirallocator" id="markdown-toc-nullpointerexception-at-orgapachehadoopfslocaldirallocator">NullPointerException at org.apache.hadoop.fs.LocalDirAllocator</a></li>
    </ul>
  </li>
</ul>

<h2 id="emr-elastic-mapreduce">EMR: Elastic MapReduce</h2>

<p><a href="https://aws.amazon.com/elasticmapreduce/">Amazon Elastic MapReduce</a> (Amazon EMR) is a web service that makes it easy to  quickly setup a Hadoop cluster. This is the <strong>recommended way</strong> to run Flink on AWS as it takes care of setting up everything.</p>

<h3 id="create-emr-cluster">Create EMR Cluster</h3>

<p>The EMR documentation contains <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-gs-launch-sample-cluster.html">examples showing how to start an EMR cluster</a>. You can follow that guide and install any EMR release. You don’t need to install <em>All Applications</em> part of the EMR release, but can stick to <em>Core Hadoop</em>:</p>

<p><img src="fig/flink-on-emr.png" class="img-responsive" /></p>

<p>When creating your cluster, make sure to setup <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-iam-roles.html">IAM roles</a> allowing you to access your S3 buckets if required.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="install-flink-on-emr-cluster">Install Flink on EMR Cluster</h3>

<p>After creating your cluster, you can <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-connect-master-node.html">connect to the master node</a> and install Flink:</p>

<ol>
  <li>Go the the <a href="">Downloads Page</a> and <strong>download a binary version of Flink matching the Hadoop version</strong> of your EMR cluster, e.g. Hadoop 2.7 for EMR releases 4.3.0, 4.4.0, or 4.5.0.</li>
  <li>Extract the Flink distribution and you are ready to deploy <a href="/setup/yarn_setup.html">Flink jobs via YARN</a> after <strong>setting the Hadoop config directory</strong>:</li>
</ol>

<div class="highlight"><pre><code class="language-bash"><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span>/etc/hadoop/conf bin/flink run -m yarn-cluster examples/streaming/WordCount.jar</code></pre></div>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h2 id="s3-simple-storage-service">S3: Simple Storage Service</h2>

<p><a href="http://aws.amazon.com/s3/">Amazon Simple Storage Service</a> (Amazon S3) provides cloud object storage for a variety of use cases. You can use S3 with Flink for <strong>reading</strong> and <strong>writing data</strong> as well in conjunction with the <a href="/apis/streaming/state_backends.html">streaming <strong>state backends</strong></a>.</p>

<p>You can use S3 objects like regular files by specifying paths in the following format:</p>

<div class="highlight"><pre><code>s3://&lt;your-bucket&gt;/&lt;endpoint&gt;
</code></pre></div>

<p>The endpoint can either be a single file or a directory, for example:</p>

<div class="highlight"><pre><code class="language-java"><span class="c1">// Read from S3 bucket</span>
<span class="n">env</span><span class="o">.</span><span class="na">readTextFile</span><span class="o">(</span><span class="s">&quot;s3://&lt;bucket&gt;/&lt;endpoint&gt;&quot;</span><span class="o">);</span>

<span class="c1">// Write to S3 bucket</span>
<span class="n">stream</span><span class="o">.</span><span class="na">writeAsText</span><span class="o">(</span><span class="s">&quot;s3://&lt;bucket&gt;/&lt;endpoint&gt;&quot;</span><span class="o">);</span>

<span class="c1">// Use S3 as FsStatebackend</span>
<span class="n">env</span><span class="o">.</span><span class="na">setStateBackend</span><span class="o">(</span><span class="k">new</span> <span class="nf">FsStateBackend</span><span class="o">(</span><span class="s">&quot;s3://&lt;your-bucket&gt;/&lt;endpoint&gt;&quot;</span><span class="o">));</span></code></pre></div>

<p>Note that these examples are <em>not</em> exhaustive and you can use S3 in other places as well, including your <a href="/setup/jobmanager_high_availability.html">high availability setup</a> or the <a href="/apis/streaming/state_backends.html#the-rocksdbstatebackend">RocksDBStateBackend</a>;  everywhere that Flink expects a FileSystem URI.</p>

<h3 id="set-s3-filesystem">Set S3 FileSystem</h3>

<div class="panel panel-default"><div class="panel-body"><strong>Note:</strong> You don’t have to configure this manually if you are running <a href="#emr-elastic-mapreduce">Flink on EMR</a>.</div></div>

<p>S3 is treated by Flink as a regular FileSystem. Interaction with S3 happens via a Hadoop <a href="https://wiki.apache.org/hadoop/AmazonS3">S3 FileSystem client</a>.</p>

<p>There are two popular S3 file system implementations available:</p>

<ol>
  <li><code>S3AFileSystem</code> (<strong>recommended</strong>): file system for reading and writing regular files using Amazon’s SDK internally. No maximum file size and works with IAM roles.</li>
  <li><code>NativeS3FileSystem</code>: file system for reading and writing regular files. Maximum object size is 5GB and does not work with IAM roles.</li>
</ol>

<h4 id="s3afilesystem-recommended"><code>S3AFileSystem</code> (Recommended)</h4>

<p>This is the recommended S3 FileSystem implementation to use. It uses Amazon’s SDK internally and works with IAM roles (see <a href="#configure-access-credentials">Configure Access Credential</a>).</p>

<p>You need to point Flink to a valid Hadoop configuration, which contains the following properties in <code>core-site.xml</code>:</p>

<div class="highlight"><pre><code class="language-xml"><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>fs.s3.impl<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>org.apache.hadoop.fs.s3a.S3AFileSystem<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="c">&lt;!-- Comma separated list of local directories used to buffer</span>
<span class="c">     large results prior to transmitting them to S3. --&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>fs.s3.buffer.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/tmp<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span></code></pre></div>

<p>This registers <code>S3AFileSystem</code> as the default FileSystem for URIs with the <code>s3://</code> scheme.</p>

<h4 id="natives3filesystem"><code>NativeS3FileSystem</code></h4>

<p>This file system is limited to files up to 5GB in size and it does not work IAM roles (see <a href="#configure-access-credentials">Configure Access Credential</a>), meaning that you have to manually configure your AWS credentials in the Hadoop config file.</p>

<p>You need to point Flink to a valid Hadoop configuration, which contains the following property in <code>core-site.xml</code>:</p>

<div class="highlight"><pre><code class="language-xml"><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>fs.s3.impl<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>org.apache.hadoop.fs.s3native.NativeS3FileSystem<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span></code></pre></div>

<p>This registers <code>NativeS3FileSystem</code> as the default FileSystem for URIs with the <code>s3://</code> scheme.</p>

<h4 id="hadoop-configuration">Hadoop Configuration</h4>

<p>You can specify the <a href="/setup/config.html#hdfs">Hadoop configuration</a> in various ways, for examples by configuring the path to the Hadoop configuration directory in <code>flink-conf.yaml</code>:</p>

<div class="highlight"><pre><code>fs.hdfs.hadoopconf: /path/to/etc/hadoop
</code></pre></div>

<p>This registers <code>path/to/etc/hadoop</code> as Hadoop’s configuration directory with Flink.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="configure-access-credentials">Configure Access Credentials</h3>

<div class="panel panel-default"><div class="panel-body"><strong>Note:</strong> You don’t have to configure this manually if you are running <a href="#emr-elastic-mapreduce">Flink on EMR</a>.</div></div>

<p>After setting up the S3 FileSystem, you need to make sure that Flink is allowed to access your S3 buckets.</p>

<h4 id="identity-and-access-management-iam-recommended">Identity and Access Management (IAM) (Recommended)</h4>

<p>The recommended way of setting up credentials on AWS is via <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">Identity and Access Management (IAM)</a>. You can use IAM features to securely give Flink instances the credentials that they need in order to access S3 buckets. Details about how to do this are beyond the scope of this documentation. Please refer to the AWS user guide. What you are looking for are <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html">IAM Roles</a>.</p>

<p>If you set this up correctly, you can manage access to S3 within AWS and don’t need to distribute any access keys to Flink.</p>

<p>Note that this only works with <code>S3AFileSystem</code> and not <code>NativeS3FileSystem</code>.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h4 id="access-keys-discouraged">Access Keys (Discouraged)</h4>

<p>Access to S3 can be granted via your <strong>access and secret key pair</strong>. Please note that this is discouraged since the <a href="https://blogs.aws.amazon.com/security/post/Tx1XG3FX6VMU6O5/A-safer-way-to-distribute-AWS-credentials-to-EC2">introduction of IAM roles</a>.</p>

<p>You need to configure both <code>fs.s3.awsAccessKeyId</code> and <code>fs.s3.awsSecretAccessKey</code>  in Hadoop’s  <code>core-site.xml</code>:</p>

<div class="highlight"><pre><code class="language-xml"><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>fs.s3.awsAccessKeyId<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>fs.s3.awsSecretAccessKey<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span></code></pre></div>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="provide-s3-filesystem-dependency">Provide S3 FileSystem Dependency</h3>

<div class="panel panel-default"><div class="panel-body"><strong>Note:</strong> You don’t have to configure this manually if you are running <a href="#emr-elastic-mapreduce">Flink on EMR</a>.</div></div>

<p>Hadoop’s S3 FileSystem clients are packaged in the <code>hadoop-aws</code>. This JAR and all its dependencies need to be added to Flink’s classpath, i.e. the class path of both Job and TaskManagers. Depending on which FileSystem implementation and which Flink and Hadoop version you use, you need to provide different dependencies (see below).</p>

<p>There are multiple ways of adding JARs to Flink’s class path, the easiest being simply to drop the JARs in Flink’s <code>/lib</code> folder. You need to copy the <code>hadoop-aws</code> JAR with all its dependencies. You can also export the directory containing these JARs as part of the <code>HADOOP_CLASSPATH</code> environment variable on all machines.</p>

<h4 id="flink-for-hadoop-27">Flink for Hadoop 2.7</h4>

<p>Depending on which file system you use, please add the following dependencies. You can find these as part of the Hadoop binaries in <code>hadoop-2.7/share/hadoop/tools/lib</code>:</p>

<ul>
  <li><code>S3AFileSystem</code>:
    <ul>
      <li><code>hadoop-aws-2.7.2.jar</code></li>
      <li><code>aws-java-sdk-1.7.4.jar</code></li>
      <li><code>httpcore-4.2.5.jar</code></li>
      <li><code>httpclient-4.2.5.jar</code></li>
    </ul>
  </li>
  <li><code>NativeS3FileSystem</code>:
    <ul>
      <li><code>hadoop-aws-2.7.2.jar</code></li>
      <li><code>guava-11.0.2.jar</code></li>
    </ul>
  </li>
</ul>

<p>Note that <code>hadoop-common</code> is available as part of Flink, but Guava is shaded by Flink.</p>

<h4 id="flink-for-hadoop-26">Flink for Hadoop 2.6</h4>

<p>Depending on which file system you use, please add the following dependencies. You can find these as part of the Hadoop binaries in <code>hadoop-2.6/share/hadoop/tools/lib</code>:</p>

<ul>
  <li><code>S3AFileSystem</code>:
    <ul>
      <li><code>hadoop-aws-2.6.4.jar</code></li>
      <li><code>aws-java-sdk-1.7.4.jar</code></li>
      <li><code>httpcore-4.2.5.jar</code></li>
      <li><code>httpclient-4.2.5.jar</code></li>
    </ul>
  </li>
  <li><code>NativeS3FileSystem</code>:
    <ul>
      <li><code>hadoop-aws-2.6.4.jar</code></li>
      <li><code>guava-11.0.2.jar</code></li>
    </ul>
  </li>
</ul>

<p>Note that <code>hadoop-common</code> is available as part of Flink, but Guava is shaded by Flink.</p>

<h4 id="flink-for-hadoop-24-and-earlier">Flink for Hadoop 2.4 and earlier</h4>

<p>These Hadoop versions only have support for <code>NativeS3FileSystem</code>. This comes pre-packaged with Flink for Hadoop 2 as part of <code>hadoop-common</code>. You don’t need to add anything to the classpath.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h2 id="common-issues">Common Issues</h2>

<p>The following sections lists common issues when working with Flink on AWS.</p>

<h3 id="missing-s3-filesystem-configuration">Missing S3 FileSystem Configuration</h3>

<p>If your job submission fails with an Exception message noting that <code>No file system found with scheme s3</code> this means that no FileSystem has been configured for S3. Please check out the <a href="#set-s3-filesystem">FileSystem Configuration section</a> for details on how to configure this properly.</p>

<div class="highlight"><pre><code>org.apache.flink.client.program.ProgramInvocationException: The program execution failed:
  Failed to submit job cd927567a81b62d7da4c18eaa91c3c39 (WordCount Example) [...]
Caused by: org.apache.flink.runtime.JobException: Creating the input splits caused an error:
  No file system found with scheme s3, referenced in file URI 's3://&lt;bucket&gt;/&lt;endpoint&gt;'. [...]
Caused by: java.io.IOException: No file system found with scheme s3,
  referenced in file URI 's3://&lt;bucket&gt;/&lt;endpoint&gt;'.
    at o.a.f.core.fs.FileSystem.get(FileSystem.java:296)
    at o.a.f.core.fs.Path.getFileSystem(Path.java:311)
    at o.a.f.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:450)
    at o.a.f.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:57)
    at o.a.f.runtime.executiongraph.ExecutionJobVertex.&lt;init&gt;(ExecutionJobVertex.java:156)
</code></pre></div>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="aws-access-key-id-and-secret-access-key-not-specified">AWS Access Key ID and Secret Access Key Not Specified</h3>

<p>If you see your job failing with an Exception noting that the <code>AWS Access Key ID and Secret Access Key must be specified as the username or password</code>, your access credentials have not been set up properly. Please refer to the <a href="#configure-access-credentials">access credential section</a> for details on how to configure this.</p>

<div class="highlight"><pre><code>org.apache.flink.client.program.ProgramInvocationException: The program execution failed:
  Failed to submit job cd927567a81b62d7da4c18eaa91c3c39 (WordCount Example) [...]
Caused by: java.io.IOException: The given file URI (s3://&lt;bucket&gt;/&lt;endpoint&gt;) points to the
  HDFS NameNode at &lt;bucket&gt;, but the File System could not be initialized with that address:
  AWS Access Key ID and Secret Access Key must be specified as the username or password
  (respectively) of a s3n URL, or by setting the fs.s3n.awsAccessKeyId
  or fs.s3n.awsSecretAccessKey properties (respectively) [...]
Caused by: java.lang.IllegalArgumentException: AWS Access Key ID and Secret Access Key must
  be specified as the username or password (respectively) of a s3 URL, or by setting
  the fs.s3n.awsAccessKeyId or fs.s3n.awsSecretAccessKey properties (respectively) [...]
    at o.a.h.fs.s3.S3Credentials.initialize(S3Credentials.java:70)
    at o.a.h.fs.s3native.Jets3tNativeFileSystemStore.initialize(Jets3tNativeFileSystemStore.java:80)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at o.a.h.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
    at o.a.h.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
    at o.a.h.fs.s3native.$Proxy6.initialize(Unknown Source)
    at o.a.h.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:330)
    at o.a.f.runtime.fs.hdfs.HadoopFileSystem.initialize(HadoopFileSystem.java:321)
</code></pre></div>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="classnotfoundexception-natives3filesystems3afilesystem-not-found">ClassNotFoundException: NativeS3FileSystem/S3AFileSystem Not Found</h3>

<p>If you see this Exception, the S3 FileSystem is not part of the class path of Flink. Please refer to <a href="#provide-s3-filesystem-dependency">S3 FileSystem dependency section</a> for details on how to configure this properly.</p>

<div class="highlight"><pre><code>Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3native.NativeS3FileSystem not found
  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2186)
  at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.getHadoopWrapperClassNameForFileSystem(HadoopFileSystem.java:460)
  at org.apache.flink.core.fs.FileSystem.getHadoopWrapperClassNameForFileSystem(FileSystem.java:352)
  at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:280)
  at org.apache.flink.core.fs.Path.getFileSystem(Path.java:311)
  at org.apache.flink.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:450)
  at org.apache.flink.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:57)
  at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.&lt;init&gt;(ExecutionJobVertex.java:156)
  ... 25 more
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3native.NativeS3FileSystem not found
  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2154)
  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2178)
  ... 32 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3native.NativeS3FileSystem not found
  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2060)
  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2152)
  ... 33 more
</code></pre></div>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="ioexception-400-bad-request">IOException: <code>400: Bad Request</code></h3>

<p>If you you have configured everything properly, but get a <code>Bad Request</code> Exception <strong>and</strong> your S3 bucket is located in region <code>eu-central-1</code>, you might be running an S3 client, which does not support <a href="http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html">Amazon’s signature version 4</a>.</p>

<p>Currently, this includes all Hadoop versions up to 2.7.2 running <code>NativeS3FileSystem</code>, which depend on <code>JetS3t 0.9.0</code> instead of a version <a href="http://www.jets3t.org/RELEASE_NOTES.html">&gt;= 0.9.4</a>.</p>

<p>The only workaround is to change the bucket region.</p>

<div class="highlight"><pre><code>[...]
Caused by: java.io.IOException: s3://&lt;bucket-in-eu-central-1&gt;/&lt;endpoint&gt; : 400 : Bad Request [...]
Caused by: org.jets3t.service.impl.rest.HttpException [...]
</code></pre></div>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="nullpointerexception-at-orgapachehadoopfslocaldirallocator">NullPointerException at org.apache.hadoop.fs.LocalDirAllocator</h3>

<p>This Exception is usually caused by skipping the local buffer directory configuration <code>fs.s3.buffer.dir</code> for the <code>S3AFileSystem</code>. Please refer to the <a href="#s3afilesystem-recommended">S3AFileSystem configuration</a> section to see how to configure the <code>S3AFileSystem</code> properly.</p>

<div class="highlight"><pre><code>[...]
Caused by: java.lang.NullPointerException at
o.a.h.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:268) at
o.a.h.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:344) at
o.a.h.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:416) at
o.a.h.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:198) at
o.a.h.fs.s3a.S3AOutputStream.&lt;init&gt;(S3AOutputStream.java:87) at
o.a.h.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:410) at
o.a.h.fs.FileSystem.create(FileSystem.java:907) at
o.a.h.fs.FileSystem.create(FileSystem.java:888) at
o.a.h.fs.FileSystem.create(FileSystem.java:785) at
o.a.f.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:404) at
o.a.f.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:48) at
... 25 more
</code></pre></div>

  </div>

  
</div>

    </div><!-- /.container -->

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.1.0/anchor.min.js"></script>
    <script src="/page/js/flink.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- Disqus -->
    
  </body>
</html>
