<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink 1.12 Documentation: Apache Kafka 连接器</title>
    <link rel="shortcut icon" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/favicon.ico" type="image/x-icon">
    <link rel="icon" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="//ci.apache.org/projects/flink/flink-docs-stable/zh/dev/connectors/kafka.html">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/flink.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/syntax.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/codetabs.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/font-awesome/css/font-awesome.min.css">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    

    <!-- Main content. -->
    <div class="container">
      
      <div class="row">
        <div class="col-lg-3" id="sidenavcol">
          <div class="sidenav-logo">
  <p><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/"><img class="bottom" alt="Apache Flink" src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/img/navbar-brand-logo.jpg"></a> v1.12</p>
</div>
<ul id="sidenav">
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/"><i class="fa fa-home title" aria-hidden="true"></i> Home</a></li><hr class="section-break"></hr>
<li><a href="#collapse-2" data-toggle="collapse"><i class="fa fa-rocket title appetizer" aria-hidden="true"></i> Try Flink<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-2"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/local_installation.html">本地模式安装</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/datastream_api.html">基于 DataStream API 实现欺诈检测</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/table_api.html">基于 Table API 实现实时报表</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/flink-operations-playground.html">Flink 操作场景</a></li>
</ul></div></li>
<li><a href="#collapse-8" data-toggle="collapse"><i class="fa fa-hand-paper-o title appetizer" aria-hidden="true"></i> 实践练习<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-8"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/datastream_api.html">DataStream API 简介</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/etl.html">数据管道 & ETL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/streaming_analytics.html">流式分析</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/event_driven.html">事件驱动应用</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/fault_tolerance.html">容错处理</a></li>
</ul></div></li>
<li><a href="#collapse-15" data-toggle="collapse"><i class="fa fa-map-o title appetizer" aria-hidden="true"></i> 概念透析<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-15"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/index.html">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/stateful-stream-processing.html">有状态流处理</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/timely-stream-processing.html">及时流处理</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/flink-architecture.html">Flink 架构</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/glossary.html">词汇表</a></li>
</ul></div></li><hr class="section-break"></hr>
<li><a href="#collapse-21" data-toggle="collapse"><i class="fa fa-code title maindish" aria-hidden="true"></i> 应用开发<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-21"><ul>
<li><a href="#collapse-22" data-toggle="collapse">DataStream API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-22"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/datastream_api.html">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/datastream_execution_mode.html">Execution Mode (Batch/Streaming)</a></li>
<li><a href="#collapse-24" data-toggle="collapse">事件时间<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-24"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_time.html">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamps_watermarks.html">生成 Watermark</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamp_extractors.html">内置 Watermark 生成器</a></li>
</ul></div></li>
<li><a href="#collapse-28" data-toggle="collapse">状态与容错<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-28"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/state.html">Working with State</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/broadcast_state.html">Broadcast State 模式</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/checkpointing.html">Checkpointing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/queryable_state.html">Queryable State</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/state_backends.html">State Backends</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/schema_evolution.html">状态数据结构升级</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/custom_serialization.html">自定义状态序列化</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/user_defined_functions.html">用户自定义 Functions</a></li>
<li><a href="#collapse-38" data-toggle="collapse">算子<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-38"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/windows.html">窗口</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/joining.html">Joining</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/process_function.html">Process Function</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/asyncio.html">异步 I/O</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/sources.html">Data Sources</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/side_output.html">旁路输出</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/application_parameters.html">Handling Application Parameters</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/testing.html">测试</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/experimental.html">实验功能</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/scala_api_extensions.html">Scala API Extensions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/java_lambdas.html">Java Lambda 表达式</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/project-configuration.html">Project Configuration</a></li>
</ul></div></li>
<li><a href="#collapse-53" data-toggle="collapse">DataSet API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-53"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/dataset_transformations.html">Transformations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/iterations.html">迭代</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/zip_elements_guide.html">Zipping Elements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/hadoop_compatibility.html">Hadoop 兼容</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/local_execution.html">本地执行</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/cluster_execution.html">集群执行</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/examples.html">Batch 示例</a></li>
</ul></div></li>
<li><a href="#collapse-62" data-toggle="collapse">Table API & SQL<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-62"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/common.html">概念与通用 API</a></li>
<li><a href="#collapse-64" data-toggle="collapse">流式概念<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-64"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/dynamic_tables.html">动态表 (Dynamic Table)</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/time_attributes.html">时间属性</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/joins.html">流上的 Join</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/temporal_tables.html">时态表（Temporal Tables）</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/match_recognize.html">模式检测</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/query_configuration.html">Query Configuration</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/types.html">数据类型</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/tableApi.html">Table API</a></li>
<li><a href="#collapse-74" data-toggle="collapse">SQL<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-74"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/queries.html">查询语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/create.html">CREATE 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/drop.html">DROP 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/alter.html">ALTER 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/insert.html">INSERT 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/hints.html">SQL Hints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/describe.html">DESCRIBE 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/explain.html">EXPLAIN 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/use.html">USE 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/show.html">SHOW 语句</a></li>
</ul></div></li>
<li><a href="#collapse-86" data-toggle="collapse">函数<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-86"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/systemFunctions.html">系统（内置）函数</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/udfs.html">自定义函数</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/modules.html">模块</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/catalogs.html">Catalogs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sqlClient.html">SQL 客户端</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/config.html">配置</a></li>
<li><a href="#collapse-94" data-toggle="collapse">Performance Tuning<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-94"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/tuning/streaming_aggregation_optimization.html">流式聚合</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sourceSinks.html">User-defined Sources & Sinks</a></li>
</ul></div></li>
<li><a href="#collapse-99" data-toggle="collapse">Python API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-99"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/installation.html">环境安装</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table_api_tutorial.html">Table API 教程</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/datastream_tutorial.html">DataStream API 教程</a></li>
<li><a href="#collapse-103" data-toggle="collapse">Table API用户指南<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-103"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/intro_to_table_api.html">Python Table API 简介</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/table_environment.html">TableEnvironment</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/operations.html">Operations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/python_types.html">数据类型</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/built_in_functions.html">系统（内置）函数</a></li>
<li><a href="#collapse-109" data-toggle="collapse">自定义函数<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-109"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/udfs/python_udfs.html">普通自定义函数（UDF）</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/udfs/vectorized_python_udfs.html">向量化自定义函数</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/conversion_of_pandas.html">PyFlink Table 和 Pandas DataFrame 互转</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/dependency_management.html">依赖管理</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/sql.html">SQL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/catalogs.html">Catalogs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/metrics.html">指标</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/python_table_api_connectors.html">连接器</a></li>
</ul></div></li>
<li><a href="#collapse-120" data-toggle="collapse">DataStream API用户指南<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-120"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/datastream-api-users-guide/data_types.html">数据类型</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/datastream-api-users-guide/operators.html">算子</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/datastream-api-users-guide/dependency_management.html">依赖管理</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/python_config.html">配置</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/environment_variables.html">环境变量</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/faq.html">常见问题</a></li>
</ul></div></li>
<li><a href="#collapse-129" data-toggle="collapse">数据类型以及序列化<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-129"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/types_serialization.html">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/custom_serializers.html">自定义序列化器</a></li>
</ul></div></li>
<li><a href="#collapse-132" data-toggle="collapse">管理执行<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-132"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/execution_configuration.html">执行配置</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/packaging.html">程序打包</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/parallel.html">并行执行</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/execution_plans.html">执行计划</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/task_failure_recovery.html">Task 故障恢复</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/migration.html">API 迁移指南</a></li>
</ul></div></li>
<li><a href="#collapse-141" data-toggle="collapse"><i class="fa fa-book title maindish" aria-hidden="true"></i> Libraries<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-141"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/cep.html">事件处理 (CEP)</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/state_processor_api.html">State Processor API</a></li>
<li><a href="#collapse-144" data-toggle="collapse">图计算: Gelly<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-144"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/graph_api.html">Graph API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/iterative_graph_processing.html">Iterative Graph Processing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/library_methods.html">Library Methods</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/graph_algorithms.html">Graph Algorithms</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/graph_generators.html">Graph Generators</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/bipartite_graph.html">Bipartite Graph</a></li>
</ul></div></li>
</ul></div></li>
<li><a href="#collapse-153" data-toggle="collapse"class="active"><i class="fa fa-random title maindish" aria-hidden="true"></i> Connectors</a><div class="collapse in" id="collapse-153"><ul>
<li><a href="#collapse-154" data-toggle="collapse"class="active">DataStream Connectors</a><div class="collapse in" id="collapse-154"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/guarantees.html">容错保证</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html" class="active">Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/cassandra.html">Cassandra</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kinesis.html">Kinesis</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/elasticsearch.html">Elasticsearch</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/file_sink.html">File Sink</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/streamfile_sink.html">Streaming File Sink</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/rabbitmq.html">RabbitMQ</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/nifi.html">NiFi</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/pubsub.html">Google Cloud PubSub</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/twitter.html">Twitter</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/jdbc.html">JDBC</a></li>
</ul></div></li>
<li><a href="#collapse-168" data-toggle="collapse">Table & SQL Connectors<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-168"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/">概览</a></li>
<li><a href="#collapse-169" data-toggle="collapse">Formats<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-169"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/csv.html">CSV</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/json.html">JSON</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/avro-confluent.html">Confluent Avro</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/avro.html">Avro</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/debezium.html">Debezium</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/canal.html">Canal</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/maxwell.html">Maxwell</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/parquet.html">Parquet</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/orc.html">Orc</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/raw.html">Raw</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/kafka.html">Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/upsert-kafka.html">Upsert Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/kinesis.html">Kinesis</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/jdbc.html">JDBC</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/elasticsearch.html">Elasticsearch</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/filesystem.html">FileSystem</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hbase.html">HBase</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/datagen.html">DataGen</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/print.html">Print</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/blackhole.html">BlackHole</a></li>
<li><a href="#collapse-191" data-toggle="collapse">Hive<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-191"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/hive_catalog.html">Hive Catalog</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/hive_dialect.html">Hive 方言</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/hive_read_write.html">Hive Read & Write</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/hive_functions.html">Hive 函数</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/downloads.html">下载</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/connectors.html">DataSet Connectors</a></li>
</ul></div></li>
<li><a href="#collapse-201" data-toggle="collapse"><i class="fa fa-sliders title maindish" aria-hidden="true"></i> Deployment<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-201"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/">概览</a></li>
<li><a href="#collapse-202" data-toggle="collapse">Resource Providers<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-202"><ul>
<li><a href="#collapse-203" data-toggle="collapse">Standalone<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-203"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/standalone/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/standalone/local.html">本地集群</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/standalone/docker.html">Docker</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/standalone/kubernetes.html">Kubernetes</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/native_kubernetes.html">Native Kubernetes</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/yarn.html">YARN</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/mesos.html">Mesos</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/config.html">配置参数</a></li>
<li><a href="#collapse-213" data-toggle="collapse">内存配置<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-213"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_setup.html">配置 Flink 进程的内存</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_setup_tm.html">配置 TaskManager 内存</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_setup_jobmanager.html">配置 JobManager 内存</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_tuning.html">调优指南</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_trouble.html">常见问题</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_migration.html">升级指南</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/cli.html">Command-Line Interface</a></li>
<li><a href="#collapse-222" data-toggle="collapse">文件系统<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-222"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/common.html">通用配置</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/s3.html">Amazon S3</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/oss.html">阿里云 OSS</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/azure.html">Azure Blob 存储</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/plugins.html">Plugins</a></li>
</ul></div></li>
<li><a href="#collapse-229" data-toggle="collapse">High Availability (HA)<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-229"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/ha/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/ha/zookeeper_ha.html">ZooKeeper HA Services</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/ha/kubernetes_ha.html">Kubernetes HA Services</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/metric_reporters.html">Metric Reporters</a></li>
<li><a href="#collapse-234" data-toggle="collapse">Security<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-234"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/security/security-ssl.html">SSL 设置</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/security/security-kerberos.html">Kerberos</a></li>
</ul></div></li>
<li><a href="#collapse-238" data-toggle="collapse">REPLs<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-238"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/repls/python_shell.html">Python REPL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/repls/scala_shell.html">Scala REPL</a></li>
</ul></div></li>
<li><a href="#collapse-242" data-toggle="collapse">Advanced<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-242"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/advanced/external_resources.html">扩展资源</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/advanced/historyserver.html">History Server</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/advanced/logging.html">日志</a></li>
</ul></div></li>
</ul></div></li>
<li><a href="#collapse-248" data-toggle="collapse"><i class="fa fa-cogs title maindish" aria-hidden="true"></i> Operations<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-248"><ul>
<li><a href="#collapse-249" data-toggle="collapse">状态与容错<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-249"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/state/checkpoints.html">Checkpoints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/state/savepoints.html">Savepoints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/state/state_backends.html">State Backends</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/state/large_state_tuning.html">大状态与 Checkpoint 调优</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/metrics.html">指标</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/rest_api.html">REST API</a></li>
<li><a href="#collapse-257" data-toggle="collapse">Debugging<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-257"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/debugging/debugging_event_time.html">调试窗口与事件时间</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/debugging/debugging_classloading.html">调试类加载</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/debugging/application_profiling.html">应用程序分析</a></li>
</ul></div></li>
<li><a href="#collapse-262" data-toggle="collapse">Monitoring<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-262"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/monitoring/checkpoint_monitoring.html">监控 Checkpoint</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/monitoring/back_pressure.html">监控反压</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/upgrading.html">升级应用程序和 Flink 版本</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/production_ready.html">生产就绪情况核对清单</a></li>
</ul></div></li><hr class="section-break"></hr>
<li><a href="#collapse-269" data-toggle="collapse"><i class="fa fa-cogs title dessert" aria-hidden="true"></i> Flink 开发<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-269"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/flinkDev/ide_setup.html">导入 Flink 到 IDE 中</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/flinkDev/building.html">从源码构建 Flink</a></li>
</ul></div></li>
<li><a href="#collapse-273" data-toggle="collapse"><i class="fa fa-book title dessert" aria-hidden="true"></i> 内幕<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-273"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/internals/job_scheduling.html">作业调度</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/internals/task_lifecycle.html">Task 生命周期</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/internals/filesystems.html">文件系统</a></li>
</ul></div></li>
  <li class="divider"></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java"><i class="fa fa-external-link title" aria-hidden="true"></i> Javadocs</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/scala/index.html#org.apache.flink.api.scala.package"><i class="fa fa-external-link title" aria-hidden="true"></i> Scaladocs</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/python"><i class="fa fa-external-link title" aria-hidden="true"></i> Pythondocs</a></li>
  <li><a href="http://flink.apache.org"><i class="fa fa-external-link title" aria-hidden="true"></i> Project Page</a></li>
</ul>

<div class="sidenav-search-box">
  <form class="navbar-form" role="search" action="//ci.apache.org/projects/flink/flink-docs-release-1.12/search-results.html">
    <div class="form-group">
      <input type="text" class="form-control" size="16px" name="q" placeholder="Search">
    </div>
    <button type="submit" class="btn btn-default">Go</button>
  </form>
</div>

<div class="sidenav-versions">
  <div class="dropdown">
    <button class="btn btn-default dropdown-toggle" type="button" data-toggle="dropdown">选择文档版本<span class="caret"></span></button>
    <ul class="dropdown-menu">
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.11">v1.11</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.10">v1.10</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.9">v1.9</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.8">v1.8</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.7">v1.7</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.6">v1.6</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.5">v1.5</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.4">v1.4</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.3">v1.3</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.2">v1.2</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.1">v1.1</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.0">v1.0</a></li>
    </ul>
  </div>
</div>

<div class="sidenav-languages"><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html">
      <button type="submit" class="btn btn-default">English</button>
    </a>
</div>

        </div>
        <div class="col-lg-9 content" id="contentcol">

          

<ol class="breadcrumb">
  
    <li><i class="fa fa-random title maindish" aria-hidden="true"></i> Connectors</li>
  
    <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/">DataStream Connectors</a></li>
  
    <li class="active">Kafka</li>
</ol>

<h1>Apache Kafka 连接器</h1>




<p>Flink 提供了 <a href="https://kafka.apache.org">Apache Kafka</a> 连接器，用于从 Kafka topic 中读取或者向其中写入数据，可提供精确一次的处理语义。</p>

<ul id="markdown-toc">
  <li><a href="#依赖" id="markdown-toc-依赖">依赖</a></li>
  <li><a href="#kafka-consumer" id="markdown-toc-kafka-consumer">Kafka Consumer</a>    <ul>
      <li><a href="#deserializationschema" id="markdown-toc-deserializationschema"><code class="highlighter-rouge">DeserializationSchema</code></a></li>
      <li><a href="#配置-kafka-consumer-开始消费的位置" id="markdown-toc-配置-kafka-consumer-开始消费的位置">配置 Kafka Consumer 开始消费的位置</a></li>
      <li><a href="#kafka-consumer-和容错" id="markdown-toc-kafka-consumer-和容错">Kafka Consumer 和容错</a></li>
      <li><a href="#kafka-consumer-topic-和分区发现" id="markdown-toc-kafka-consumer-topic-和分区发现">Kafka Consumer Topic 和分区发现</a></li>
      <li><a href="#kafka-consumer-提交-offset-的行为配置" id="markdown-toc-kafka-consumer-提交-offset-的行为配置">Kafka Consumer 提交 Offset 的行为配置</a></li>
      <li><a href="#kafka-consumer-和-时间戳抽取以及-watermark-发送" id="markdown-toc-kafka-consumer-和-时间戳抽取以及-watermark-发送">Kafka Consumer 和 时间戳抽取以及 watermark 发送</a></li>
    </ul>
  </li>
  <li><a href="#kafka-producer" id="markdown-toc-kafka-producer">Kafka Producer</a></li>
  <li><a href="#serializationschema" id="markdown-toc-serializationschema"><code class="highlighter-rouge">SerializationSchema</code></a>    <ul>
      <li><a href="#kafka-producer-和容错" id="markdown-toc-kafka-producer-和容错">Kafka Producer 和容错</a></li>
    </ul>
  </li>
  <li><a href="#kafka-连接器指标" id="markdown-toc-kafka-连接器指标">Kafka 连接器指标</a></li>
  <li><a href="#启用-kerberos-身份验证" id="markdown-toc-启用-kerberos-身份验证">启用 Kerberos 身份验证</a></li>
  <li><a href="#升级到最近的连接器版本" id="markdown-toc-升级到最近的连接器版本">升级到最近的连接器版本</a></li>
  <li><a href="#问题排查" id="markdown-toc-问题排查">问题排查</a>    <ul>
      <li><a href="#数据丢失" id="markdown-toc-数据丢失">数据丢失</a></li>
      <li><a href="#unknowntopicorpartitionexception" id="markdown-toc-unknowntopicorpartitionexception">UnknownTopicOrPartitionException</a></li>
    </ul>
  </li>
</ul>

<p><a name="dependency"></a></p>

<h2 id="依赖">依赖</h2>

<p>Apache Flink 集成了通用的 Kafka 连接器，它会尽力与 Kafka client 的最新版本保持同步。该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。
当前 Kafka client 向后兼容 0.10.0 或更高版本的 Kafka broker。
有关 Kafka 兼容性的更多细节，请参考  <a href="https://kafka.apache.org/protocol.html#protocol_compatibility">Kafka 官方文档</a>。</p>

<div class="codetabs">
  <div data-lang="universal">

    <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>flink-connector-kafka_2.11<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

  </div>
</div>

<p>Flink 目前的流连接器还不是二进制发行版的一部分。
<a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/project-configuration.html">在此处</a>可以了解到如何链接它们，从而在集群中运行。</p>

<p><a name="kafka-consumer"></a></p>

<h2 id="kafka-consumer">Kafka Consumer</h2>

<p>Flink 的 Kafka consumer 称为 <code class="highlighter-rouge">FlinkKafkaConsumer</code>。它提供对一个或多个 Kafka topics 的访问。</p>

<p>构造函数接受以下参数：</p>

<ol>
  <li>Topic 名称或者名称列表</li>
  <li>用于反序列化 Kafka 数据的 DeserializationSchema 或者 KafkaDeserializationSchema</li>
  <li>Kafka 消费者的属性。需要以下属性：
    <ul>
      <li>“bootstrap.servers”（以逗号分隔的 Kafka broker 列表）</li>
      <li>“group.id” 消费组 ID</li>
    </ul>
  </li>
</ol>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">);</span>
<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span>
    <span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">));</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">stream</span> <span class="k">=</span> <span class="n">env</span>
    <span class="o">.</span><span class="py">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">))</span></code></pre></figure>

  </div>
</div>

<p><a name="the-deserializationschema"></a></p>

<h3 id="deserializationschema"><code class="highlighter-rouge">DeserializationSchema</code></h3>

<p>Flink Kafka Consumer 需要知道如何将 Kafka 中的二进制数据转换为 Java 或者 Scala 对象。<code class="highlighter-rouge">KafkaDeserializationSchema</code> 允许用户指定这样的 schema，每条 Kafka 中的消息会调用 <code class="highlighter-rouge">T deserialize(ConsumerRecord&lt;byte[], byte[]&gt; record)</code> 反序列化。</p>

<p>为了方便使用，Flink 提供了以下几种 schemas：</p>

<ol>
  <li>
    <p><code class="highlighter-rouge">TypeInformationSerializationSchema</code>（和 <code class="highlighter-rouge">TypeInformationKeyValueSerializationSchema</code>) 基于 Flink 的 <code class="highlighter-rouge">TypeInformation</code> 创建 <code class="highlighter-rouge">schema</code>。
 如果该数据的读和写都发生在 Flink 中，那么这将是非常有用的。此 schema 是其他通用序列化方法的高性能 Flink 替代方案。</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">JsonDeserializationSchema</code>（和 <code class="highlighter-rouge">JSONKeyValueDeserializationSchema</code>）将序列化的 JSON 转化为 ObjectNode 对象，可以使用 <code class="highlighter-rouge">objectNode.get("field").as(Int/String/...)()</code> 来访问某个字段。
 KeyValue objectNode 包含一个含所有字段的 key 和 values 字段，以及一个可选的”metadata”字段，可以访问到消息的 offset、partition、topic 等信息。</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">AvroDeserializationSchema</code> 使用静态提供的 schema 读取 Avro 格式的序列化数据。
 它能够从 Avro 生成的类（<code class="highlighter-rouge">AvroDeserializationSchema.forSpecific(...)</code>）中推断出 schema，或者可以与 <code class="highlighter-rouge">GenericRecords</code>
 一起使用手动提供的 schema（用 <code class="highlighter-rouge">AvroDeserializationSchema.forGeneric(...)</code>）。此反序列化 schema 要求序列化记录不能包含嵌入式架构！</p>

    <ul>
      <li>此模式还有一个版本，可以在 <a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Confluent Schema Registry</a> 中查找编写器的 schema（用于编写记录的 schema）。</li>
      <li>使用这些反序列化 schema 记录将读取从 schema 注册表检索到的 schema 转换为静态提供的 schema（或者通过 <code class="highlighter-rouge">ConfluentRegistryAvroDeserializationSchema.forGeneric(...)</code> 或 <code class="highlighter-rouge">ConfluentRegistryAvroDeserializationSchema.forSpecific(...)</code>）。</li>
    </ul>

    <p><br />要使用此反序列化 schema 必须添加以下依赖：</p>
  </li>
</ol>

<div class="codetabs">
  <div data-lang="AvroDeserializationSchema">

    <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-avro<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

  </div>
  <div data-lang="ConfluentRegistryAvroDeserializationSchema">

    <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-avro-confluent-registry<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

  </div>
</div>

<p>当遇到因一些原因而无法反序列化的损坏消息时，反序列化 schema 会返回 <code class="highlighter-rouge">null</code>，以允许 Flink Kafka 消费者悄悄地跳过损坏的消息。请注意，由于 consumer 的容错能力（请参阅下面的部分以获取更多详细信息），在损坏的消息上失败作业将使 consumer 尝试再次反序列化消息。因此，如果反序列化仍然失败，则 consumer 将在该损坏的消息上进入不间断重启和失败的循环。</p>

<p><a name="kafka-consumers-start-position-configuration"></a></p>

<h3 id="配置-kafka-consumer-开始消费的位置">配置 Kafka Consumer 开始消费的位置</h3>

<p>Flink Kafka Consumer 允许通过配置来确定 Kafka 分区的起始位置。</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="nc">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">myConsumer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(...);</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromEarliest</span><span class="o">();</span>     <span class="c1">// 尽可能从最早的记录开始</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromLatest</span><span class="o">();</span>       <span class="c1">// 从最新的记录开始</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromTimestamp</span><span class="o">(...);</span> <span class="c1">// 从指定的时间开始（毫秒）</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromGroupOffsets</span><span class="o">();</span> <span class="c1">// 默认的方法</span>

<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">);</span>
<span class="o">...</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">env</span> <span class="k">=</span> <span class="nv">StreamExecutionEnvironment</span><span class="o">.</span><span class="py">getExecutionEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="nv">myConsumer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">[</span><span class="kt">String</span><span class="o">](...)</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromEarliest</span><span class="o">()</span>      <span class="c1">// 尽可能从最早的记录开始</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromLatest</span><span class="o">()</span>        <span class="c1">// 从最新的记录开始</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromTimestamp</span><span class="o">(...)</span>  <span class="c1">// 从指定的时间开始（毫秒）</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromGroupOffsets</span><span class="o">()</span>  <span class="c1">// 默认的方法</span>

<span class="k">val</span> <span class="nv">stream</span> <span class="k">=</span> <span class="nv">env</span><span class="o">.</span><span class="py">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">)</span>
<span class="o">...</span></code></pre></figure>

  </div>
</div>

<p>Flink Kafka Consumer 的所有版本都具有上述明确的起始位置配置方法。</p>

<ul>
  <li><code class="highlighter-rouge">setStartFromGroupOffsets</code>（默认方法）：从 Kafka brokers 中的 consumer 组（consumer 属性中的 <code class="highlighter-rouge">group.id</code> 设置）提交的偏移量中开始读取分区。
 如果找不到分区的偏移量，那么将会使用配置中的 <code class="highlighter-rouge">auto.offset.reset</code> 设置。</li>
  <li><code class="highlighter-rouge">setStartFromEarliest()</code> 或者 <code class="highlighter-rouge">setStartFromLatest()</code>：从最早或者最新的记录开始消费，在这些模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</li>
  <li><code class="highlighter-rouge">setStartFromTimestamp(long)</code>：从指定的时间戳开始。对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</li>
</ul>

<p>你也可以为每个分区指定 consumer 应该开始消费的具体 offset：</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">Map</span><span class="o">&lt;</span><span class="nc">KafkaTopicPartition</span><span class="o">,</span> <span class="nc">Long</span><span class="o">&gt;</span> <span class="n">specificStartOffsets</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">&lt;&gt;();</span>
<span class="n">specificStartOffsets</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">0</span><span class="o">),</span> <span class="mi">23L</span><span class="o">);</span>
<span class="n">specificStartOffsets</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">1</span><span class="o">),</span> <span class="mi">31L</span><span class="o">);</span>
<span class="n">specificStartOffsets</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="mi">43L</span><span class="o">);</span>

<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromSpecificOffsets</span><span class="o">(</span><span class="n">specificStartOffsets</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">specificStartOffsets</span> <span class="k">=</span> <span class="k">new</span> <span class="nv">java</span><span class="o">.</span><span class="py">util</span><span class="o">.</span><span class="py">HashMap</span><span class="o">[</span><span class="kt">KafkaTopicPartition</span>, <span class="kt">java.lang.Long</span><span class="o">]()</span>
<span class="nv">specificStartOffsets</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">0</span><span class="o">),</span> <span class="mi">23L</span><span class="o">)</span>
<span class="nv">specificStartOffsets</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">1</span><span class="o">),</span> <span class="mi">31L</span><span class="o">)</span>
<span class="nv">specificStartOffsets</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="mi">43L</span><span class="o">)</span>

<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromSpecificOffsets</span><span class="o">(</span><span class="n">specificStartOffsets</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>上面的例子中使用的配置是指定从 <code class="highlighter-rouge">myTopic</code> 主题的 0 、1 和 2 分区的指定偏移量开始消费。offset 值是 consumer 应该为每个分区读取的下一条消息。请注意：如果 consumer 需要读取在提供的 offset 映射中没有指定 offset 的分区，那么它将回退到该特定分区的默认组偏移行为（即 <code class="highlighter-rouge">setStartFromGroupOffsets()</code>）。</p>

<p>请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，这些起始位置配置方法不会影响消费的起始位置。在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定（有关 checkpointing 的信息，请参阅下一节，以便为 consumer 启用容错功能）。</p>

<p><a name="kafka-consumers-and-fault-tolerance"></a></p>

<h3 id="kafka-consumer-和容错">Kafka Consumer 和容错</h3>

<p>伴随着启用 Flink 的 checkpointing 后，Flink Kafka Consumer 将使用 topic 中的记录，并以一致的方式定期检查其所有 Kafka offset 和其他算子的状态。如果 Job 失败，Flink 会将流式程序恢复到最新 checkpoint 的状态，并从存储在 checkpoint 中的 offset 开始重新消费 Kafka 中的消息。</p>

<p>因此，设置 checkpoint 的间隔定义了程序在发生故障时最多需要返回多少。</p>

<p>为了使 Kafka Consumer 支持容错，需要在 <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/config.html#execution-checkpointing-interval">执行环境</a> 中启用拓扑的 checkpointing。</p>

<p>如果未启用 checkpoint，那么 Kafka consumer 将定期向 Zookeeper 提交 offset。</p>

<p><a name="kafka-consumers-topic-and-partition-discovery"></a></p>

<h3 id="kafka-consumer-topic-和分区发现">Kafka Consumer Topic 和分区发现</h3>

<p><a name="partition-discovery"></a></p>

<h4 id="分区发现">分区发现</h4>

<p>Flink Kafka Consumer 支持发现动态创建的 Kafka 分区，并使用精准一次的语义保证去消耗它们。在初始检索分区元数据之后（即，当 Job 开始运行时）发现的所有分区将从最早可能的 offset 中消费。</p>

<p>默认情况下，是禁用了分区发现的。若要启用它，请在提供的属性配置中为 <code class="highlighter-rouge">flink.partition-discovery.interval-millis</code> 设置大于 0 的值，表示发现分区的间隔是以毫秒为单位的。</p>

<p><a name="topic-discovery"></a></p>

<h4 id="topic-发现">Topic 发现</h4>

<p>在更高的级别上，Flink Kafka Consumer 还能够使用正则表达式基于 Topic 名称的模式匹配来发现 Topic。请看下面的例子：</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="nc">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">);</span>

<span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">myConsumer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(</span>
    <span class="n">java</span><span class="o">.</span><span class="na">util</span><span class="o">.</span><span class="na">regex</span><span class="o">.</span><span class="na">Pattern</span><span class="o">.</span><span class="na">compile</span><span class="o">(</span><span class="s">"test-topic-[0-9]"</span><span class="o">),</span>
    <span class="k">new</span> <span class="nf">SimpleStringSchema</span><span class="o">(),</span>
    <span class="n">properties</span><span class="o">);</span>

<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">);</span>
<span class="o">...</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">env</span> <span class="k">=</span> <span class="nv">StreamExecutionEnvironment</span><span class="o">.</span><span class="py">getExecutionEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="nv">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">myConsumer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span>
  <span class="nv">java</span><span class="o">.</span><span class="py">util</span><span class="o">.</span><span class="py">regex</span><span class="o">.</span><span class="py">Pattern</span><span class="o">.</span><span class="py">compile</span><span class="o">(</span><span class="s">"test-topic-[0-9]"</span><span class="o">),</span>
  <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">,</span>
  <span class="n">properties</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">stream</span> <span class="k">=</span> <span class="nv">env</span><span class="o">.</span><span class="py">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">)</span>
<span class="o">...</span></code></pre></figure>

  </div>
</div>

<p>在上面的例子中，当 Job 开始运行时，Consumer 将订阅名称与指定正则表达式匹配的所有主题（以 <code class="highlighter-rouge">test-topic</code> 开头并以单个数字结尾）。</p>

<p>要允许 consumer 在作业开始运行后发现动态创建的主题，那么请为 <code class="highlighter-rouge">flink.partition-discovery.interval-millis</code> 设置非负值。这允许 consumer 发现名称与指定模式匹配的新主题的分区。</p>

<p><a name="kafka-consumers-offset-committing-behaviour-configuration"></a></p>

<h3 id="kafka-consumer-提交-offset-的行为配置">Kafka Consumer 提交 Offset 的行为配置</h3>

<p>Flink Kafka Consumer 允许有配置如何将 offset 提交回 Kafka broker 的行为。请注意：Flink Kafka Consumer 不依赖于提交的 offset 来实现容错保证。提交的 offset 只是一种方法，用于公开 consumer 的进度以便进行监控。</p>

<p>配置 offset 提交行为的方法是否相同，取决于是否为 job 启用了 checkpointing。</p>

<ul>
  <li>
    <p><em>禁用 Checkpointing：</em> 如果禁用了 checkpointing，则 Flink Kafka Consumer 依赖于内部使用的 Kafka client 自动定期 offset 提交功能。
 因此，要禁用或启用 offset 的提交，只需将 <code class="highlighter-rouge">enable.auto.commit</code> 或者 <code class="highlighter-rouge">auto.commit.interval.ms</code> 的Key 值设置为提供的 <code class="highlighter-rouge">Properties</code> 配置中的适当值。</p>
  </li>
  <li>
    <p><em>启用 Checkpointing：</em> 如果启用了 checkpointing，那么当 checkpointing 完成时，Flink Kafka Consumer 将提交的 offset 存储在 checkpoint 状态中。
 这确保 Kafka broker 中提交的 offset 与 checkpoint 状态中的 offset 一致。
 用户可以通过调用 consumer 上的 <code class="highlighter-rouge">setCommitOffsetsOnCheckpoints(boolean)</code> 方法来禁用或启用 offset 的提交(默认情况下，这个值是 true )。
 注意，在这个场景中，<code class="highlighter-rouge">Properties</code> 中的自动定期 offset 提交设置会被完全忽略。</p>
  </li>
</ul>

<p><a name="kafka-consumers-and-timestamp-extractionwatermark-emission"></a></p>

<h3 id="kafka-consumer-和-时间戳抽取以及-watermark-发送">Kafka Consumer 和 时间戳抽取以及 watermark 发送</h3>

<p>在许多场景中，记录的时间戳是(显式或隐式)嵌入到记录本身中。此外，用户可能希望定期或以不规则的方式 Watermark，例如基于 Kafka 流中包含当前事件时间的 watermark 的特殊记录。对于这些情况，Flink Kafka Consumer 允许指定 <code class="highlighter-rouge">AssignerWithPeriodicWatermarks</code> 或 <code class="highlighter-rouge">AssignerWithPunctuatedWatermarks</code>。</p>

<p>你可以按照<a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamps_watermarks.html">此处</a>的说明指定自定义时间戳抽取器或者 Watermark 发送器，或者使用 <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamp_extractors.html">内置的</a>。你也可以通过以下方式将其传递给你的 consumer：</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">);</span>

<span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">myConsumer</span> <span class="o">=</span>
    <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">);</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">assignTimestampsAndWatermarks</span><span class="o">(</span>
    <span class="nc">WatermarkStrategy</span><span class="o">.</span>
        <span class="o">.</span><span class="na">forBoundedOutOfOrderness</span><span class="o">(</span><span class="nc">Duration</span><span class="o">.</span><span class="na">ofSeconds</span><span class="o">(</span><span class="mi">20</span><span class="o">)));</span>

<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">myConsumer</span> <span class="k">=</span>
    <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">(</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">);</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">assignTimestampsAndWatermarks</span><span class="o">(</span>
    <span class="nc">WatermarkStrategy</span><span class="o">.</span>
        <span class="o">.</span><span class="py">forBoundedOutOfOrderness</span><span class="o">(</span><span class="nv">Duration</span><span class="o">.</span><span class="py">ofSeconds</span><span class="o">(</span><span class="mi">20</span><span class="o">)))</span>

<span class="k">val</span> <span class="nv">stream</span> <span class="k">=</span> <span class="nv">env</span><span class="o">.</span><span class="py">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p><strong>请注意</strong>：如果 watermark assigner 依赖于从 Kafka 读取的消息来上涨其 watermark （通常就是这种情况），那么所有主题和分区都需要有连续的消息流。否则，整个应用程序的 watermark 将无法上涨，所有基于时间的算子（例如时间窗口或带有计时器的函数）也无法运行。单个的 Kafka 分区也会导致这种反应。考虑设置适当的 <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamps_watermarks.html#dealing-with-idle-sources">idelness timeouts</a> 来缓解这个问题。</p>

<p><a name="kafka-producer"></a></p>

<h2 id="kafka-producer">Kafka Producer</h2>

<p>Flink Kafka Producer 被称为 <code class="highlighter-rouge">FlinkKafkaProducer</code>。它允许将消息流写入一个或多个 Kafka topic。</p>

<p>构造器接收下列参数：</p>

<ol>
  <li>事件被写入的默认输出 topic</li>
  <li>序列化数据写入 Kafka 的 SerializationSchema / KafkaSerializationSchema</li>
  <li>Kafka client 的 Properties。下列 property 是必须的：
    <ul>
      <li>“bootstrap.servers” （逗号分隔 Kafka broker 列表）</li>
    </ul>
  </li>
  <li>容错语义</li>
</ol>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="o">...;</span>

<span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>

<span class="nc">FlinkKafkaProducer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">myProducer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaProducer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;(</span>
        <span class="s">"my-topic"</span><span class="o">,</span>                  <span class="c1">// 目标 topic</span>
        <span class="k">new</span> <span class="nf">SimpleStringSchema</span><span class="o">()</span>     <span class="c1">// 序列化 schema</span>
        <span class="n">properties</span><span class="o">,</span>                  <span class="c1">// producer 配置</span>
        <span class="nc">FlinkKafkaProducer</span><span class="o">.</span><span class="na">Semantic</span><span class="o">.</span><span class="na">EXACTLY_ONCE</span><span class="o">);</span> <span class="c1">// 容错</span>

<span class="n">stream</span><span class="o">.</span><span class="na">addSink</span><span class="o">(</span><span class="n">myProducer</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="nv">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">myProducer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaProducer</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span>
        <span class="s">"my-topic"</span><span class="o">,</span>               <span class="c1">// 目标 topic</span>
        <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="c1">// 序列化 schema</span>
        <span class="n">properties</span><span class="o">,</span>               <span class="c1">// producer 配置</span>
        <span class="nv">FlinkKafkaProducer</span><span class="o">.</span><span class="py">Semantic</span><span class="o">.</span><span class="py">EXACTLY_ONCE</span><span class="o">)</span> <span class="c1">// 容错</span>

<span class="nv">stream</span><span class="o">.</span><span class="py">addSink</span><span class="o">(</span><span class="n">myProducer</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p><a name="the-serializationschema"></a></p>

<h2 id="serializationschema"><code class="highlighter-rouge">SerializationSchema</code></h2>

<p>Flink Kafka Producer 需要知道如何将 Java/Scala 对象转化为二进制数据。</p>

<p><code class="highlighter-rouge">KafkaSerializationSchema</code> 允许用户指定这样的 schema。它会为每个记录调用 <code class="highlighter-rouge">ProducerRecord&lt;byte[], byte[]&gt; serialize(T element, @Nullable Long timestamp)</code> 方法，产生一个写入到 Kafka 的 <code class="highlighter-rouge">ProducerRecord</code>。</p>

<p>用户可以对如何将数据写到 Kafka 进行细粒度的控制。你可以通过 producer record：</p>

<ul>
  <li>设置 header 值</li>
  <li>为每个 record 定义 key</li>
  <li>指定数据的自定义分区</li>
</ul>

<p><a name="kafka-producers-and-fault-tolerance"></a></p>

<h3 id="kafka-producer-和容错">Kafka Producer 和容错</h3>

<p>启用 Flink 的 checkpointing 后，<code class="highlighter-rouge">FlinkKafkaProducer</code> 可以提供精确一次的语义保证。</p>

<p>除了启用 Flink 的 checkpointing，你也可以通过将适当的 <code class="highlighter-rouge">semantic</code> 参数传递给 <code class="highlighter-rouge">FlinkKafkaProducer</code> 来选择三种不同的操作模式：</p>

<ul>
  <li><code class="highlighter-rouge">Semantic.NONE</code>：Flink 不会有任何语义的保证，产生的记录可能会丢失或重复。</li>
  <li><code class="highlighter-rouge">Semantic.AT_LEAST_ONCE</code>（默认设置）：可以保证不会丢失任何记录（但是记录可能会重复）</li>
  <li><code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code>：使用 Kafka 事务提供精确一次语义。无论何时，在使用事务写入 Kafka 时，都要记得为所有消费 Kafka 消息的应用程序设置所需的 <code class="highlighter-rouge">isolation.level</code>（<code class="highlighter-rouge">read_committed</code> 或 <code class="highlighter-rouge">read_uncommitted</code> - 后者是默认值）。</li>
</ul>

<p><a name="caveats"></a></p>

<h5 id="注意事项">注意事项</h5>

<p><code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code> 模式依赖于事务提交的能力。事务提交发生于触发 checkpoint 之前，以及从 checkpoint 恢复之后。如果从 Flink 应用程序崩溃到完全重启的时间超过了 Kafka 的事务超时时间，那么将会有数据丢失（Kafka 会自动丢弃超出超时时间的事务）。考虑到这一点，请根据预期的宕机时间来合理地配置事务超时时间。</p>

<p>默认情况下，Kafka broker 将 <code class="highlighter-rouge">transaction.max.timeout.ms</code> 设置为 15 分钟。此属性不允许为大于其值的 producer 设置事务超时时间。
默认情况下，<code class="highlighter-rouge">FlinkKafkaProducer</code> 将 producer config 中的 <code class="highlighter-rouge">transaction.timeout.ms</code> 属性设置为 1 小时，因此在使用 <code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code> 模式之前应该增加 <code class="highlighter-rouge">transaction.max.timeout.ms</code> 的值。</p>

<p>在 <code class="highlighter-rouge">KafkaConsumer</code> 的 <code class="highlighter-rouge">read_committed</code> 模式中，任何未结束（既未中止也未完成）的事务将阻塞来自给定 Kafka topic 的未结束事务之后的所有读取数据。
换句话说，在遵循如下一系列事件之后：</p>

<ol>
  <li>用户启动了 <code class="highlighter-rouge">transaction1</code> 并使用它写了一些记录</li>
  <li>用户启动了 <code class="highlighter-rouge">transaction2</code> 并使用它编写了一些其他记录</li>
  <li>用户提交了 <code class="highlighter-rouge">transaction2</code></li>
</ol>

<p>即使 <code class="highlighter-rouge">transaction2</code> 中的记录已提交，在提交或中止 <code class="highlighter-rouge">transaction1</code> 之前，消费者也不会看到这些记录。这有 2 层含义：</p>

<ul>
  <li>首先，在 Flink 应用程序的正常工作期间，用户可以预料 Kafka 主题中生成的记录的可见性会延迟，相当于已完成 checkpoint 之间的平均时间。</li>
  <li>其次，在 Flink 应用程序失败的情况下，此应用程序正在写入的供消费者读取的主题将被阻塞，直到应用程序重新启动或配置的事务超时时间过去后，才恢复正常。此标注仅适用于有多个 agent 或者应用程序写入同一 Kafka 主题的情况。</li>
</ul>

<p><strong>注意</strong>：<code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code> 模式为每个 <code class="highlighter-rouge">FlinkKafkaProducer</code> 实例使用固定大小的 KafkaProducer 池。每个 checkpoint 使用其中一个 producer。如果并发 checkpoint 的数量超过池的大小，<code class="highlighter-rouge">FlinkKafkaProducer</code> 将抛出异常，并导致整个应用程序失败。请合理地配置最大池大小和最大并发 checkpoint 数量。</p>

<p><strong>注意</strong>：<code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code> 会尽一切可能不留下任何逗留的事务，否则会阻塞其他消费者从这个 Kafka topic 中读取数据。但是，如果 Flink 应用程序在第一次 checkpoint 之前就失败了，那么在重新启动此类应用程序后，系统中不会有先前池大小（pool size）相关的信息。因此，在第一次 checkpoint 完成前对 Flink 应用程序进行缩容，且并发数缩容倍数大于安全系数 <code class="highlighter-rouge">FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR</code> 的值的话，是不安全的。</p>

<p><a name="kafka-connector-metrics"></a></p>

<h2 id="kafka-连接器指标">Kafka 连接器指标</h2>

<p>Flink 的 Kafka 连接器通过 Flink 的 <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/metrics.html">metric 系统</a> 提供一些指标来分析 Kafka Connector 的状况。Producer 通过 Flink 的 metrics 系统为所有支持的版本导出 Kafka 的内部指标。consumer 从 Kafka 0.10 版本开始导出所有指标。Kafka 在其<a href="http://kafka.apache.org/documentation/#selector_monitoring">文档</a>中列出了所有导出的指标。</p>

<p>除了这些指标之外，所有 consumer 都暴露了每个主题分区的 <code class="highlighter-rouge">current-offsets</code> 和 <code class="highlighter-rouge">committed-offsets</code>。<code class="highlighter-rouge">current-offsets</code> 是指分区中的当前偏移量。指的是我们成功检索和发出的最后一个元素的偏移量。<code class="highlighter-rouge">committed-offsets</code> 是最后提交的偏移量。这为用户提供了 at-least-once 语义，用于提交给 Zookeeper 或 broker 的偏移量。对于 Flink 的偏移检查点，系统提供精准一次语义。</p>

<p>提交给 ZK 或 broker 的偏移量也可以用来跟踪 Kafka consumer 的读取进度。每个分区中提交的偏移量和最近偏移量之间的差异称为 <em>consumer lag</em>。如果 Flink 拓扑消耗来自 topic 的数据的速度比添加新数据的速度慢，那么延迟将会增加，consumer 将会滞后。对于大型生产部署，我们建议监视该指标，以避免增加延迟。</p>

<p><a name="enabling-kerberos-authentication"></a></p>

<h2 id="启用-kerberos-身份验证">启用 Kerberos 身份验证</h2>

<p>Flink 通过 Kafka 连接器提供了一流的支持，可以对 Kerberos 配置的 Kafka 安装进行身份验证。只需在 <code class="highlighter-rouge">flink-conf.yaml</code> 中配置 Flink。像这样为 Kafka 启用 Kerberos 身份验证：</p>

<ol>
  <li>通过设置以下内容配置 Kerberos 票据
    <ul>
      <li><code class="highlighter-rouge">security.kerberos.login.use-ticket-cache</code>：默认情况下，这个值是 <code class="highlighter-rouge">true</code>，Flink 将尝试在 <code class="highlighter-rouge">kinit</code> 管理的票据缓存中使用 Kerberos 票据。注意！在 YARN 上部署的 Flink  jobs 中使用 Kafka 连接器时，使用票据缓存的 Kerberos 授权将不起作用。使用 Mesos 进行部署时也是如此，因为 Mesos 部署不支持使用票据缓存进行授权。</li>
      <li><code class="highlighter-rouge">security.kerberos.login.keytab</code> 和 <code class="highlighter-rouge">security.kerberos.login.principal</code>：要使用 Kerberos keytabs，需为这两个属性设置值。</li>
    </ul>
  </li>
  <li>将 <code class="highlighter-rouge">KafkaClient</code> 追加到 <code class="highlighter-rouge">security.kerberos.login.contexts</code>：这告诉 Flink 将配置的 Kerberos 票据提供给 Kafka 登录上下文以用于 Kafka 身份验证。</li>
</ol>

<p>一旦启用了基于 Kerberos 的 Flink 安全性后，只需在提供的属性配置中包含以下两个设置（通过传递给内部 Kafka 客户端），即可使用 Flink Kafka Consumer 或 Producer 向 Kafk a进行身份验证：</p>

<ul>
  <li>将 <code class="highlighter-rouge">security.protocol</code> 设置为 <code class="highlighter-rouge">SASL_PLAINTEXT</code>（默认为 <code class="highlighter-rouge">NONE</code>）：用于与 Kafka broker 进行通信的协议。使用独立 Flink 部署时，也可以使用 <code class="highlighter-rouge">SASL_SSL</code>；请在<a href="https://kafka.apache.org/documentation/#security_configclients">此处</a>查看如何为 SSL 配置 Kafka 客户端。</li>
  <li>将 <code class="highlighter-rouge">sasl.kerberos.service.name</code> 设置为 <code class="highlighter-rouge">kafka</code>（默认为 <code class="highlighter-rouge">kafka</code>）：此值应与用于 Kafka broker 配置的 <code class="highlighter-rouge">sasl.kerberos.service.name</code> 相匹配。客户端和服务器配置之间的服务名称不匹配将导致身份验证失败。</li>
</ul>

<p>有关 Kerberos 安全性 Flink 配置的更多信息，请参见<a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/config.html">这里</a>。你也可以在<a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/security/security-kerberos.html">这里</a>进一步了解 Flink 如何在内部设置基于 kerberos 的安全性。</p>

<p><a name="upgrading-to-the-latest-connector-version"></a></p>

<h2 id="升级到最近的连接器版本">升级到最近的连接器版本</h2>

<p>通用的升级步骤概述见 <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/upgrading.html">升级 Jobs 和 Flink 版本指南</a>。对于 Kafka，你还需要遵循这些步骤：</p>

<ul>
  <li>不要同时升级 Flink 和 Kafka 连接器</li>
  <li>确保你对 Consumer 设置了 <code class="highlighter-rouge">group.id</code></li>
  <li>在 Consumer 上设置 <code class="highlighter-rouge">setCommitOffsetsOnCheckpoints(true)</code>，以便读 offset 提交到 Kafka。务必在停止和恢复 savepoint 前执行此操作。你可能需要在旧的连接器版本上进行停止/重启循环来启用此设置。</li>
  <li>在 Consumer 上设置 <code class="highlighter-rouge">setStartFromGroupOffsets(true)</code>，以便我们从 Kafka 获取读 offset。这只会在 Flink 状态中没有读 offset 时生效，这也是为什么下一步非要重要的原因。</li>
  <li>修改 source/sink 分配到的 <code class="highlighter-rouge">uid</code>。这会确保新的 source/sink 不会从旧的 sink/source 算子中读取状态。</li>
  <li>使用 <code class="highlighter-rouge">--allow-non-restored-state</code> 参数启动新 job，因为我们在 savepoint 中仍然有先前连接器版本的状态。</li>
</ul>

<p><a name="troubleshooting"></a></p>

<h2 id="问题排查">问题排查</h2>

<div class="alert alert-warning">
如果你在使用 Flink 时对 Kafka 有问题，请记住，Flink 只封装 <a href="https://kafka.apache.org/documentation/#consumerapi">KafkaConsumer</a> 或 <a href="https://kafka.apache.org/documentation/#producerapi">KafkaProducer</a>，你的问题可能独立于 Flink，有时可以通过升级 Kafka broker 程序、重新配置 Kafka broker 程序或在 Flink 中重新配置 <tt>KafkaConsumer</tt> 或 <tt>KafkaProducer</tt> 来解决。下面列出了一些常见问题的示例。
</div>

<p><a name="data-loss"></a></p>

<h3 id="数据丢失">数据丢失</h3>

<p>根据你的 Kafka 配置，即使在 Kafka 确认写入后，你仍然可能会遇到数据丢失。特别要记住在 Kafka 的配置中设置以下属性：</p>

<ul>
  <li><code class="highlighter-rouge">acks</code></li>
  <li><code class="highlighter-rouge">log.flush.interval.messages</code></li>
  <li><code class="highlighter-rouge">log.flush.interval.ms</code></li>
  <li><code class="highlighter-rouge">log.flush.*</code></li>
</ul>

<p>上述选项的默认值是很容易导致数据丢失的。请参考 Kafka 文档以获得更多的解释。</p>

<p><a name="unknowntopicorpartitionexception"></a></p>

<h3 id="unknowntopicorpartitionexception">UnknownTopicOrPartitionException</h3>

<p>导致此错误的一个可能原因是正在进行新的 leader 选举，例如在重新启动 Kafka broker 之后或期间。这是一个可重试的异常，因此 Flink job 应该能够重启并恢复正常运行。也可以通过更改 producer 设置中的 <code class="highlighter-rouge">retries</code> 属性来规避。但是，这可能会导致重新排序消息，反过来可以通过将 <code class="highlighter-rouge">max.in.flight.requests.per.connection</code> 设置为 1 来避免不需要的消息。</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>



<div class="footer">
  <a href="https://cwiki.apache.org/confluence/display/FLINK/Flink+Translation+Specifications" target="_blank">
    
      想参与贡献翻译？
    
  </a>
</div>


        </div>
      </div>
    </div><!-- /.container -->

    <!-- default code tab -->
    <script>var defaultCodeTab = "";</script>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/js/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.1.0/anchor.min.js"></script>
    <script src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/js/flink.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- Disqus -->
    
  </body>
</html>
