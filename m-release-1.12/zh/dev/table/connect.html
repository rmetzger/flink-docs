<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink 1.12 Documentation: Table API Legacy Connectors</title>
    <link rel="shortcut icon" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/favicon.ico" type="image/x-icon">
    <link rel="icon" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="//ci.apache.org/projects/flink/flink-docs-stable/zh/dev/table/connect.html">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/flink.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/syntax.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/codetabs.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/font-awesome/css/font-awesome.min.css">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    

    <!-- Main content. -->
    <div class="container">
      
      <div class="row">
        <div class="col-lg-3" id="sidenavcol">
          <div class="sidenav-logo">
  <p><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/"><img class="bottom" alt="Apache Flink" src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/img/navbar-brand-logo.jpg"></a> v1.12</p>
</div>
<ul id="sidenav">
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/"><i class="fa fa-home title" aria-hidden="true"></i> Home</a></li><hr class="section-break"></hr>
<li><a href="#collapse-2" data-toggle="collapse"><i class="fa fa-rocket title appetizer" aria-hidden="true"></i> Try Flink<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-2"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/local_installation.html">本地模式安装</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/datastream_api.html">基于 DataStream API 实现欺诈检测</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/table_api.html">基于 Table API 实现实时报表</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/try-flink/flink-operations-playground.html">Flink 操作场景</a></li>
</ul></div></li>
<li><a href="#collapse-8" data-toggle="collapse"><i class="fa fa-hand-paper-o title appetizer" aria-hidden="true"></i> 实践练习<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-8"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/datastream_api.html">DataStream API 简介</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/etl.html">数据管道 & ETL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/streaming_analytics.html">流式分析</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/event_driven.html">事件驱动应用</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink/fault_tolerance.html">容错处理</a></li>
</ul></div></li>
<li><a href="#collapse-15" data-toggle="collapse"><i class="fa fa-map-o title appetizer" aria-hidden="true"></i> 概念透析<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-15"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/index.html">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/stateful-stream-processing.html">有状态流处理</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/timely-stream-processing.html">及时流处理</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/flink-architecture.html">Flink 架构</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/concepts/glossary.html">词汇表</a></li>
</ul></div></li><hr class="section-break"></hr>
<li><a href="#collapse-21" data-toggle="collapse"><i class="fa fa-code title maindish" aria-hidden="true"></i> 应用开发<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-21"><ul>
<li><a href="#collapse-22" data-toggle="collapse">DataStream API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-22"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/datastream_api.html">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/datastream_execution_mode.html">Execution Mode (Batch/Streaming)</a></li>
<li><a href="#collapse-24" data-toggle="collapse">事件时间<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-24"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_time.html">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamps_watermarks.html">生成 Watermark</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamp_extractors.html">内置 Watermark 生成器</a></li>
</ul></div></li>
<li><a href="#collapse-28" data-toggle="collapse">状态与容错<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-28"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/state.html">Working with State</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/broadcast_state.html">Broadcast State 模式</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/checkpointing.html">Checkpointing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/queryable_state.html">Queryable State</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/state_backends.html">State Backends</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/schema_evolution.html">状态数据结构升级</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/custom_serialization.html">自定义状态序列化</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/user_defined_functions.html">用户自定义 Functions</a></li>
<li><a href="#collapse-38" data-toggle="collapse">算子<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-38"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/windows.html">窗口</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/joining.html">Joining</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/process_function.html">Process Function</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/asyncio.html">异步 I/O</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/sources.html">Data Sources</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/side_output.html">旁路输出</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/application_parameters.html">Handling Application Parameters</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/testing.html">测试</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/experimental.html">实验功能</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/scala_api_extensions.html">Scala API Extensions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/java_lambdas.html">Java Lambda 表达式</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/project-configuration.html">Project Configuration</a></li>
</ul></div></li>
<li><a href="#collapse-53" data-toggle="collapse">DataSet API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-53"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/dataset_transformations.html">Transformations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/iterations.html">迭代</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/zip_elements_guide.html">Zipping Elements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/hadoop_compatibility.html">Hadoop 兼容</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/local_execution.html">本地执行</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/cluster_execution.html">集群执行</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/examples.html">Batch 示例</a></li>
</ul></div></li>
<li><a href="#collapse-62" data-toggle="collapse">Table API & SQL<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-62"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/common.html">概念与通用 API</a></li>
<li><a href="#collapse-64" data-toggle="collapse">流式概念<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-64"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/dynamic_tables.html">动态表 (Dynamic Table)</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/time_attributes.html">时间属性</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/joins.html">流上的 Join</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/temporal_tables.html">时态表（Temporal Tables）</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/match_recognize.html">模式检测</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/streaming/query_configuration.html">Query Configuration</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/types.html">数据类型</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/tableApi.html">Table API</a></li>
<li><a href="#collapse-74" data-toggle="collapse">SQL<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-74"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/queries.html">查询语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/create.html">CREATE 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/drop.html">DROP 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/alter.html">ALTER 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/insert.html">INSERT 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/hints.html">SQL Hints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/describe.html">DESCRIBE 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/explain.html">EXPLAIN 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/use.html">USE 语句</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/show.html">SHOW 语句</a></li>
</ul></div></li>
<li><a href="#collapse-86" data-toggle="collapse">函数<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-86"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/systemFunctions.html">系统（内置）函数</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/udfs.html">自定义函数</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/modules.html">模块</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/catalogs.html">Catalogs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sqlClient.html">SQL 客户端</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/config.html">配置</a></li>
<li><a href="#collapse-94" data-toggle="collapse">Performance Tuning<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-94"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/tuning/streaming_aggregation_optimization.html">流式聚合</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sourceSinks.html">User-defined Sources & Sinks</a></li>
</ul></div></li>
<li><a href="#collapse-99" data-toggle="collapse">Python API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-99"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/installation.html">环境安装</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table_api_tutorial.html">Table API 教程</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/datastream_tutorial.html">DataStream API 教程</a></li>
<li><a href="#collapse-103" data-toggle="collapse">Table API用户指南<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-103"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/intro_to_table_api.html">Python Table API 简介</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/table_environment.html">TableEnvironment</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/operations.html">Operations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/python_types.html">数据类型</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/built_in_functions.html">系统（内置）函数</a></li>
<li><a href="#collapse-109" data-toggle="collapse">自定义函数<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-109"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/udfs/python_udfs.html">普通自定义函数（UDF）</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/udfs/vectorized_python_udfs.html">向量化自定义函数</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/conversion_of_pandas.html">PyFlink Table 和 Pandas DataFrame 互转</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/dependency_management.html">依赖管理</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/sql.html">SQL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/catalogs.html">Catalogs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/metrics.html">指标</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/table-api-users-guide/python_table_api_connectors.html">连接器</a></li>
</ul></div></li>
<li><a href="#collapse-120" data-toggle="collapse">DataStream API用户指南<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-120"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/datastream-api-users-guide/data_types.html">数据类型</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/datastream-api-users-guide/operators.html">算子</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/datastream-api-users-guide/dependency_management.html">依赖管理</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/python_config.html">配置</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/environment_variables.html">环境变量</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/python/faq.html">常见问题</a></li>
</ul></div></li>
<li><a href="#collapse-129" data-toggle="collapse">数据类型以及序列化<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-129"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/types_serialization.html">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/custom_serializers.html">自定义序列化器</a></li>
</ul></div></li>
<li><a href="#collapse-132" data-toggle="collapse">管理执行<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-132"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/execution_configuration.html">执行配置</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/packaging.html">程序打包</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/parallel.html">并行执行</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/execution_plans.html">执行计划</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/task_failure_recovery.html">Task 故障恢复</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/migration.html">API 迁移指南</a></li>
</ul></div></li>
<li><a href="#collapse-141" data-toggle="collapse"><i class="fa fa-book title maindish" aria-hidden="true"></i> Libraries<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-141"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/cep.html">事件处理 (CEP)</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/state_processor_api.html">State Processor API</a></li>
<li><a href="#collapse-144" data-toggle="collapse">图计算: Gelly<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-144"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/graph_api.html">Graph API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/iterative_graph_processing.html">Iterative Graph Processing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/library_methods.html">Library Methods</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/graph_algorithms.html">Graph Algorithms</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/graph_generators.html">Graph Generators</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/libs/gelly/bipartite_graph.html">Bipartite Graph</a></li>
</ul></div></li>
</ul></div></li>
<li><a href="#collapse-153" data-toggle="collapse"><i class="fa fa-random title maindish" aria-hidden="true"></i> Connectors<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-153"><ul>
<li><a href="#collapse-154" data-toggle="collapse">DataStream Connectors<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-154"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/guarantees.html">容错保证</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html">Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/cassandra.html">Cassandra</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kinesis.html">Kinesis</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/elasticsearch.html">Elasticsearch</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/file_sink.html">File Sink</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/streamfile_sink.html">Streaming File Sink</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/rabbitmq.html">RabbitMQ</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/nifi.html">NiFi</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/pubsub.html">Google Cloud PubSub</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/twitter.html">Twitter</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/jdbc.html">JDBC</a></li>
</ul></div></li>
<li><a href="#collapse-168" data-toggle="collapse">Table & SQL Connectors<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-168"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/">概览</a></li>
<li><a href="#collapse-169" data-toggle="collapse">Formats<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-169"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/csv.html">CSV</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/json.html">JSON</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/avro-confluent.html">Confluent Avro</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/avro.html">Avro</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/debezium.html">Debezium</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/canal.html">Canal</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/maxwell.html">Maxwell</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/parquet.html">Parquet</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/orc.html">Orc</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/formats/raw.html">Raw</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/kafka.html">Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/upsert-kafka.html">Upsert Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/kinesis.html">Kinesis</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/jdbc.html">JDBC</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/elasticsearch.html">Elasticsearch</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/filesystem.html">FileSystem</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hbase.html">HBase</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/datagen.html">DataGen</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/print.html">Print</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/blackhole.html">BlackHole</a></li>
<li><a href="#collapse-191" data-toggle="collapse">Hive<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-191"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/hive_catalog.html">Hive Catalog</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/hive_dialect.html">Hive 方言</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/hive_read_write.html">Hive Read & Write</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/hive_functions.html">Hive 函数</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/downloads.html">下载</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/batch/connectors.html">DataSet Connectors</a></li>
</ul></div></li>
<li><a href="#collapse-201" data-toggle="collapse"><i class="fa fa-sliders title maindish" aria-hidden="true"></i> Deployment<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-201"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/">概览</a></li>
<li><a href="#collapse-202" data-toggle="collapse">Resource Providers<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-202"><ul>
<li><a href="#collapse-203" data-toggle="collapse">Standalone<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-203"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/standalone/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/standalone/local.html">本地集群</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/standalone/docker.html">Docker</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/standalone/kubernetes.html">Kubernetes</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/native_kubernetes.html">Native Kubernetes</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/yarn.html">YARN</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/resource-providers/mesos.html">Mesos</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/config.html">配置参数</a></li>
<li><a href="#collapse-213" data-toggle="collapse">内存配置<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-213"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_setup.html">配置 Flink 进程的内存</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_setup_tm.html">配置 TaskManager 内存</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_setup_jobmanager.html">配置 JobManager 内存</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_tuning.html">调优指南</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_trouble.html">常见问题</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/memory/mem_migration.html">升级指南</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/cli.html">Command-Line Interface</a></li>
<li><a href="#collapse-222" data-toggle="collapse">文件系统<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-222"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/common.html">通用配置</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/s3.html">Amazon S3</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/oss.html">阿里云 OSS</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/azure.html">Azure Blob 存储</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/filesystems/plugins.html">Plugins</a></li>
</ul></div></li>
<li><a href="#collapse-229" data-toggle="collapse">High Availability (HA)<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-229"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/ha/">概览</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/ha/zookeeper_ha.html">ZooKeeper HA Services</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/ha/kubernetes_ha.html">Kubernetes HA Services</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/metric_reporters.html">Metric Reporters</a></li>
<li><a href="#collapse-234" data-toggle="collapse">Security<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-234"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/security/security-ssl.html">SSL 设置</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/security/security-kerberos.html">Kerberos</a></li>
</ul></div></li>
<li><a href="#collapse-238" data-toggle="collapse">REPLs<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-238"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/repls/python_shell.html">Python REPL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/repls/scala_shell.html">Scala REPL</a></li>
</ul></div></li>
<li><a href="#collapse-242" data-toggle="collapse">Advanced<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-242"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/advanced/external_resources.html">扩展资源</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/advanced/historyserver.html">History Server</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/advanced/logging.html">日志</a></li>
</ul></div></li>
</ul></div></li>
<li><a href="#collapse-248" data-toggle="collapse"><i class="fa fa-cogs title maindish" aria-hidden="true"></i> Operations<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-248"><ul>
<li><a href="#collapse-249" data-toggle="collapse">状态与容错<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-249"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/state/checkpoints.html">Checkpoints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/state/savepoints.html">Savepoints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/state/state_backends.html">State Backends</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/state/large_state_tuning.html">大状态与 Checkpoint 调优</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/metrics.html">指标</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/rest_api.html">REST API</a></li>
<li><a href="#collapse-257" data-toggle="collapse">Debugging<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-257"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/debugging/debugging_event_time.html">调试窗口与事件时间</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/debugging/debugging_classloading.html">调试类加载</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/debugging/application_profiling.html">应用程序分析</a></li>
</ul></div></li>
<li><a href="#collapse-262" data-toggle="collapse">Monitoring<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-262"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/monitoring/checkpoint_monitoring.html">监控 Checkpoint</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/monitoring/back_pressure.html">监控反压</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/upgrading.html">升级应用程序和 Flink 版本</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/ops/production_ready.html">生产就绪情况核对清单</a></li>
</ul></div></li><hr class="section-break"></hr>
<li><a href="#collapse-269" data-toggle="collapse"><i class="fa fa-cogs title dessert" aria-hidden="true"></i> Flink 开发<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-269"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/flinkDev/ide_setup.html">导入 Flink 到 IDE 中</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/flinkDev/building.html">从源码构建 Flink</a></li>
</ul></div></li>
<li><a href="#collapse-273" data-toggle="collapse"><i class="fa fa-book title dessert" aria-hidden="true"></i> 内幕<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-273"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/internals/job_scheduling.html">作业调度</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/internals/task_lifecycle.html">Task 生命周期</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/internals/filesystems.html">文件系统</a></li>
</ul></div></li>
  <li class="divider"></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java"><i class="fa fa-external-link title" aria-hidden="true"></i> Javadocs</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/scala/index.html#org.apache.flink.api.scala.package"><i class="fa fa-external-link title" aria-hidden="true"></i> Scaladocs</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/python"><i class="fa fa-external-link title" aria-hidden="true"></i> Pythondocs</a></li>
  <li><a href="http://flink.apache.org"><i class="fa fa-external-link title" aria-hidden="true"></i> Project Page</a></li>
</ul>

<div class="sidenav-search-box">
  <form class="navbar-form" role="search" action="//ci.apache.org/projects/flink/flink-docs-release-1.12/search-results.html">
    <div class="form-group">
      <input type="text" class="form-control" size="16px" name="q" placeholder="Search">
    </div>
    <button type="submit" class="btn btn-default">Go</button>
  </form>
</div>

<div class="sidenav-versions">
  <div class="dropdown">
    <button class="btn btn-default dropdown-toggle" type="button" data-toggle="dropdown">选择文档版本<span class="caret"></span></button>
    <ul class="dropdown-menu">
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.11">v1.11</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.10">v1.10</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.9">v1.9</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.8">v1.8</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.7">v1.7</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.6">v1.6</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.5">v1.5</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.4">v1.4</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.3">v1.3</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.2">v1.2</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.1">v1.1</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.0">v1.0</a></li>
    </ul>
  </div>
</div>

<div class="sidenav-languages"><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connect.html">
      <button type="submit" class="btn btn-default">English</button>
    </a>
</div>

        </div>
        <div class="col-lg-9 content" id="contentcol">

          

<ol class="breadcrumb">
  
    <li class="active">Table API Legacy Connectors</li>
</ol>

<h1>Table API Legacy Connectors</h1>




<p>Flink’s Table API &amp; SQL programs can be connected to other external systems for reading and writing both batch and streaming tables. A table source provides access to data which is stored in external systems (such as a database, key-value store, message queue, or file system). A table sink emits a table to an external storage system. Depending on the type of source and sink, they support different formats such as CSV, Parquet, or ORC.</p>

<p>This page describes how to declare built-in table sources and/or table sinks and register them in Flink. After a source or sink has been registered, it can be accessed by Table API &amp; SQL statements.</p>

<p><span class="label label-danger">Attention</span> If you want to implement your own <em>custom</em> table source or sink, have a look at the <a href="sourceSinks.html">user-defined sources &amp; sinks page</a>.</p>

<ul id="markdown-toc">
  <li><a href="#dependencies" id="markdown-toc-dependencies">Dependencies</a>    <ul>
      <li><a href="#connectors" id="markdown-toc-connectors">Connectors</a></li>
      <li><a href="#formats" id="markdown-toc-formats">Formats</a></li>
    </ul>
  </li>
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#table-schema" id="markdown-toc-table-schema">Table Schema</a>    <ul>
      <li><a href="#rowtime-attributes" id="markdown-toc-rowtime-attributes">Rowtime Attributes</a></li>
      <li><a href="#type-strings" id="markdown-toc-type-strings">Type Strings</a></li>
    </ul>
  </li>
  <li><a href="#update-modes" id="markdown-toc-update-modes">Update Modes</a></li>
  <li><a href="#table-connectors" id="markdown-toc-table-connectors">Table Connectors</a>    <ul>
      <li><a href="#file-system-connector" id="markdown-toc-file-system-connector">File System Connector</a></li>
      <li><a href="#kafka-connector" id="markdown-toc-kafka-connector">Kafka Connector</a></li>
      <li><a href="#elasticsearch-connector" id="markdown-toc-elasticsearch-connector">Elasticsearch Connector</a></li>
      <li><a href="#hbase-connector" id="markdown-toc-hbase-connector">HBase Connector</a></li>
      <li><a href="#jdbc-connector" id="markdown-toc-jdbc-connector">JDBC Connector</a></li>
      <li><a href="#hive-connector" id="markdown-toc-hive-connector">Hive Connector</a></li>
    </ul>
  </li>
  <li><a href="#table-formats" id="markdown-toc-table-formats">Table Formats</a>    <ul>
      <li><a href="#csv-format" id="markdown-toc-csv-format">CSV Format</a></li>
      <li><a href="#json-format" id="markdown-toc-json-format">JSON Format</a></li>
      <li><a href="#apache-avro-format" id="markdown-toc-apache-avro-format">Apache Avro Format</a></li>
      <li><a href="#old-csv-format" id="markdown-toc-old-csv-format">Old CSV Format</a></li>
    </ul>
  </li>
  <li><a href="#further-tablesources-and-tablesinks" id="markdown-toc-further-tablesources-and-tablesinks">Further TableSources and TableSinks</a>    <ul>
      <li><a href="#orctablesource" id="markdown-toc-orctablesource">OrcTableSource</a></li>
      <li><a href="#csvtablesink" id="markdown-toc-csvtablesink">CsvTableSink</a></li>
      <li><a href="#cassandraappendtablesink" id="markdown-toc-cassandraappendtablesink">CassandraAppendTableSink</a></li>
    </ul>
  </li>
</ul>

<h2 id="dependencies">Dependencies</h2>

<p>The following tables list all available connectors and formats. Their mutual compatibility is tagged in the corresponding sections for <a href="connect.html#table-connectors">table connectors</a> and <a href="connect.html#table-formats">table formats</a>. The following tables provide dependency information for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</p>

<h3 id="connectors">Connectors</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th style="text-align: left">Version</th>
      <th style="text-align: left">Maven dependency</th>
      <th style="text-align: left">SQL Client JAR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Filesystem</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Built-in</td>
      <td style="text-align: left">Built-in</td>
    </tr>
    <tr>
      <td style="text-align: left">Elasticsearch</td>
      <td style="text-align: left">6</td>
      <td style="text-align: left"><code class="highlighter-rouge">flink-connector-elasticsearch6</code></td>
      <td style="text-align: left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-elasticsearch6_2.11/1.12.0/flink-sql-connector-elasticsearch6_2.11-1.12.0.jar">Download</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Elasticsearch</td>
      <td style="text-align: left">7</td>
      <td style="text-align: left"><code class="highlighter-rouge">flink-connector-elasticsearch7</code></td>
      <td style="text-align: left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-elasticsearch7_2.11/1.12.0/flink-sql-connector-elasticsearch7_2.11-1.12.0.jar">Download</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Apache Kafka</td>
      <td style="text-align: left">0.11+ (<code class="highlighter-rouge">universal</code>)</td>
      <td style="text-align: left"><code class="highlighter-rouge">flink-connector-kafka</code></td>
      <td style="text-align: left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.11/1.12.0/flink-sql-connector-kafka_2.11-1.12.0.jar">Download</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Apache HBase</td>
      <td style="text-align: left">1.4.3</td>
      <td style="text-align: left"><code class="highlighter-rouge">flink-connector-hbase</code></td>
      <td style="text-align: left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-hbase_2.11/1.12.0/flink-connector-hbase_2.11-1.12.0.jar">Download</a></td>
    </tr>
    <tr>
      <td style="text-align: left">JDBC</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"><code class="highlighter-rouge">flink-connector-jdbc</code></td>
      <td style="text-align: left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-jdbc_2.11/1.12.0/flink-connector-jdbc_2.11-1.12.0.jar">Download</a></td>
    </tr>
  </tbody>
</table>

<h3 id="formats">Formats</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th style="text-align: left">Maven dependency</th>
      <th style="text-align: left">SQL Client JAR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Old CSV (for files)</td>
      <td style="text-align: left">Built-in</td>
      <td style="text-align: left">Built-in</td>
    </tr>
    <tr>
      <td style="text-align: left">CSV (for Kafka)</td>
      <td style="text-align: left"><code class="highlighter-rouge">flink-csv</code></td>
      <td style="text-align: left">Built-in</td>
    </tr>
    <tr>
      <td style="text-align: left">JSON</td>
      <td style="text-align: left"><code class="highlighter-rouge">flink-json</code></td>
      <td style="text-align: left">Built-in</td>
    </tr>
    <tr>
      <td style="text-align: left">Apache Avro</td>
      <td style="text-align: left"><code class="highlighter-rouge">flink-avro</code></td>
      <td style="text-align: left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-avro/1.12.0/flink-avro-1.12.0-sql-jar.jar">Download</a></td>
    </tr>
  </tbody>
</table>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h2 id="overview">Overview</h2>

<p>Beginning from Flink 1.6, the declaration of a connection to an external system is separated from the actual implementation.</p>

<p>Connections can be specified either</p>

<ul>
  <li><strong>programmatically</strong> using a <code class="highlighter-rouge">Descriptor</code> under <code class="highlighter-rouge">org.apache.flink.table.descriptors</code> for Table &amp; SQL API</li>
  <li>or <strong>declaratively</strong> via <a href="http://yaml.org/">YAML configuration files</a> for the SQL Client.</li>
</ul>

<p>This allows not only for better unification of APIs and SQL Client but also for better extensibility in case of <a href="sourceSinks.html">custom implementations</a> without changing the actual declaration.</p>

<p>Every declaration is similar to a SQL <code class="highlighter-rouge">CREATE TABLE</code> statement. One can define the name of the table, the schema of the table, a connector, and a data format upfront for connecting to an external system.</p>

<p>The <strong>connector</strong> describes the external system that stores the data of a table. Storage systems such as <a href="http://kafka.apache.org/">Apache Kafka</a> or a regular file system can be declared here. The connector might already provide a fixed format.</p>

<p>Some systems support different <strong>data formats</strong>. For example, a table that is stored in Kafka or in files can encode its rows with CSV, JSON, or Avro. A database connector might need the table schema here. Whether or not a storage system requires the definition of a format, is documented for every <a href="connect.html#table-connectors">connector</a>. Different systems also require different <a href="connect.html#table-formats">types of formats</a> (e.g., column-oriented formats vs. row-oriented formats). The documentation states which format types and connectors are compatible.</p>

<p>The <strong>table schema</strong> defines the schema of a table that is exposed to SQL queries. It describes how a source maps the data format to the table schema and a sink vice versa. The schema has access to fields defined by the connector or format. It can use one or more fields for extracting or inserting <a href="streaming/time_attributes.html">time attributes</a>. If input fields have no deterministic field order, the schema clearly defines column names, their order, and origin.</p>

<p>The subsequent sections will cover each definition part (<a href="connect.html#table-connectors">connector</a>, <a href="connect.html#table-formats">format</a>, and <a href="connect.html#table-schema">schema</a>) in more detail. The following example shows how to pass them:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="n">tableEnvironment</span><span class="p">.</span><span class="n">executeSql</span><span class="p">(</span>
    <span class="nv">"CREATE TABLE MyTable (</span><span class="se">\n</span><span class="nv">"</span> <span class="o">+</span>
    <span class="nv">"  ...    -- declare table schema </span><span class="se">\n</span><span class="nv">"</span> <span class="o">+</span>
    <span class="nv">") WITH (</span><span class="se">\n</span><span class="nv">"</span> <span class="o">+</span>
    <span class="nv">"  'connector.type' = '...',  -- declare connector specific properties</span><span class="se">\n</span><span class="nv">"</span> <span class="o">+</span>
    <span class="nv">"  ...</span><span class="se">\n</span><span class="nv">"</span> <span class="o">+</span>
    <span class="nv">"  'update-mode' = 'append',  -- declare update mode</span><span class="se">\n</span><span class="nv">"</span> <span class="o">+</span>
    <span class="nv">"  'format.type' = '...',     -- declare format specific properties</span><span class="se">\n</span><span class="nv">"</span> <span class="o">+</span>
    <span class="nv">"  ...</span><span class="se">\n</span><span class="nv">"</span> <span class="o">+</span>
    <span class="nv">")"</span><span class="p">);</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">tableEnvironment</span>
  <span class="o">.</span><span class="na">connect</span><span class="o">(...)</span>
  <span class="o">.</span><span class="na">withFormat</span><span class="o">(...)</span>
  <span class="o">.</span><span class="na">withSchema</span><span class="o">(...)</span>
  <span class="o">.</span><span class="na">inAppendMode</span><span class="o">()</span>
  <span class="o">.</span><span class="na">createTemporaryTable</span><span class="o">(</span><span class="s">"MyTable"</span><span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">table_environment</span> \
    <span class="p">.</span><span class="n">connect</span><span class="p">(...)</span> \
    <span class="p">.</span><span class="n">with_format</span><span class="p">(...)</span> \
    <span class="p">.</span><span class="n">with_schema</span><span class="p">(...)</span> \
    <span class="p">.</span><span class="n">in_append_mode</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">create_temporary_table</span><span class="p">(</span><span class="s">"MyTable"</span><span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">name</span><span class="pi">:</span> <span class="s">MyTable</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">source</span>
<span class="na">update-mode</span><span class="pi">:</span> <span class="s">append</span>
<span class="na">connector</span><span class="pi">:</span> <span class="s">...</span>
<span class="na">format</span><span class="pi">:</span> <span class="s">...</span>
<span class="na">schema</span><span class="pi">:</span> <span class="s">...</span></code></pre></figure>

  </div>
</div>

<p>The table’s type (<code class="highlighter-rouge">source</code>, <code class="highlighter-rouge">sink</code>, or <code class="highlighter-rouge">both</code>) determines how a table is registered. In case of table type <code class="highlighter-rouge">both</code>, both a table source and table sink are registered under the same name. Logically, this means that we can both read and write to such a table similarly to a table in a regular DBMS.</p>

<p>For streaming queries, an <a href="connect.html#update-modes">update mode</a> declares how to communicate between a dynamic table and the storage system for continuous queries. The connector might already provide a default update mode, e.g. Kafka connector works in append mode by default.</p>

<p>The following code shows a full example of how to connect to Kafka for reading Json records.</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="c1">-- declare the schema of the table</span>
  <span class="nv">`user`</span> <span class="nb">BIGINT</span><span class="p">,</span>
  <span class="n">message</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">ts</span> <span class="n">STRING</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="c1">-- declare the external system to connect to</span>
  <span class="s1">'connector.type'</span> <span class="o">=</span> <span class="s1">'kafka'</span><span class="p">,</span>
  <span class="s1">'connector.version'</span> <span class="o">=</span> <span class="s1">'0.10'</span><span class="p">,</span>
  <span class="s1">'connector.topic'</span> <span class="o">=</span> <span class="s1">'topic_name'</span><span class="p">,</span>
  <span class="s1">'connector.startup-mode'</span> <span class="o">=</span> <span class="s1">'earliest-offset'</span><span class="p">,</span>
  <span class="s1">'connector.properties.bootstrap.servers'</span> <span class="o">=</span> <span class="s1">'localhost:9092'</span><span class="p">,</span>

  <span class="c1">-- declare a format for this system</span>
  <span class="s1">'format.type'</span> <span class="o">=</span> <span class="s1">'json'</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">tableEnvironment</span>
  <span class="c1">// declare the external system to connect to</span>
  <span class="o">.</span><span class="na">connect</span><span class="o">(</span>
    <span class="k">new</span> <span class="nf">Kafka</span><span class="o">()</span>
      <span class="o">.</span><span class="na">version</span><span class="o">(</span><span class="s">"0.10"</span><span class="o">)</span>
      <span class="o">.</span><span class="na">topic</span><span class="o">(</span><span class="s">"test-input"</span><span class="o">)</span>
      <span class="o">.</span><span class="na">startFromEarliest</span><span class="o">()</span>
      <span class="o">.</span><span class="na">property</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>
  <span class="o">)</span>

  <span class="c1">// declare a format for this system</span>
  <span class="o">.</span><span class="na">withFormat</span><span class="o">(</span>
    <span class="k">new</span> <span class="nf">Json</span><span class="o">()</span>
  <span class="o">)</span>

  <span class="c1">// declare the schema of the table</span>
  <span class="o">.</span><span class="na">withSchema</span><span class="o">(</span>
    <span class="k">new</span> <span class="nf">Schema</span><span class="o">()</span>
      <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"rowtime"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">TIMESTAMP</span><span class="o">(</span><span class="mi">3</span><span class="o">))</span>
        <span class="o">.</span><span class="na">rowtime</span><span class="o">(</span><span class="k">new</span> <span class="nc">Rowtime</span><span class="o">()</span>
          <span class="o">.</span><span class="na">timestampsFromField</span><span class="o">(</span><span class="s">"timestamp"</span><span class="o">)</span>
          <span class="o">.</span><span class="na">watermarksPeriodicBounded</span><span class="o">(</span><span class="mi">60000</span><span class="o">)</span>
        <span class="o">)</span>
      <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"user"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">BIGINT</span><span class="o">())</span>
      <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"message"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">STRING</span><span class="o">())</span>
  <span class="o">)</span>

  <span class="c1">// create a table with given name</span>
  <span class="o">.</span><span class="na">createTemporaryTable</span><span class="o">(</span><span class="s">"MyUserTable"</span><span class="o">);</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">table_environment</span> \
    <span class="p">.</span><span class="n">connect</span><span class="p">(</span>  <span class="c1"># declare the external system to connect to
</span>        <span class="n">Kafka</span><span class="p">()</span>
        <span class="p">.</span><span class="n">version</span><span class="p">(</span><span class="s">"0.10"</span><span class="p">)</span>
        <span class="p">.</span><span class="n">topic</span><span class="p">(</span><span class="s">"test-input"</span><span class="p">)</span>
        <span class="p">.</span><span class="n">start_from_earliest</span><span class="p">()</span>
        <span class="p">.</span><span class="nb">property</span><span class="p">(</span><span class="s">"bootstrap.servers"</span><span class="p">,</span> <span class="s">"localhost:9092"</span><span class="p">)</span>
    <span class="p">)</span> \
    <span class="p">.</span><span class="n">with_format</span><span class="p">(</span>  <span class="c1"># declare a format for this system
</span>        <span class="n">Json</span><span class="p">()</span>
    <span class="p">)</span> \
    <span class="p">.</span><span class="n">with_schema</span><span class="p">(</span>  <span class="c1"># declare the schema of the table
</span>        <span class="n">Schema</span><span class="p">()</span>
        <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"rowtime"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
        <span class="p">.</span><span class="n">rowtime</span><span class="p">(</span>
            <span class="n">Rowtime</span><span class="p">()</span>
            <span class="p">.</span><span class="n">timestamps_from_field</span><span class="p">(</span><span class="s">"timestamp"</span><span class="p">)</span>
            <span class="p">.</span><span class="n">watermarks_periodic_bounded</span><span class="p">(</span><span class="mi">60000</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"user"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">BIGINT</span><span class="p">())</span>
        <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"message"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">STRING</span><span class="p">())</span>
    <span class="p">)</span> \
    <span class="p">.</span><span class="n">create_temporary_table</span><span class="p">(</span><span class="s">"MyUserTable"</span><span class="p">)</span>
    <span class="c1"># register as source, sink, or both and under a name</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">tables</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MyUserTable</span>      <span class="c1"># name the new table</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">source</span>           <span class="c1"># declare if the table should be "source", "sink", or "both"</span>

    <span class="c1"># declare the external system to connect to</span>
    <span class="na">connector</span><span class="pi">:</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">kafka</span>
      <span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0.10"</span>
      <span class="na">topic</span><span class="pi">:</span> <span class="s">test-input</span>
      <span class="na">startup-mode</span><span class="pi">:</span> <span class="s">earliest-offset</span>
      <span class="na">properties</span><span class="pi">:</span>
        <span class="s">bootstrap.servers</span><span class="pi">:</span> <span class="s">localhost:9092</span>

    <span class="c1"># declare a format for this system</span>
    <span class="na">format</span><span class="pi">:</span>
      <span class="na">type</span><span class="pi">:</span> <span class="s">json</span>

    <span class="c1"># declare the schema of the table</span>
    <span class="na">schema</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">rowtime</span>
        <span class="na">data-type</span><span class="pi">:</span> <span class="s">TIMESTAMP(3)</span>
        <span class="na">rowtime</span><span class="pi">:</span>
          <span class="na">timestamps</span><span class="pi">:</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">from-field</span>
            <span class="na">from</span><span class="pi">:</span> <span class="s">ts</span>
          <span class="na">watermarks</span><span class="pi">:</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">periodic-bounded</span>
            <span class="na">delay</span><span class="pi">:</span> <span class="s2">"</span><span class="s">60000"</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">user</span>
        <span class="na">data-type</span><span class="pi">:</span> <span class="s">BIGINT</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">message</span>
        <span class="na">data-type</span><span class="pi">:</span> <span class="s">STRING</span></code></pre></figure>

  </div>
</div>

<p>In both ways the desired connection properties are converted into normalized, string-based key-value pairs. So-called <a href="sourceSinks.html#define-a-tablefactory">table factories</a> create configured table sources, table sinks, and corresponding formats from the key-value pairs. All table factories that can be found via Java’s <a href="https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html">Service Provider Interfaces (SPI)</a> are taken into account when searching for exactly-one matching table factory.</p>

<p>If no factory can be found or multiple factories match for the given properties, an exception will be thrown with additional information about considered factories and supported properties.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h2 id="table-schema">Table Schema</h2>

<p>The table schema defines the names and types of columns similar to the column definitions of a SQL <code class="highlighter-rouge">CREATE TABLE</code> statement. In addition, one can specify how columns are mapped from and to fields of the format in which the table data is encoded. The origin of a field might be important if the name of the column should differ from the input/output format. For instance, a column <code class="highlighter-rouge">user_name</code> should reference the field <code class="highlighter-rouge">$$-user-name</code> from a JSON format. Additionally, the schema is needed to map types from an external system to Flink’s representation. In case of a table sink, it ensures that only data with valid schema is written to an external system.</p>

<p>The following example shows a simple schema without time attributes and one-to-one field mapping of input/output to table columns.</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyTable</span> <span class="p">(</span>
  <span class="n">MyField1</span> <span class="nb">INT</span><span class="p">,</span>
  <span class="n">MyField2</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">MyField3</span> <span class="nb">BOOLEAN</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">withSchema</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Schema</span><span class="o">()</span>
    <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"MyField1"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">INT</span><span class="o">())</span>     <span class="c1">// required: specify the fields of the table (in this order)</span>
    <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"MyField2"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">STRING</span><span class="o">())</span>
    <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"MyField3"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">BOOLEAN</span><span class="o">())</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">with_schema</span><span class="p">(</span>
    <span class="n">Schema</span><span class="p">()</span>
    <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"MyField1"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">INT</span><span class="p">())</span>  <span class="c1"># required: specify the fields of the table (in this order)
</span>    <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"MyField2"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">STRING</span><span class="p">())</span>
    <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"MyField3"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">BOOLEAN</span><span class="p">())</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">schema</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MyField1</span>    <span class="c1"># required: specify the fields of the table (in this order)</span>
    <span class="na">data-type</span><span class="pi">:</span> <span class="s">INT</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MyField2</span>
    <span class="na">data-type</span><span class="pi">:</span> <span class="s">STRING</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MyField3</span>
    <span class="na">data-type</span><span class="pi">:</span> <span class="s">BOOLEAN</span></code></pre></figure>

  </div>
</div>

<p>In order to declare time attributes in the schema, the following ways are supported:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyTable</span> <span class="p">(</span>
  <span class="n">MyField1</span> <span class="k">AS</span> <span class="n">PROCTIME</span><span class="p">(),</span> <span class="c1">-- declares this field as a processing-time attribute</span>
  <span class="n">MyField2</span> <span class="nb">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
  <span class="n">mf3</span> <span class="nb">BOOLEAN</span><span class="p">,</span>
  <span class="n">MyField3</span> <span class="k">AS</span> <span class="n">mf3</span><span class="p">,</span>  <span class="c1">--  reference/alias an original field to a new field</span>
  <span class="c1">-- declares this MyField2 as a event-time attribute</span>
  <span class="n">WATERMARK</span> <span class="k">FOR</span> <span class="n">MyField2</span> <span class="k">AS</span> <span class="n">MyField2</span> <span class="o">-</span> <span class="n">INTERVAL</span> <span class="s1">'1'</span> <span class="k">SECOND</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">withSchema</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Schema</span><span class="o">()</span>
    <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"MyField1"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">TIMESTAMP</span><span class="o">(</span><span class="mi">3</span><span class="o">))</span>
      <span class="o">.</span><span class="na">proctime</span><span class="o">()</span>      <span class="c1">// optional: declares this field as a processing-time attribute</span>
    <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"MyField2"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">TIMESTAMP</span><span class="o">(</span><span class="mi">3</span><span class="o">))</span>
      <span class="o">.</span><span class="na">rowtime</span><span class="o">(...)</span>    <span class="c1">// optional: declares this field as a event-time attribute</span>
    <span class="o">.</span><span class="na">field</span><span class="o">(</span><span class="s">"MyField3"</span><span class="o">,</span> <span class="nc">DataTypes</span><span class="o">.</span><span class="na">BOOLEAN</span><span class="o">())</span>
      <span class="o">.</span><span class="na">from</span><span class="o">(</span><span class="s">"mf3"</span><span class="o">)</span>     <span class="c1">// optional: original field in the input that is referenced/aliased by this field</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">with_schema</span><span class="p">(</span>
    <span class="n">Schema</span><span class="p">()</span>
    <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"MyField1"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
      <span class="p">.</span><span class="n">proctime</span><span class="p">()</span>  <span class="c1"># optional: declares this field as a processing-time attribute
</span>    <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"MyField2"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
      <span class="p">.</span><span class="n">rowtime</span><span class="p">(...)</span>  <span class="c1"># optional: declares this field as a event-time attribute
</span>    <span class="p">.</span><span class="n">field</span><span class="p">(</span><span class="s">"MyField3"</span><span class="p">,</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">BOOLEAN</span><span class="p">())</span>
      <span class="p">.</span><span class="n">from_origin_field</span><span class="p">(</span><span class="s">"mf3"</span><span class="p">)</span>  <span class="c1"># optional: original field in the input that is referenced/aliased by this field
</span><span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">schema</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MyField1</span>
    <span class="na">data-type</span><span class="pi">:</span> <span class="s">TIMESTAMP(3)</span>
    <span class="na">proctime</span><span class="pi">:</span> <span class="no">true</span>    <span class="c1"># optional: boolean flag whether this field should be a processing-time attribute</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MyField2</span>
    <span class="na">data-type</span><span class="pi">:</span> <span class="s">TIMESTAMP(3)</span>
    <span class="na">rowtime</span><span class="pi">:</span> <span class="s">...</span>      <span class="c1"># optional: wether this field should be a event-time attribute</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">MyField3</span>
    <span class="na">data-type</span><span class="pi">:</span> <span class="s">BOOLEAN</span>
    <span class="na">from</span><span class="pi">:</span> <span class="s">mf3</span>         <span class="c1"># optional: original field in the input that is referenced/aliased by this field</span></code></pre></figure>

  </div>
</div>

<p>Time attributes are essential when working with unbounded streaming tables. Therefore both processing-time and event-time (also known as “rowtime”) attributes can be defined as part of the schema.</p>

<p>For more information about time handling in Flink and especially event-time, we recommend the general <a href="streaming/time_attributes.html">event-time section</a>.</p>

<h3 id="rowtime-attributes">Rowtime Attributes</h3>

<p>In order to control the event-time behavior for tables, Flink provides predefined timestamp extractors and watermark strategies.</p>

<p>Please refer to <a href="sql/create.html#create-table">CREATE TABLE statements</a> for more information about defining time attributes in DDL.</p>

<p>The following timestamp extractors are supported:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="c1">-- use the existing TIMESTAMP(3) field in schema as the rowtime attribute</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyTable</span> <span class="p">(</span>
  <span class="n">ts_field</span> <span class="nb">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
  <span class="n">WATERMARK</span> <span class="k">FOR</span> <span class="n">ts_field</span> <span class="k">AS</span> <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span>

<span class="c1">-- use system functions or UDFs or expressions to extract the expected TIMESTAMP(3) rowtime field</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyTable</span> <span class="p">(</span>
  <span class="n">log_ts</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">ts_field</span> <span class="k">AS</span> <span class="n">TO_TIMESTAMP</span><span class="p">(</span><span class="n">log_ts</span><span class="p">),</span>
  <span class="n">WATERMARK</span> <span class="k">FOR</span> <span class="n">ts_field</span> <span class="k">AS</span> <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span>

<span class="c1">-- NOTE: preserving assigned timestamp from the source as rowtime attribute is not supported in DDL currently.</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="c1">// Converts an existing LONG or SQL_TIMESTAMP field in the input into the rowtime attribute.</span>
<span class="o">.</span><span class="na">rowtime</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Rowtime</span><span class="o">()</span>
    <span class="o">.</span><span class="na">timestampsFromField</span><span class="o">(</span><span class="s">"ts_field"</span><span class="o">)</span>    <span class="c1">// required: original field name in the input</span>
<span class="o">)</span>

<span class="c1">// Converts the assigned timestamps from a DataStream API record into the rowtime attribute</span>
<span class="c1">// and thus preserves the assigned timestamps from the source.</span>
<span class="c1">// This requires a source that assigns timestamps (e.g., Kafka 0.10+).</span>
<span class="o">.</span><span class="na">rowtime</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Rowtime</span><span class="o">()</span>
    <span class="o">.</span><span class="na">timestampsFromSource</span><span class="o">()</span>
<span class="o">)</span>

<span class="c1">// Sets a custom timestamp extractor to be used for the rowtime attribute.</span>
<span class="c1">// The extractor must extend `org.apache.flink.table.sources.tsextractors.TimestampExtractor`.</span>
<span class="o">.</span><span class="na">rowtime</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Rowtime</span><span class="o">()</span>
    <span class="o">.</span><span class="na">timestampsFromExtractor</span><span class="o">(...)</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Converts an existing BIGINT or TIMESTAMP field in the input into the rowtime attribute.
</span><span class="p">.</span><span class="n">rowtime</span><span class="p">(</span>
    <span class="n">Rowtime</span><span class="p">()</span>
    <span class="p">.</span><span class="n">timestamps_from_field</span><span class="p">(</span><span class="s">"ts_field"</span><span class="p">)</span>  <span class="c1"># required: original field name in the input
</span><span class="p">)</span>

<span class="c1"># Converts the assigned timestamps into the rowtime attribute
# and thus preserves the assigned timestamps from the source.
# This requires a source that assigns timestamps (e.g., Kafka 0.10+).
</span><span class="p">.</span><span class="n">rowtime</span><span class="p">(</span>
    <span class="n">Rowtime</span><span class="p">()</span>
    <span class="p">.</span><span class="n">timestamps_from_source</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># Sets a custom timestamp extractor to be used for the rowtime attribute.
# The extractor must extend `org.apache.flink.table.sources.tsextractors.TimestampExtractor`.
# Due to python can not accept java object, so it requires a full-qualified class name of the extractor.
</span><span class="p">.</span><span class="n">rowtime</span><span class="p">(</span>
    <span class="n">Rowtime</span><span class="p">()</span>
    <span class="p">.</span><span class="n">timestamps_from_extractor</span><span class="p">(...)</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="c1"># Converts an existing BIGINT or TIMESTAMP field in the input into the rowtime attribute.</span>
<span class="na">rowtime</span><span class="pi">:</span>
  <span class="na">timestamps</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">from-field</span>
    <span class="na">from</span><span class="pi">:</span> <span class="s2">"</span><span class="s">ts_field"</span>                 <span class="c1"># required: original field name in the input</span>

<span class="c1"># Converts the assigned timestamps from a DataStream API record into the rowtime attribute</span>
<span class="c1"># and thus preserves the assigned timestamps from the source.</span>
<span class="na">rowtime</span><span class="pi">:</span>
  <span class="na">timestamps</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">from-source</span></code></pre></figure>

  </div>
</div>

<p>The following watermark strategies are supported:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="c1">-- Sets a watermark strategy for strictly ascending rowtime attributes. Emits a watermark of the</span>
<span class="c1">-- maximum observed timestamp so far. Rows that have a timestamp smaller to the max timestamp</span>
<span class="c1">-- are not late.</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyTable</span> <span class="p">(</span>
  <span class="n">ts_field</span> <span class="nb">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
  <span class="n">WATERMARK</span> <span class="k">FOR</span> <span class="n">ts_field</span> <span class="k">AS</span> <span class="n">ts_field</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span>

<span class="c1">-- Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum</span>
<span class="c1">-- observed timestamp so far minus 1. Rows that have a timestamp equal to the max timestamp</span>
<span class="c1">-- are not late.</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyTable</span> <span class="p">(</span>
  <span class="n">ts_field</span> <span class="nb">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
  <span class="n">WATERMARK</span> <span class="k">FOR</span> <span class="n">ts_field</span> <span class="k">AS</span> <span class="n">ts_field</span> <span class="o">-</span> <span class="n">INTERVAL</span> <span class="s1">'0.001'</span> <span class="k">SECOND</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span>

<span class="c1">-- Sets a watermark strategy for rowtime attributes which are out-of-order by a bounded time interval.</span>
<span class="c1">-- Emits watermarks which are the maximum observed timestamp minus the specified delay, e.g. 2 seconds.</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyTable</span> <span class="p">(</span>
  <span class="n">ts_field</span> <span class="nb">TIMESTAMP</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
  <span class="n">WATERMARK</span> <span class="k">FOR</span> <span class="n">ts_field</span> <span class="k">AS</span> <span class="n">ts_field</span> <span class="o">-</span> <span class="n">INTERVAL</span> <span class="s1">'2'</span> <span class="k">SECOND</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span>

<span class="c1">-- NOTE: preserving assigned watermark from the source is not supported in DDL currently.</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="c1">// Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum</span>
<span class="c1">// observed timestamp so far minus 1. Rows that have a timestamp equal to the max timestamp</span>
<span class="c1">// are not late.</span>
<span class="o">.</span><span class="na">rowtime</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Rowtime</span><span class="o">()</span>
    <span class="o">.</span><span class="na">watermarksPeriodicAscending</span><span class="o">()</span>
<span class="o">)</span>

<span class="c1">// Sets a built-in watermark strategy for rowtime attributes which are out-of-order by a bounded time interval.</span>
<span class="c1">// Emits watermarks which are the maximum observed timestamp minus the specified delay.</span>
<span class="o">.</span><span class="na">rowtime</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Rowtime</span><span class="o">()</span>
    <span class="o">.</span><span class="na">watermarksPeriodicBounded</span><span class="o">(</span><span class="mi">2000</span><span class="o">)</span>    <span class="c1">// delay in milliseconds</span>
<span class="o">)</span>

<span class="c1">// Sets a built-in watermark strategy which indicates the watermarks should be preserved from the</span>
<span class="c1">// underlying DataStream API and thus preserves the assigned watermarks from the source.</span>
<span class="o">.</span><span class="na">rowtime</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Rowtime</span><span class="o">()</span>
    <span class="o">.</span><span class="na">watermarksFromSource</span><span class="o">()</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum
# observed timestamp so far minus 1. Rows that have a timestamp equal to the max timestamp
# are not late.
</span><span class="p">.</span><span class="n">rowtime</span><span class="p">(</span>
    <span class="n">Rowtime</span><span class="p">()</span>
    <span class="p">.</span><span class="n">watermarks_periodic_ascending</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># Sets a built-in watermark strategy for rowtime attributes which are out-of-order by a bounded time interval.
# Emits watermarks which are the maximum observed timestamp minus the specified delay.
</span><span class="p">.</span><span class="n">rowtime</span><span class="p">(</span>
    <span class="n">Rowtime</span><span class="p">()</span>
    <span class="p">.</span><span class="n">watermarks_periodic_bounded</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>  <span class="c1"># delay in milliseconds
</span><span class="p">)</span>

<span class="c1"># Sets a built-in watermark strategy which indicates the watermarks should be preserved from the
# underlying DataStream API and thus preserves the assigned watermarks from the source.
</span><span class="p">.</span><span class="n">rowtime</span><span class="p">(</span>
    <span class="n">Rowtime</span><span class="p">()</span>
    <span class="p">.</span><span class="n">watermarks_from_source</span><span class="p">()</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="c1"># Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum</span>
<span class="c1"># observed timestamp so far minus 1. Rows that have a timestamp equal to the max timestamp</span>
<span class="c1"># are not late.</span>
<span class="na">rowtime</span><span class="pi">:</span>
  <span class="na">watermarks</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">periodic-ascending</span>

<span class="c1"># Sets a built-in watermark strategy for rowtime attributes which are out-of-order by a bounded time interval.</span>
<span class="c1"># Emits watermarks which are the maximum observed timestamp minus the specified delay.</span>
<span class="na">rowtime</span><span class="pi">:</span>
  <span class="na">watermarks</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">periodic-bounded</span>
    <span class="na">delay</span><span class="pi">:</span> <span class="s">...</span>                <span class="c1"># required: delay in milliseconds</span>

<span class="c1"># Sets a built-in watermark strategy which indicates the watermarks should be preserved from the</span>
<span class="c1"># underlying DataStream API and thus preserves the assigned watermarks from the source.</span>
<span class="na">rowtime</span><span class="pi">:</span>
  <span class="na">watermarks</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">from-source</span></code></pre></figure>

  </div>
</div>

<p>Make sure to always declare both timestamps and watermarks. Watermarks are required for triggering time-based operations.</p>

<h3 id="type-strings">Type Strings</h3>

<p>Because <code class="highlighter-rouge">DataType</code> is only available in a programming language, type strings are supported for being defined in a YAML file.
The type strings are the same to type declaration in SQL, please see the <a href="types.html">Data Types</a> page about how to declare a type in SQL.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h2 id="update-modes">Update Modes</h2>

<p>For streaming queries, it is required to declare how to perform the <a href="streaming/dynamic_tables.html#continuous-queries">conversion between a dynamic table and an external connector</a>. The <em>update mode</em> specifies which kind of messages should be exchanged with the external system:</p>

<p><strong>Append Mode:</strong> In append mode, a dynamic table and an external connector only exchange INSERT messages.</p>

<p><strong>Retract Mode:</strong> In retract mode, a dynamic table and an external connector exchange ADD and RETRACT messages. An INSERT change is encoded as an ADD message, a DELETE change as a RETRACT message, and an UPDATE change as a RETRACT message for the updated (previous) row and an ADD message for the updating (new) row. In this mode, a key must not be defined as opposed to upsert mode. However, every update consists of two messages which is less efficient.</p>

<p><strong>Upsert Mode:</strong> In upsert mode, a dynamic table and an external connector exchange UPSERT and DELETE messages. This mode requires a (possibly composite) unique key by which updates can be propagated. The external connector needs to be aware of the unique key attribute in order to apply messages correctly. INSERT and UPDATE changes are encoded as UPSERT messages. DELETE changes as DELETE messages. The main difference to a retract stream is that UPDATE changes are encoded with a single message and are therefore more efficient.</p>

<p><span class="label label-danger">Attention</span> The documentation of each connector states which update modes are supported.</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyTable</span> <span class="p">(</span>
 <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
 <span class="s1">'update-mode'</span> <span class="o">=</span> <span class="s1">'append'</span>  <span class="c1">-- otherwise: 'retract' or 'upsert'</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">connect</span><span class="o">(...)</span>
  <span class="o">.</span><span class="na">inAppendMode</span><span class="o">()</span>    <span class="c1">// otherwise: inUpsertMode() or inRetractMode()</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">connect</span><span class="p">(...)</span> \
    <span class="p">.</span><span class="n">in_append_mode</span><span class="p">()</span>  <span class="c1"># otherwise: in_upsert_mode() or in_retract_mode()</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">tables</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">...</span>
    <span class="na">update-mode</span><span class="pi">:</span> <span class="s">append</span>    <span class="c1"># otherwise: "retract" or "upsert"</span></code></pre></figure>

  </div>
</div>

<p>See also the <a href="streaming/dynamic_tables.html#continuous-queries">general streaming concepts documentation</a> for more information.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h2 id="table-connectors">Table Connectors</h2>

<p>Flink provides a set of connectors for connecting to external systems.</p>

<p>Please note that not all connectors are available in both batch and streaming yet. Furthermore, not every streaming connector supports every streaming mode. Therefore, each connector is tagged accordingly. A format tag indicates that the connector requires a certain type of format.</p>

<h3 id="file-system-connector">File System Connector</h3>

<p><span class="label label-primary">Source: Batch</span>
<span class="label label-primary">Source: Streaming Append Mode</span>
<span class="label label-primary">Sink: Batch</span>
<span class="label label-primary">Sink: Streaming Append Mode</span>
<span class="label label-info">Format: OldCsv-only</span></p>

<p>The file system connector allows for reading and writing from a local or distributed filesystem. A filesystem can be defined as:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'connector.type'</span> <span class="o">=</span> <span class="s1">'filesystem'</span><span class="p">,</span>                <span class="c1">-- required: specify to connector type</span>
  <span class="s1">'connector.path'</span> <span class="o">=</span> <span class="s1">'file:///path/to/whatever'</span><span class="p">,</span>  <span class="c1">-- required: path to a file or directory</span>
  <span class="s1">'format.type'</span> <span class="o">=</span> <span class="s1">'...'</span><span class="p">,</span>                          <span class="c1">-- required: file system connector requires to specify a format,</span>
  <span class="p">...</span>                                             <span class="c1">-- currently only 'csv' format is supported.</span>
                                                  <span class="c1">-- Please refer to old CSV format part of Table Formats</span>
                                                  <span class="c1">-- section for more details.</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">connect</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">FileSystem</span><span class="o">()</span>
    <span class="o">.</span><span class="na">path</span><span class="o">(</span><span class="s">"file:///path/to/whatever"</span><span class="o">)</span>    <span class="c1">// required: path to a file or directory</span>
<span class="o">)</span>
<span class="o">.</span><span class="na">withFormat</span><span class="o">(</span>                             <span class="c1">// required: file system connector requires to specify a format,</span>
  <span class="o">...</span>                                    <span class="c1">// currently only OldCsv format is supported.</span>
<span class="o">)</span>                                        <span class="c1">// Please refer to old CSV format part of Table Formats</span>
                                         <span class="c1">// section for more details.</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">connect</span><span class="p">(</span>
    <span class="n">FileSystem</span><span class="p">()</span>
    <span class="p">.</span><span class="n">path</span><span class="p">(</span><span class="s">"file:///path/to/whatever"</span><span class="p">)</span>  <span class="c1"># required: path to a file or directory
</span><span class="p">)</span>
<span class="p">.</span><span class="n">withFormat</span><span class="p">(</span>                           <span class="c1"># required: file system connector requires to specify a format,
</span>  <span class="p">...</span>                                  <span class="c1"># currently only OldCsv format is supported.
</span><span class="p">)</span>                                      <span class="c1"># Please refer to old CSV format part of Table Formats
</span>                                       <span class="c1"># section for more details.</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">connector</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">filesystem</span>
  <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">file:///path/to/whatever"</span>    <span class="c1"># required: path to a file or directory</span>
<span class="na">format</span><span class="pi">:</span>                               <span class="c1"># required: file system connector requires to specify a format,</span>
  <span class="s">...</span>                                 <span class="c1"># currently only 'csv' format is supported.</span>
                                      <span class="c1"># Please refer to old CSV format part of Table Formats</span>
                                      <span class="c1"># section for more details.</span></code></pre></figure>

  </div>
</div>

<p>The file system connector itself is included in Flink and does not require an additional dependency. A corresponding format needs to be specified for reading and writing rows from and to a file system.</p>

<p><span class="label label-danger">Attention</span> Make sure to include <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/internals/filesystems.html">Flink File System specific dependencies</a>.</p>

<p><span class="label label-danger">Attention</span> File system sources and sinks for streaming are only experimental. In the future, we will support actual streaming use cases, i.e., directory monitoring and bucket output.</p>

<h3 id="kafka-connector">Kafka Connector</h3>

<p><span class="label label-primary">Source: Streaming Append Mode</span>
<span class="label label-primary">Sink: Streaming Append Mode</span>
<span class="label label-info">Format: CSV, JSON, Avro</span></p>

<p>The Kafka connector allows for reading and writing from and to an Apache Kafka topic. It can be defined as follows:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'connector.type'</span> <span class="o">=</span> <span class="s1">'kafka'</span><span class="p">,</span>

  <span class="s1">'connector.version'</span> <span class="o">=</span> <span class="s1">'0.11'</span><span class="p">,</span>     <span class="c1">-- required: valid connector versions are</span>
                                    <span class="c1">-- "0.8", "0.9", "0.10", "0.11", and "universal"</span>

  <span class="s1">'connector.topic'</span> <span class="o">=</span> <span class="s1">'topic_name'</span><span class="p">,</span> <span class="c1">-- required: topic name from which the table is read</span>

  <span class="c1">-- required: specify the Kafka server connection string</span>
  <span class="s1">'connector.properties.bootstrap.servers'</span> <span class="o">=</span> <span class="s1">'localhost:9092'</span><span class="p">,</span>
  <span class="c1">-- required for Kafka source, optional for Kafka sink, specify consumer group</span>
  <span class="s1">'connector.properties.group.id'</span> <span class="o">=</span> <span class="s1">'testGroup'</span><span class="p">,</span>
  <span class="c1">-- optional: valid modes are "earliest-offset", "latest-offset", "group-offsets", "specific-offsets" or "timestamp"</span>
  <span class="s1">'connector.startup-mode'</span> <span class="o">=</span> <span class="s1">'earliest-offset'</span><span class="p">,</span>

  <span class="c1">-- optional: used in case of startup mode with specific offsets</span>
  <span class="s1">'connector.specific-offsets'</span> <span class="o">=</span> <span class="s1">'partition:0,offset:42;partition:1,offset:300'</span><span class="p">,</span>

  <span class="c1">-- optional: used in case of startup mode with timestamp</span>
  <span class="s1">'connector.startup-timestamp-millis'</span> <span class="o">=</span> <span class="s1">'1578538374471'</span><span class="p">,</span>

  <span class="s1">'connector.sink-partitioner'</span> <span class="o">=</span> <span class="s1">'...'</span><span class="p">,</span>  <span class="c1">-- optional: output partitioning from Flink's partitions</span>
                                         <span class="c1">-- into Kafka's partitions valid are "fixed"</span>
                                         <span class="c1">-- (each Flink partition ends up in at most one Kafka partition),</span>
                                         <span class="c1">-- "round-robin" (a Flink partition is distributed to</span>
                                         <span class="c1">-- Kafka partitions round-robin)</span>
                                         <span class="c1">-- "custom" (use a custom FlinkKafkaPartitioner subclass)</span>

  <span class="c1">-- optional: used in case of sink partitioner custom</span>
  <span class="s1">'connector.sink-partitioner-class'</span> <span class="o">=</span> <span class="s1">'org.mycompany.MyPartitioner'</span><span class="p">,</span>

  <span class="s1">'format.type'</span> <span class="o">=</span> <span class="s1">'...'</span><span class="p">,</span>                 <span class="c1">-- required: Kafka connector requires to specify a format,</span>
  <span class="p">...</span>                                    <span class="c1">-- the supported formats are 'csv', 'json' and 'avro'.</span>
                                         <span class="c1">-- Please refer to Table Formats section for more details.</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">connect</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Kafka</span><span class="o">()</span>
    <span class="o">.</span><span class="na">version</span><span class="o">(</span><span class="s">"0.11"</span><span class="o">)</span>    <span class="c1">// required: valid connector versions are</span>
                        <span class="c1">//   "0.8", "0.9", "0.10", "0.11", and "universal"</span>
    <span class="o">.</span><span class="na">topic</span><span class="o">(</span><span class="s">"..."</span><span class="o">)</span>       <span class="c1">// required: topic name from which the table is read</span>

    <span class="c1">// optional: connector specific properties</span>
    <span class="o">.</span><span class="na">property</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>
    <span class="o">.</span><span class="na">property</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"testGroup"</span><span class="o">)</span>

    <span class="c1">// optional: select a startup mode for Kafka offsets</span>
    <span class="o">.</span><span class="na">startFromEarliest</span><span class="o">()</span>
    <span class="o">.</span><span class="na">startFromLatest</span><span class="o">()</span>
    <span class="o">.</span><span class="na">startFromSpecificOffsets</span><span class="o">(...)</span>
    <span class="o">.</span><span class="na">startFromTimestamp</span><span class="o">(...)</span>

    <span class="c1">// optional: output partitioning from Flink's partitions into Kafka's partitions</span>
    <span class="o">.</span><span class="na">sinkPartitionerFixed</span><span class="o">()</span>         <span class="c1">// each Flink partition ends up in at-most one Kafka partition (default)</span>
    <span class="o">.</span><span class="na">sinkPartitionerRoundRobin</span><span class="o">()</span>    <span class="c1">// a Flink partition is distributed to Kafka partitions round-robin</span>
    <span class="o">.</span><span class="na">sinkPartitionerCustom</span><span class="o">(</span><span class="nc">MyCustom</span><span class="o">.</span><span class="na">class</span><span class="o">)</span>    <span class="c1">// use a custom FlinkKafkaPartitioner subclass</span>
<span class="o">)</span>
<span class="o">.</span><span class="na">withFormat</span><span class="o">(</span>                                  <span class="c1">// required: Kafka connector requires to specify a format,</span>
  <span class="o">...</span>                                         <span class="c1">// the supported formats are Csv, Json and Avro.</span>
<span class="o">)</span>                                             <span class="c1">// Please refer to Table Formats section for more details.</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">connect</span><span class="p">(</span>
    <span class="n">Kafka</span><span class="p">()</span>
    <span class="p">.</span><span class="n">version</span><span class="p">(</span><span class="s">"0.11"</span><span class="p">)</span>  <span class="c1"># required: valid connector versions are
</span>                      <span class="c1"># "0.8", "0.9", "0.10", "0.11", and "universal"
</span>    <span class="p">.</span><span class="n">topic</span><span class="p">(</span><span class="s">"..."</span><span class="p">)</span>     <span class="c1"># required: topic name from which the table is read
</span>
    <span class="c1"># optional: connector specific properties
</span>    <span class="p">.</span><span class="nb">property</span><span class="p">(</span><span class="s">"bootstrap.servers"</span><span class="p">,</span> <span class="s">"localhost:9092"</span><span class="p">)</span>
    <span class="p">.</span><span class="nb">property</span><span class="p">(</span><span class="s">"group.id"</span><span class="p">,</span> <span class="s">"testGroup"</span><span class="p">)</span>

    <span class="c1"># optional: select a startup mode for Kafka offsets
</span>    <span class="p">.</span><span class="n">start_from_earliest</span><span class="p">()</span>
    <span class="p">.</span><span class="n">start_from_latest</span><span class="p">()</span>
    <span class="p">.</span><span class="n">start_from_specific_offsets</span><span class="p">(...)</span>
    <span class="p">.</span><span class="n">start_from_timestamp</span><span class="p">(...)</span>

    <span class="c1"># optional: output partitioning from Flink's partitions into Kafka's partitions
</span>    <span class="p">.</span><span class="n">sink_partitioner_fixed</span><span class="p">()</span>        <span class="c1"># each Flink partition ends up in at-most one Kafka partition (default)
</span>    <span class="p">.</span><span class="n">sink_partitioner_round_robin</span><span class="p">()</span>  <span class="c1"># a Flink partition is distributed to Kafka partitions round-robin
</span>    <span class="p">.</span><span class="n">sink_partitioner_custom</span><span class="p">(</span><span class="s">"full.qualified.custom.class.name"</span><span class="p">)</span>  <span class="c1"># use a custom FlinkKafkaPartitioner subclass
</span><span class="p">)</span>
<span class="p">.</span><span class="n">withFormat</span><span class="p">(</span>                         <span class="c1"># required: Kafka connector requires to specify a format,
</span>  <span class="p">...</span>                                <span class="c1"># the supported formats are Csv, Json and Avro.
</span><span class="p">)</span>                                    <span class="c1"># Please refer to Table Formats section for more details.</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">connector</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">kafka</span>
  <span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0.11"</span>     <span class="c1"># required: valid connector versions are</span>
                      <span class="c1">#   "0.8", "0.9", "0.10", "0.11", and "universal"</span>
  <span class="na">topic</span><span class="pi">:</span> <span class="s">...</span>          <span class="c1"># required: topic name from which the table is read</span>

  <span class="na">properties</span><span class="pi">:</span>
    <span class="s">bootstrap.servers</span><span class="pi">:</span> <span class="s">localhost:9092</span>  <span class="c1"># required: specify the Kafka server connection string</span>
    <span class="s">group.id</span><span class="pi">:</span> <span class="s">testGroup</span>                <span class="c1"># optional: required in Kafka consumer, specify consumer group</span>

  <span class="na">startup-mode</span><span class="pi">:</span> <span class="s">...</span>                                               <span class="c1"># optional: valid modes are "earliest-offset", "latest-offset",</span>
                                                                  <span class="c1"># "group-offsets", "specific-offsets" or "timestamp"</span>
  <span class="na">specific-offsets</span><span class="pi">:</span> <span class="s">partition:0,offset:42;partition:1,offset:300</span>  <span class="c1"># optional: used in case of startup mode with specific offsets</span>
  <span class="na">startup-timestamp-millis</span><span class="pi">:</span> <span class="m">1578538374471</span>                         <span class="c1"># optional: used in case of startup mode with timestamp</span>

  <span class="na">sink-partitioner</span><span class="pi">:</span> <span class="s">...</span>    <span class="c1"># optional: output partitioning from Flink's partitions into Kafka's partitions</span>
                           <span class="c1"># valid are "fixed" (each Flink partition ends up in at most one Kafka partition),</span>
                           <span class="c1"># "round-robin" (a Flink partition is distributed to Kafka partitions round-robin)</span>
                           <span class="c1"># "custom" (use a custom FlinkKafkaPartitioner subclass)</span>
  <span class="na">sink-partitioner-class</span><span class="pi">:</span> <span class="s">org.mycompany.MyPartitioner</span>  <span class="c1"># optional: used in case of sink partitioner custom</span>

  <span class="na">format</span><span class="pi">:</span>                  <span class="c1"># required: Kafka connector requires to specify a format,</span>
    <span class="s">...</span>                    <span class="c1"># the supported formats are "csv", "json" and "avro".</span>
                           <span class="c1"># Please refer to Table Formats section for more details.</span></code></pre></figure>

  </div>
</div>

<p><strong>Specify the start reading position:</strong> By default, the Kafka source will start reading data from the committed group offsets in Zookeeper or Kafka brokers. You can specify other start positions, which correspond to the configurations in section <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html#kafka-consumers-start-position-configuration">Kafka Consumers Start Position Configuration</a>.</p>

<p><strong>Flink-Kafka Sink Partitioning:</strong> By default, a Kafka sink writes to at most as many partitions as its own parallelism (each parallel instance of the sink writes to exactly one partition). In order to distribute the writes to more partitions or control the routing of rows into partitions, a custom sink partitioner can be provided. The round-robin partitioner is useful to avoid an unbalanced partitioning. However, it will cause a lot of network connections between all the Flink instances and all the Kafka brokers.</p>

<p><strong>Consistency guarantees:</strong> By default, a Kafka sink ingests data with at-least-once guarantees into a Kafka topic if the query is executed with <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/checkpointing.html#enabling-and-configuring-checkpointing">checkpointing enabled</a>.</p>

<p><strong>Kafka 0.10+ Timestamps:</strong> Since Kafka 0.10, Kafka messages have a timestamp as metadata that specifies when the record was written into the Kafka topic. These timestamps can be used for a <a href="connect.html#defining-the-schema">rowtime attribute</a> by selecting <code class="highlighter-rouge">timestamps: from-source</code> in YAML and <code class="highlighter-rouge">timestampsFromSource()</code> in Java/Scala respectively.</p>

<p><strong>Kafka 0.11+ Versioning:</strong> Since Flink 1.7, the Kafka connector definition should be independent of a hard-coded Kafka version. Use the connector version <code class="highlighter-rouge">universal</code> as a wildcard for Flink’s Kafka connector that is compatible with all Kafka versions starting from 0.11.</p>

<p>Make sure to add the version-specific Kafka dependency. In addition, a corresponding format needs to be specified for reading and writing rows from and to Kafka.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="elasticsearch-connector">Elasticsearch Connector</h3>

<p><span class="label label-primary">Sink: Streaming Append Mode</span>
<span class="label label-primary">Sink: Streaming Upsert Mode</span>
<span class="label label-info">Format: JSON-only</span></p>

<p>The Elasticsearch connector allows for writing into an index of the Elasticsearch search engine.</p>

<p>The connector can operate in <a href="#update-modes">upsert mode</a> for exchanging UPSERT/DELETE messages with the external system using a <a href="./streaming/dynamic_tables.html#table-to-stream-conversion">key defined by the query</a>.</p>

<p>For append-only queries, the connector can also operate in <a href="#update-modes">append mode</a> for exchanging only INSERT messages with the external system. If no key is defined by the query, a key is automatically generated by Elasticsearch.</p>

<p>The connector can be defined as follows:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'connector.type'</span> <span class="o">=</span> <span class="s1">'elasticsearch'</span><span class="p">,</span> <span class="c1">-- required: specify this table type is elasticsearch</span>

  <span class="s1">'connector.version'</span> <span class="o">=</span> <span class="s1">'6'</span><span class="p">,</span>          <span class="c1">-- required: valid connector versions are "6"</span>

  <span class="s1">'connector.hosts'</span> <span class="o">=</span> <span class="s1">'http://host_name:9092;http://host_name:9093'</span><span class="p">,</span>  <span class="c1">-- required: one or more Elasticsearch hosts to connect to</span>

  <span class="s1">'connector.index'</span> <span class="o">=</span> <span class="s1">'myusers'</span><span class="p">,</span>       <span class="c1">-- required: Elasticsearch index. Flink supports both static index and dynamic index.</span>
                                       <span class="c1">-- If you want to have a static index, this option value should be a plain string,</span>
                                       <span class="c1">-- e.g. 'myusers', all the records will be consistently written into "myusers" index.</span>
                                       <span class="c1">-- If you want to have a dynamic index, you can use '{field_name}' to reference a field</span>
                                       <span class="c1">-- value in the record to dynamically generate a target index. You can also use</span>
                                       <span class="c1">-- '{field_name|date_format_string}' to convert a field value of TIMESTAMP/DATE/TIME type</span>
                                       <span class="c1">-- into the format specified by date_format_string. The date_format_string is</span>
                                       <span class="c1">-- compatible with Java's [DateTimeFormatter](https://docs.oracle.com/javase/8/docs/api/index.html).</span>
                                       <span class="c1">-- For example, if the option value is 'myusers-{log_ts|yyyy-MM-dd}', then a</span>
                                       <span class="c1">-- record with log_ts field value 2020-03-27 12:25:55 will be written into</span>
                                       <span class="c1">-- "myusers-2020-03-27" index.</span>

  <span class="s1">'connector.document-type'</span> <span class="o">=</span> <span class="s1">'user'</span><span class="p">,</span>  <span class="c1">-- required: Elasticsearch document type</span>

  <span class="s1">'update-mode'</span> <span class="o">=</span> <span class="s1">'append'</span><span class="p">,</span>            <span class="c1">-- optional: update mode when used as table sink.</span>

  <span class="s1">'connector.key-delimiter'</span> <span class="o">=</span> <span class="s1">'$'</span><span class="p">,</span>     <span class="c1">-- optional: delimiter for composite keys ("_" by default)</span>
                                       <span class="c1">-- e.g., "$" would result in IDs "KEY1$KEY2$KEY3"</span>

  <span class="s1">'connector.key-null-literal'</span> <span class="o">=</span> <span class="s1">'n/a'</span><span class="p">,</span>  <span class="c1">-- optional: representation for null fields in keys ("null" by default)</span>

  <span class="s1">'connector.failure-handler'</span> <span class="o">=</span> <span class="s1">'...'</span><span class="p">,</span>   <span class="c1">-- optional: failure handling strategy in case a request to</span>
                                         <span class="c1">-- Elasticsearch fails ("fail" by default).</span>
                                         <span class="c1">-- valid strategies are</span>
                                         <span class="c1">-- "fail" (throws an exception if a request fails and</span>
                                         <span class="c1">-- thus causes a job failure),</span>
                                         <span class="c1">-- "ignore" (ignores failures and drops the request),</span>
                                         <span class="c1">-- "retry-rejected" (re-adds requests that have failed due</span>
                                         <span class="c1">-- to queue capacity saturation),</span>
                                         <span class="c1">-- or "custom" for failure handling with a</span>
                                         <span class="c1">-- ActionRequestFailureHandler subclass</span>

  <span class="c1">-- optional: configure how to buffer elements before sending them in bulk to the cluster for efficiency</span>
  <span class="s1">'connector.flush-on-checkpoint'</span> <span class="o">=</span> <span class="s1">'true'</span><span class="p">,</span>   <span class="c1">-- optional: disables flushing on checkpoint (see notes below!)</span>
                                              <span class="c1">-- ("true" by default)</span>
  <span class="s1">'connector.bulk-flush.max-actions'</span> <span class="o">=</span> <span class="s1">'42'</span><span class="p">,</span>  <span class="c1">-- optional: maximum number of actions to buffer</span>
                                              <span class="c1">-- for each bulk request</span>
  <span class="s1">'connector.bulk-flush.max-size'</span> <span class="o">=</span> <span class="s1">'42 mb'</span><span class="p">,</span>  <span class="c1">-- optional: maximum size of buffered actions in bytes</span>
                                              <span class="c1">-- per bulk request</span>
                                              <span class="c1">-- (only MB granularity is supported)</span>
  <span class="s1">'connector.bulk-flush.interval'</span> <span class="o">=</span> <span class="s1">'60000'</span><span class="p">,</span>  <span class="c1">-- optional: bulk flush interval (in milliseconds)</span>
  <span class="s1">'connector.bulk-flush.backoff.type'</span> <span class="o">=</span> <span class="s1">'...'</span><span class="p">,</span>       <span class="c1">-- optional: backoff strategy ("disabled" by default)</span>
                                                      <span class="c1">-- valid strategies are "disabled", "constant",</span>
                                                      <span class="c1">-- or "exponential"</span>
  <span class="s1">'connector.bulk-flush.backoff.max-retries'</span> <span class="o">=</span> <span class="s1">'3'</span><span class="p">,</span>  <span class="c1">-- optional: maximum number of retries</span>
  <span class="s1">'connector.bulk-flush.backoff.delay'</span> <span class="o">=</span> <span class="s1">'30000'</span><span class="p">,</span>    <span class="c1">-- optional: delay between each backoff attempt</span>
                                                      <span class="c1">-- (in milliseconds)</span>

  <span class="c1">-- optional: connection properties to be used during REST communication to Elasticsearch</span>
  <span class="s1">'connector.connection-max-retry-timeout'</span> <span class="o">=</span> <span class="s1">'3'</span><span class="p">,</span>     <span class="c1">-- optional: maximum timeout (in milliseconds)</span>
                                                      <span class="c1">-- between retries</span>
  <span class="s1">'connector.connection-path-prefix'</span> <span class="o">=</span> <span class="s1">'/v1'</span>          <span class="c1">-- optional: prefix string to be added to every</span>
                                                      <span class="c1">-- REST communication</span>

  <span class="s1">'format.type'</span> <span class="o">=</span> <span class="s1">'...'</span><span class="p">,</span>   <span class="c1">-- required: Elasticsearch connector requires to specify a format,</span>
  <span class="p">...</span>                      <span class="c1">-- currently only 'json' format is supported.</span>
                           <span class="c1">-- Please refer to Table Formats section for more details.</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">connect</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Elasticsearch</span><span class="o">()</span>
    <span class="o">.</span><span class="na">version</span><span class="o">(</span><span class="s">"6"</span><span class="o">)</span>                      <span class="c1">// required: valid connector versions are "6"</span>
    <span class="o">.</span><span class="na">host</span><span class="o">(</span><span class="s">"localhost"</span><span class="o">,</span> <span class="mi">9200</span><span class="o">,</span> <span class="s">"http"</span><span class="o">)</span>   <span class="c1">// required: one or more Elasticsearch hosts to connect to</span>
    <span class="o">.</span><span class="na">index</span><span class="o">(</span><span class="s">"MyUsers"</span><span class="o">)</span>                  <span class="c1">// required: Elasticsearch index</span>
    <span class="o">.</span><span class="na">documentType</span><span class="o">(</span><span class="s">"user"</span><span class="o">)</span>              <span class="c1">// required: Elasticsearch document type</span>

    <span class="o">.</span><span class="na">keyDelimiter</span><span class="o">(</span><span class="s">"$"</span><span class="o">)</span>        <span class="c1">// optional: delimiter for composite keys ("_" by default)</span>
                              <span class="c1">//   e.g., "$" would result in IDs "KEY1$KEY2$KEY3"</span>
    <span class="o">.</span><span class="na">keyNullLiteral</span><span class="o">(</span><span class="s">"n/a"</span><span class="o">)</span>    <span class="c1">// optional: representation for null fields in keys ("null" by default)</span>

    <span class="c1">// optional: failure handling strategy in case a request to Elasticsearch fails (fail by default)</span>
    <span class="o">.</span><span class="na">failureHandlerFail</span><span class="o">()</span>          <span class="c1">// optional: throws an exception if a request fails and causes a job failure</span>
    <span class="o">.</span><span class="na">failureHandlerIgnore</span><span class="o">()</span>        <span class="c1">//   or ignores failures and drops the request</span>
    <span class="o">.</span><span class="na">failureHandlerRetryRejected</span><span class="o">()</span> <span class="c1">//   or re-adds requests that have failed due to queue capacity saturation</span>
    <span class="o">.</span><span class="na">failureHandlerCustom</span><span class="o">(...)</span>     <span class="c1">//   or custom failure handling with a ActionRequestFailureHandler subclass</span>

    <span class="c1">// optional: configure how to buffer elements before sending them in bulk to the cluster for efficiency</span>
    <span class="o">.</span><span class="na">disableFlushOnCheckpoint</span><span class="o">()</span>    <span class="c1">// optional: disables flushing on checkpoint (see notes below!)</span>
    <span class="o">.</span><span class="na">bulkFlushMaxActions</span><span class="o">(</span><span class="mi">42</span><span class="o">)</span>       <span class="c1">// optional: maximum number of actions to buffer for each bulk request</span>
    <span class="o">.</span><span class="na">bulkFlushMaxSize</span><span class="o">(</span><span class="s">"42 mb"</span><span class="o">)</span>     <span class="c1">// optional: maximum size of buffered actions in bytes per bulk request</span>
                                   <span class="c1">//   (only MB granularity is supported)</span>
    <span class="o">.</span><span class="na">bulkFlushInterval</span><span class="o">(</span><span class="mi">60000L</span><span class="o">)</span>     <span class="c1">// optional: bulk flush interval (in milliseconds)</span>

    <span class="o">.</span><span class="na">bulkFlushBackoffConstant</span><span class="o">()</span>    <span class="c1">// optional: use a constant backoff type</span>
    <span class="o">.</span><span class="na">bulkFlushBackoffExponential</span><span class="o">()</span> <span class="c1">//   or use an exponential backoff type</span>
    <span class="o">.</span><span class="na">bulkFlushBackoffMaxRetries</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span> <span class="c1">// optional: maximum number of retries</span>
    <span class="o">.</span><span class="na">bulkFlushBackoffDelay</span><span class="o">(</span><span class="mi">30000L</span><span class="o">)</span> <span class="c1">// optional: delay between each backoff attempt (in milliseconds)</span>

    <span class="c1">// optional: connection properties to be used during REST communication to Elasticsearch</span>
    <span class="o">.</span><span class="na">connectionMaxRetryTimeout</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>  <span class="c1">// optional: maximum timeout (in milliseconds) between retries</span>
    <span class="o">.</span><span class="na">connectionPathPrefix</span><span class="o">(</span><span class="s">"/v1"</span><span class="o">)</span>   <span class="c1">// optional: prefix string to be added to every REST communication</span>
<span class="o">)</span>
<span class="o">.</span><span class="na">withFormat</span><span class="o">(</span>                      <span class="c1">// required: Elasticsearch connector requires to specify a format,</span>
  <span class="o">...</span>                             <span class="c1">// currently only Json format is supported.</span>
                                  <span class="c1">// Please refer to Table Formats section for more details.</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">connect</span><span class="p">(</span>
    <span class="n">Elasticsearch</span><span class="p">()</span>
    <span class="p">.</span><span class="n">version</span><span class="p">(</span><span class="s">"6"</span><span class="p">)</span>                      <span class="c1"># required: valid connector versions are "6"
</span>    <span class="p">.</span><span class="n">host</span><span class="p">(</span><span class="s">"localhost"</span><span class="p">,</span> <span class="mi">9200</span><span class="p">,</span> <span class="s">"http"</span><span class="p">)</span>   <span class="c1"># required: one or more Elasticsearch hosts to connect to
</span>    <span class="p">.</span><span class="n">index</span><span class="p">(</span><span class="s">"MyUsers"</span><span class="p">)</span>                  <span class="c1"># required: Elasticsearch index
</span>    <span class="p">.</span><span class="n">document_type</span><span class="p">(</span><span class="s">"user"</span><span class="p">)</span>             <span class="c1"># required: Elasticsearch document type
</span>
    <span class="p">.</span><span class="n">key_delimiter</span><span class="p">(</span><span class="s">"$"</span><span class="p">)</span>       <span class="c1"># optional: delimiter for composite keys ("_" by default)
</span>                              <span class="c1">#   e.g., "$" would result in IDs "KEY1$KEY2$KEY3"
</span>    <span class="p">.</span><span class="n">key_null_literal</span><span class="p">(</span><span class="s">"n/a"</span><span class="p">)</span>  <span class="c1"># optional: representation for null fields in keys ("null" by default)
</span>
    <span class="c1"># optional: failure handling strategy in case a request to Elasticsearch fails (fail by default)
</span>    <span class="p">.</span><span class="n">failure_handler_fail</span><span class="p">()</span>             <span class="c1"># optional: throws an exception if a request fails and causes a job failure
</span>    <span class="p">.</span><span class="n">failure_handler_ignore</span><span class="p">()</span>           <span class="c1">#   or ignores failures and drops the request
</span>    <span class="p">.</span><span class="n">failure_handler_retry_rejected</span><span class="p">()</span>   <span class="c1">#   or re-adds requests that have failed due to queue capacity saturation
</span>    <span class="p">.</span><span class="n">failure_handler_custom</span><span class="p">(...)</span>        <span class="c1">#   or custom failure handling with a ActionRequestFailureHandler subclass
</span>
    <span class="c1"># optional: configure how to buffer elements before sending them in bulk to the cluster for efficiency
</span>    <span class="p">.</span><span class="n">disable_flush_on_checkpoint</span><span class="p">()</span>      <span class="c1"># optional: disables flushing on checkpoint (see notes below!)
</span>    <span class="p">.</span><span class="n">bulk_flush_max_actions</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>         <span class="c1"># optional: maximum number of actions to buffer for each bulk request
</span>    <span class="p">.</span><span class="n">bulk_flush_max_size</span><span class="p">(</span><span class="s">"42 mb"</span><span class="p">)</span>       <span class="c1"># optional: maximum size of buffered actions in bytes per bulk request
</span>                                        <span class="c1">#   (only MB granularity is supported)
</span>    <span class="p">.</span><span class="n">bulk_flush_interval</span><span class="p">(</span><span class="mi">60000</span><span class="p">)</span>         <span class="c1"># optional: bulk flush interval (in milliseconds)
</span>
    <span class="p">.</span><span class="n">bulk_flush_backoff_constant</span><span class="p">()</span>      <span class="c1"># optional: use a constant backoff type
</span>    <span class="p">.</span><span class="n">bulk_flush_backoff_exponential</span><span class="p">()</span>   <span class="c1">#   or use an exponential backoff type
</span>    <span class="p">.</span><span class="n">bulk_flush_backoff_max_retries</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># optional: maximum number of retries
</span>    <span class="p">.</span><span class="n">bulk_flush_backoff_delay</span><span class="p">(</span><span class="mi">30000</span><span class="p">)</span>    <span class="c1"># optional: delay between each backoff attempt (in milliseconds)
</span>
    <span class="c1"># optional: connection properties to be used during REST communication to Elasticsearch
</span>    <span class="p">.</span><span class="n">connection_max_retry_timeout</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>    <span class="c1"># optional: maximum timeout (in milliseconds) between retries
</span>    <span class="p">.</span><span class="n">connection_path_prefix</span><span class="p">(</span><span class="s">"/v1"</span><span class="p">)</span>      <span class="c1"># optional: prefix string to be added to every REST communication
</span><span class="p">)</span>
<span class="p">.</span><span class="n">withFormat</span><span class="p">(</span>                      <span class="o">//</span> <span class="n">required</span><span class="p">:</span> <span class="n">Elasticsearch</span> <span class="n">connector</span> <span class="n">requires</span> <span class="n">to</span> <span class="n">specify</span> <span class="n">a</span> <span class="nb">format</span><span class="p">,</span>
  <span class="p">...</span>                             <span class="o">//</span> <span class="n">currently</span> <span class="n">only</span> <span class="n">Json</span> <span class="nb">format</span> <span class="ow">is</span> <span class="n">supported</span><span class="p">.</span>
                                  <span class="o">//</span> <span class="n">Please</span> <span class="n">refer</span> <span class="n">to</span> <span class="n">Table</span> <span class="n">Formats</span> <span class="n">section</span> <span class="k">for</span> <span class="n">more</span> <span class="n">details</span><span class="p">.</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">connector</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">elasticsearch</span>
  <span class="na">version</span><span class="pi">:</span> <span class="m">6</span>                                            <span class="c1"># required: valid connector versions are "6"</span>
    <span class="na">hosts</span><span class="pi">:</span> <span class="s">http://host_name:9092;http://host_name:9093</span>  <span class="c1"># required: one or more Elasticsearch hosts to connect to</span>
    <span class="na">index</span><span class="pi">:</span> <span class="s2">"</span><span class="s">MyUsers"</span>        <span class="c1"># required: Elasticsearch index</span>
    <span class="na">document-type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">user"</span>   <span class="c1"># required: Elasticsearch document type</span>

    <span class="na">key-delimiter</span><span class="pi">:</span> <span class="s2">"</span><span class="s">$"</span>      <span class="c1"># optional: delimiter for composite keys ("_" by default)</span>
                            <span class="c1">#   e.g., "$" would result in IDs "KEY1$KEY2$KEY3"</span>
    <span class="na">key-null-literal</span><span class="pi">:</span> <span class="s2">"</span><span class="s">n/a"</span> <span class="c1"># optional: representation for null fields in keys ("null" by default)</span>

    <span class="c1"># optional: failure handling strategy in case a request to Elasticsearch fails ("fail" by default)</span>
    <span class="na">failure-handler</span><span class="pi">:</span> <span class="s">...</span>    <span class="c1"># valid strategies are "fail" (throws an exception if a request fails and</span>
                            <span class="c1">#   thus causes a job failure), "ignore" (ignores failures and drops the request),</span>
                            <span class="c1">#   "retry-rejected" (re-adds requests that have failed due to queue capacity</span>
                            <span class="c1">#   saturation), or "custom" for failure handling with a</span>
                            <span class="c1">#   ActionRequestFailureHandler subclass</span>

    <span class="c1"># optional: configure how to buffer elements before sending them in bulk to the cluster for efficiency</span>
    <span class="na">flush-on-checkpoint</span><span class="pi">:</span> <span class="no">true</span>   <span class="c1"># optional: disables flushing on checkpoint (see notes below!) ("true" by default)</span>
    <span class="na">bulk-flush</span><span class="pi">:</span>
      <span class="na">max-actions</span><span class="pi">:</span> <span class="m">42</span>           <span class="c1"># optional: maximum number of actions to buffer for each bulk request</span>
      <span class="na">max-size</span><span class="pi">:</span> <span class="s">42 mb</span>           <span class="c1"># optional: maximum size of buffered actions in bytes per bulk request</span>
                                <span class="c1">#   (only MB granularity is supported)</span>
      <span class="na">interval</span><span class="pi">:</span> <span class="m">60000</span>           <span class="c1"># optional: bulk flush interval (in milliseconds)</span>
      <span class="na">backoff</span><span class="pi">:</span>                 <span class="c1"># optional: backoff strategy ("disabled" by default)</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">...</span>               <span class="c1">#   valid strategies are "disabled", "constant", or "exponential"</span>
        <span class="na">max-retries</span><span class="pi">:</span> <span class="m">3</span>          <span class="c1"># optional: maximum number of retries</span>
        <span class="na">delay</span><span class="pi">:</span> <span class="m">30000</span>            <span class="c1"># optional: delay between each backoff attempt (in milliseconds)</span>

    <span class="c1"># optional: connection properties to be used during REST communication to Elasticsearch</span>
    <span class="na">connection-max-retry-timeout</span><span class="pi">:</span> <span class="m">3</span>   <span class="c1"># optional: maximum timeout (in milliseconds) between retries</span>
    <span class="na">connection-path-prefix</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/v1"</span>     <span class="c1"># optional: prefix string to be added to every REST communication</span>

    <span class="na">format</span><span class="pi">:</span>                     <span class="c1"># required: Elasticsearch connector requires to specify a format,</span>
      <span class="s">...</span>                       <span class="c1"># currently only "json" format is supported.</span>
                                <span class="c1"># Please refer to Table Formats section for more details.</span></code></pre></figure>

  </div>
</div>

<p><strong>Bulk flushing:</strong> For more information about characteristics of the optional flushing parameters see the <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/elasticsearch.html">corresponding low-level documentation</a>.</p>

<p><strong>Disabling flushing on checkpoint:</strong> When disabled, a sink will not wait for all pending action requests to be acknowledged by Elasticsearch on checkpoints. Thus, a sink does NOT provide any strong guarantees for at-least-once delivery of action requests.</p>

<p><strong>Key extraction:</strong> Flink automatically extracts valid keys from a query. For example, a query <code class="highlighter-rouge">SELECT a, b, c FROM t GROUP BY a, b</code> defines a composite key of the fields <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>. The Elasticsearch connector generates a document ID string for every row by concatenating all key fields in the order defined in the query using a key delimiter. A custom representation of null literals for key fields can be defined.</p>

<p><span class="label label-danger">Attention</span> A JSON format defines how to encode documents for the external system, therefore, it must be added as a <a href="connect.html#formats">dependency</a>.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="hbase-connector">HBase Connector</h3>

<p><span class="label label-primary">Source: Batch</span>
<span class="label label-primary">Sink: Batch</span>
<span class="label label-primary">Sink: Streaming Append Mode</span>
<span class="label label-primary">Sink: Streaming Upsert Mode</span>
<span class="label label-primary">Temporal Join: Sync Mode</span></p>

<p>The HBase connector allows for reading from and writing to an HBase cluster.</p>

<p>The connector can operate in <a href="#update-modes">upsert mode</a> for exchanging UPSERT/DELETE messages with the external system using a <a href="./streaming/dynamic_tables.html#table-to-stream-conversion">key defined by the query</a>.</p>

<p>For append-only queries, the connector can also operate in <a href="#update-modes">append mode</a> for exchanging only INSERT messages with the external system.</p>

<p>The connector can be defined as follows:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="n">hbase_rowkey_name</span> <span class="n">rowkey_type</span><span class="p">,</span>
  <span class="n">hbase_column_family_name1</span> <span class="k">ROW</span><span class="o">&lt;</span><span class="p">...</span><span class="o">&gt;</span><span class="p">,</span>
  <span class="n">hbase_column_family_name2</span> <span class="k">ROW</span><span class="o">&lt;</span><span class="p">...</span><span class="o">&gt;</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'connector.type'</span> <span class="o">=</span> <span class="s1">'hbase'</span><span class="p">,</span> <span class="c1">-- required: specify this table type is hbase</span>

  <span class="s1">'connector.version'</span> <span class="o">=</span> <span class="s1">'1.4.3'</span><span class="p">,</span>          <span class="c1">-- required: valid connector versions are "1.4.3"</span>

  <span class="s1">'connector.table-name'</span> <span class="o">=</span> <span class="s1">'hbase_table_name'</span><span class="p">,</span>  <span class="c1">-- required: hbase table name</span>

  <span class="c1">-- required: HBase Zookeeper quorum configuration</span>
  <span class="s1">'connector.zookeeper.quorum'</span> <span class="o">=</span> <span class="s1">'localhost:2181'</span><span class="p">,</span>
  <span class="c1">-- optional: the root dir in Zookeeper for HBase cluster, default value is '/hbase'</span>
  <span class="s1">'connector.zookeeper.znode.parent'</span> <span class="o">=</span> <span class="s1">'/test'</span><span class="p">,</span>

  <span class="c1">-- optional: writing option, determines how many size in memory of buffered rows to insert per round trip.</span>
  <span class="c1">-- This can help performance on writing to JDBC database. The default value is "2mb".</span>
  <span class="s1">'connector.write.buffer-flush.max-size'</span> <span class="o">=</span> <span class="s1">'10mb'</span><span class="p">,</span>

  <span class="c1">-- optional: writing option, determines how many rows to insert per round trip.</span>
  <span class="c1">-- This can help performance on writing to JDBC database. No default value,</span>
  <span class="c1">-- i.e. the default flushing is not depends on the number of buffered rows.</span>
  <span class="s1">'connector.write.buffer-flush.max-rows'</span> <span class="o">=</span> <span class="s1">'1000'</span><span class="p">,</span>

  <span class="c1">-- optional: writing option, sets a flush interval flushing buffered requesting</span>
  <span class="c1">-- if the interval passes, in milliseconds. Default value is "0s", which means</span>
  <span class="c1">-- no asynchronous flush thread will be scheduled.</span>
  <span class="s1">'connector.write.buffer-flush.interval'</span> <span class="o">=</span> <span class="s1">'2s'</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">connect</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">HBase</span><span class="o">()</span>
    <span class="o">.</span><span class="na">version</span><span class="o">(</span><span class="s">"1.4.3"</span><span class="o">)</span>                      <span class="c1">// required: currently only support "1.4.3"</span>
    <span class="o">.</span><span class="na">tableName</span><span class="o">(</span><span class="s">"hbase_table_name"</span><span class="o">)</span>         <span class="c1">// required: HBase table name</span>
    <span class="o">.</span><span class="na">zookeeperQuorum</span><span class="o">(</span><span class="s">"localhost:2181"</span><span class="o">)</span>     <span class="c1">// required: HBase Zookeeper quorum configuration</span>
    <span class="o">.</span><span class="na">zookeeperNodeParent</span><span class="o">(</span><span class="s">"/test"</span><span class="o">)</span>          <span class="c1">// optional: the root dir in Zookeeper for HBase cluster.</span>
                                           <span class="c1">// The default value is "/hbase".</span>
    <span class="o">.</span><span class="na">writeBufferFlushMaxSize</span><span class="o">(</span><span class="s">"10mb"</span><span class="o">)</span>       <span class="c1">// optional: writing option, determines how many size in memory of buffered</span>
                                           <span class="c1">// rows to insert per round trip. This can help performance on writing to JDBC</span>
                                           <span class="c1">// database. The default value is "2mb".</span>
    <span class="o">.</span><span class="na">writeBufferFlushMaxRows</span><span class="o">(</span><span class="mi">1000</span><span class="o">)</span>         <span class="c1">// optional: writing option, determines how many rows to insert per round trip.</span>
                                           <span class="c1">// This can help performance on writing to JDBC database. No default value,</span>
                                           <span class="c1">// i.e. the default flushing is not depends on the number of buffered rows.</span>
    <span class="o">.</span><span class="na">writeBufferFlushInterval</span><span class="o">(</span><span class="s">"2s"</span><span class="o">)</span>        <span class="c1">// optional: writing option, sets a flush interval flushing buffered requesting</span>
                                           <span class="c1">// if the interval passes, in milliseconds. Default value is "0s", which means</span>
                                           <span class="c1">// no asynchronous flush thread will be scheduled.</span>
<span class="o">)</span></code></pre></figure>

  </div>
  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">connect</span><span class="p">(</span>
    <span class="n">HBase</span><span class="p">()</span>
    <span class="p">.</span><span class="n">version</span><span class="p">(</span><span class="s">'1.4.3'</span><span class="p">)</span>                      <span class="c1"># required: currently only support '1.4.3'
</span>    <span class="p">.</span><span class="n">table_name</span><span class="p">(</span><span class="s">'hbase_table_name'</span><span class="p">)</span>        <span class="c1"># required: HBase table name
</span>    <span class="p">.</span><span class="n">zookeeper_quorum</span><span class="p">(</span><span class="s">'localhost:2181'</span><span class="p">)</span>    <span class="c1"># required: HBase Zookeeper quorum configuration
</span>    <span class="p">.</span><span class="n">zookeeper_node_parent</span><span class="p">(</span><span class="s">'/test'</span><span class="p">)</span>        <span class="c1"># optional: the root dir in Zookeeper for Hbase cluster.
</span>                                           <span class="c1"># The default value is '/hbase'
</span>    <span class="p">.</span><span class="n">write_buffer_flush_max_size</span><span class="p">(</span><span class="s">'10mb'</span><span class="p">)</span>   <span class="c1"># optional: writing option, determines how many size in memory of buffered
</span>                                           <span class="c1"># rows to insert per round trip. This can help performance on writing to JDBC
</span>                                           <span class="c1"># database. The default value is '2mb'
</span>    <span class="p">.</span><span class="n">write_buffer_flush_max_rows</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>     <span class="c1"># optional: writing option, determines how many rows to insert per round trip.
</span>                                           <span class="c1"># This can help performance on writing to JDBC database. No default value,
</span>                                           <span class="c1"># i.e. the default flushing is not depends on the number of buffered rows.
</span>    <span class="p">.</span><span class="n">write_buffer_flush_interval</span><span class="p">(</span><span class="s">'2s'</span><span class="p">)</span>     <span class="c1"># optional: writing option, sets a flush interval flushing buffered requesting
</span>                                           <span class="c1"># if the interval passes, in milliseconds. Default value is '0s', which means
</span>                                           <span class="c1"># no asynchronous flush thread will he scheduled.
</span><span class="p">)</span></code></pre></figure>

  </div>
  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">connector</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">hbase</span>
  <span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1.4.3"</span>               <span class="c1"># required: currently only support "1.4.3"</span>

  <span class="na">table-name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">hbase_table_name"</span> <span class="c1"># required: HBase table name</span>

  <span class="na">zookeeper</span><span class="pi">:</span>
    <span class="na">quorum</span><span class="pi">:</span> <span class="s2">"</span><span class="s">localhost:2181"</span>     <span class="c1"># required: HBase Zookeeper quorum configuration</span>
    <span class="s">znode.parent</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/test"</span>        <span class="c1"># optional: the root dir in Zookeeper for HBase cluster.</span>
                                 <span class="c1"># The default value is "/hbase".</span>

  <span class="s">write.buffer-flush</span><span class="pi">:</span>
    <span class="na">max-size</span><span class="pi">:</span> <span class="s2">"</span><span class="s">10mb"</span>             <span class="c1"># optional: writing option, determines how many size in memory of buffered</span>
                                 <span class="c1"># rows to insert per round trip. This can help performance on writing to JDBC</span>
                                 <span class="c1"># database. The default value is "2mb".</span>
    <span class="na">max-rows</span><span class="pi">:</span> <span class="m">1000</span>               <span class="c1"># optional: writing option, determines how many rows to insert per round trip.</span>
                                 <span class="c1"># This can help performance on writing to JDBC database. No default value,</span>
                                 <span class="c1"># i.e. the default flushing is not depends on the number of buffered rows.</span>
    <span class="na">interval</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2s"</span>               <span class="c1"># optional: writing option, sets a flush interval flushing buffered requesting</span>
                                 <span class="c1"># if the interval passes, in milliseconds. Default value is "0s", which means</span>
                                 <span class="c1"># no asynchronous flush thread will be scheduled.</span></code></pre></figure>

  </div>
</div>

<p><strong>Columns:</strong> All the column families in HBase table must be declared as <code class="highlighter-rouge">ROW</code> type, the field name maps to the column family name, and the nested field names map to the column qualifier names. There is no need to declare all the families and qualifiers in the schema, users can declare what’s necessary. Except the <code class="highlighter-rouge">ROW</code> type fields, the only one field of atomic type (e.g. <code class="highlighter-rouge">STRING</code>, <code class="highlighter-rouge">BIGINT</code>) will be recognized as row key of the table. There’s no constraints on the name of row key field.</p>

<p><strong>Temporal join:</strong> Lookup join against HBase do not use any caching; data is always queired directly through the HBase client.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="jdbc-connector">JDBC Connector</h3>

<p><span class="label label-primary">Source: Batch</span>
<span class="label label-primary">Sink: Batch</span>
<span class="label label-primary">Sink: Streaming Append Mode</span>
<span class="label label-primary">Sink: Streaming Upsert Mode</span>
<span class="label label-primary">Temporal Join: Sync Mode</span></p>

<p>The JDBC connector allows for reading from and writing into an JDBC client.</p>

<p>The connector can operate in <a href="#update-modes">upsert mode</a> for exchanging UPSERT/DELETE messages with the external system using a <a href="./streaming/dynamic_tables.html#table-to-stream-conversion">key defined by the query</a>.</p>

<p>For append-only queries, the connector can also operate in <a href="#update-modes">append mode</a> for exchanging only INSERT messages with the external system.</p>

<p>To use JDBC connector, need to choose an actual driver to use. Here are drivers currently supported:</p>

<p><strong>Supported Drivers:</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th style="text-align: left">Group Id</th>
      <th style="text-align: left">Artifact Id</th>
      <th style="text-align: left">JAR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">MySQL</td>
      <td style="text-align: left">mysql</td>
      <td style="text-align: left">mysql-connector-java</td>
      <td style="text-align: left"><a href="https://repo.maven.apache.org/maven2/mysql/mysql-connector-java/">Download</a></td>
    </tr>
    <tr>
      <td style="text-align: left">PostgreSQL</td>
      <td style="text-align: left">org.postgresql</td>
      <td style="text-align: left">postgresql</td>
      <td style="text-align: left"><a href="https://jdbc.postgresql.org/download.html">Download</a></td>
    </tr>
    <tr>
      <td style="text-align: left">Derby</td>
      <td style="text-align: left">org.apache.derby</td>
      <td style="text-align: left">derby</td>
      <td style="text-align: left"><a href="http://db.apache.org/derby/derby_downloads.html">Download</a></td>
    </tr>
  </tbody>
</table>

<p><strong>Catalog</strong></p>

<p>JDBC Connector can be used together with <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/catalogs.html#jdbccatalog"><code class="highlighter-rouge">JdbcCatalog</code></a> to greatly simplify development effort and improve user experience.</p>

<p><br /></p>

<p>The connector can be defined as follows:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'connector.type'</span> <span class="o">=</span> <span class="s1">'jdbc'</span><span class="p">,</span> <span class="c1">-- required: specify this table type is jdbc</span>

  <span class="s1">'connector.url'</span> <span class="o">=</span> <span class="s1">'jdbc:mysql://localhost:3306/flink-test'</span><span class="p">,</span> <span class="c1">-- required: JDBC DB url</span>

  <span class="s1">'connector.table'</span> <span class="o">=</span> <span class="s1">'jdbc_table_name'</span><span class="p">,</span>  <span class="c1">-- required: jdbc table name</span>

  <span class="c1">-- optional: the class name of the JDBC driver to use to connect to this URL.</span>
  <span class="c1">-- If not set, it will automatically be derived from the URL.</span>
  <span class="s1">'connector.driver'</span> <span class="o">=</span> <span class="s1">'com.mysql.jdbc.Driver'</span><span class="p">,</span>

  <span class="c1">-- optional: jdbc user name and password</span>
  <span class="s1">'connector.username'</span> <span class="o">=</span> <span class="s1">'name'</span><span class="p">,</span>
  <span class="s1">'connector.password'</span> <span class="o">=</span> <span class="s1">'password'</span><span class="p">,</span>

  <span class="c1">-- **followings are scan options, optional, used when reading from table**</span>

  <span class="c1">-- These options must all be specified if any of them is specified. In addition,</span>
  <span class="c1">-- partition.num must be specified. They describe how to partition the table when</span>
  <span class="c1">-- reading in parallel from multiple tasks. partition.column must be a numeric,</span>
  <span class="c1">-- date, or timestamp column from the table in question. Notice that lowerBound and</span>
  <span class="c1">-- upperBound are just used to decide the partition stride, not for filtering the</span>
  <span class="c1">-- rows in table. So all rows in the table will be partitioned and returned.</span>

  <span class="s1">'connector.read.partition.column'</span> <span class="o">=</span> <span class="s1">'column_name'</span><span class="p">,</span> <span class="c1">-- optional: the column name used for partitioning the input.</span>
  <span class="s1">'connector.read.partition.num'</span> <span class="o">=</span> <span class="s1">'50'</span><span class="p">,</span> <span class="c1">-- optional: the number of partitions.</span>
  <span class="s1">'connector.read.partition.lower-bound'</span> <span class="o">=</span> <span class="s1">'500'</span><span class="p">,</span> <span class="c1">-- optional: the smallest value of the first partition.</span>
  <span class="s1">'connector.read.partition.upper-bound'</span> <span class="o">=</span> <span class="s1">'1000'</span><span class="p">,</span> <span class="c1">-- optional: the largest value of the last partition.</span>

  <span class="c1">-- optional, Gives the reader a hint as to the number of rows that should be fetched</span>
  <span class="c1">-- from the database when reading per round trip. If the value specified is zero, then</span>
  <span class="c1">-- the hint is ignored. The default value is zero.</span>
  <span class="s1">'connector.read.fetch-size'</span> <span class="o">=</span> <span class="s1">'100'</span><span class="p">,</span>

  <span class="c1">-- **followings are lookup options, optional, used in temporary join**</span>

  <span class="c1">-- optional, max number of rows of lookup cache, over this value, the oldest rows will</span>
  <span class="c1">-- be eliminated. "cache.max-rows" and "cache.ttl" options must all be specified if any</span>
  <span class="c1">-- of them is specified. Cache is not enabled as default.</span>
  <span class="s1">'connector.lookup.cache.max-rows'</span> <span class="o">=</span> <span class="s1">'5000'</span><span class="p">,</span>

  <span class="c1">-- optional, the max time to live for each rows in lookup cache, over this time, the oldest rows</span>
  <span class="c1">-- will be expired. "cache.max-rows" and "cache.ttl" options must all be specified if any of</span>
  <span class="c1">-- them is specified. Cache is not enabled as default.</span>
  <span class="s1">'connector.lookup.cache.ttl'</span> <span class="o">=</span> <span class="s1">'10s'</span><span class="p">,</span>

  <span class="s1">'connector.lookup.max-retries'</span> <span class="o">=</span> <span class="s1">'3'</span><span class="p">,</span> <span class="c1">-- optional, max retry times if lookup database failed</span>

  <span class="c1">-- **followings are sink options, optional, used when writing into table**</span>

  <span class="c1">-- optional, flush max size (includes all append, upsert and delete records),</span>
  <span class="c1">-- over this number of records, will flush data. The default value is "5000".</span>
  <span class="s1">'connector.write.flush.max-rows'</span> <span class="o">=</span> <span class="s1">'5000'</span><span class="p">,</span>

  <span class="c1">-- optional, flush interval mills, over this time, asynchronous threads will flush data.</span>
  <span class="c1">-- The default value is "0s", which means no asynchronous flush thread will be scheduled.</span>
  <span class="s1">'connector.write.flush.interval'</span> <span class="o">=</span> <span class="s1">'2s'</span><span class="p">,</span>

  <span class="c1">-- optional, max retry times if writing records to database failed</span>
  <span class="s1">'connector.write.max-retries'</span> <span class="o">=</span> <span class="s1">'3'</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">connector</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">jdbc</span>
  <span class="na">url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">jdbc:mysql://localhost:3306/flink-test"</span>     <span class="c1"># required: JDBC DB url</span>
  <span class="na">table</span><span class="pi">:</span> <span class="s2">"</span><span class="s">jdbc_table_name"</span>        <span class="c1"># required: jdbc table name</span>
  <span class="na">driver</span><span class="pi">:</span> <span class="s2">"</span><span class="s">com.mysql.jdbc.Driver"</span> <span class="c1"># optional: the class name of the JDBC driver to use to connect to this URL.</span>
                                  <span class="c1"># If not set, it will automatically be derived from the URL.</span>

  <span class="na">username</span><span class="pi">:</span> <span class="s2">"</span><span class="s">name"</span>                <span class="c1"># optional: jdbc user name and password</span>
  <span class="na">password</span><span class="pi">:</span> <span class="s2">"</span><span class="s">password"</span>

  <span class="na">read</span><span class="pi">:</span> <span class="c1"># scan options, optional, used when reading from table</span>
    <span class="na">partition</span><span class="pi">:</span> <span class="c1"># These options must all be specified if any of them is specified. In addition, partition.num must be specified. They</span>
               <span class="c1"># describe how to partition the table when reading in parallel from multiple tasks. partition.column must be a numeric,</span>
               <span class="c1"># date, or timestamp column from the table in question. Notice that lowerBound and upperBound are just used to decide</span>
               <span class="c1"># the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.</span>
               <span class="c1"># This option applies only to reading.</span>
      <span class="na">column</span><span class="pi">:</span> <span class="s2">"</span><span class="s">column_name"</span> <span class="c1"># optional, name of the column used for partitioning the input.</span>
      <span class="na">num</span><span class="pi">:</span> <span class="m">50</span>               <span class="c1"># optional, the number of partitions.</span>
      <span class="na">lower-bound</span><span class="pi">:</span> <span class="m">500</span>      <span class="c1"># optional, the smallest value of the first partition.</span>
      <span class="na">upper-bound</span><span class="pi">:</span> <span class="m">1000</span>     <span class="c1"># optional, the largest value of the last partition.</span>
    <span class="na">fetch-size</span><span class="pi">:</span> <span class="m">100</span>         <span class="c1"># optional, Gives the reader a hint as to the number of rows that should be fetched</span>
                            <span class="c1"># from the database when reading per round trip. If the value specified is zero, then</span>
                            <span class="c1"># the hint is ignored. The default value is zero.</span>

  <span class="na">lookup</span><span class="pi">:</span> <span class="c1"># lookup options, optional, used in temporary join</span>
    <span class="na">cache</span><span class="pi">:</span>
      <span class="na">max-rows</span><span class="pi">:</span> <span class="m">5000</span> <span class="c1"># optional, max number of rows of lookup cache, over this value, the oldest rows will</span>
                     <span class="c1"># be eliminated. "cache.max-rows" and "cache.ttl" options must all be specified if any</span>
                     <span class="c1"># of them is specified. Cache is not enabled as default.</span>
      <span class="na">ttl</span><span class="pi">:</span> <span class="s2">"</span><span class="s">10s"</span>     <span class="c1"># optional, the max time to live for each rows in lookup cache, over this time, the oldest rows</span>
                     <span class="c1"># will be expired. "cache.max-rows" and "cache.ttl" options must all be specified if any of</span>
                     <span class="c1"># them is specified. Cache is not enabled as default.</span>
    <span class="na">max-retries</span><span class="pi">:</span> <span class="m">3</span>   <span class="c1"># optional, max retry times if lookup database failed</span>

  <span class="na">write</span><span class="pi">:</span> <span class="c1"># sink options, optional, used when writing into table</span>
      <span class="na">flush</span><span class="pi">:</span>
        <span class="na">max-rows</span><span class="pi">:</span> <span class="m">5000</span> <span class="c1"># optional, flush max size (includes all append, upsert and delete records),</span>
                       <span class="c1"># over this number of records, will flush data. The default value is "5000".</span>
        <span class="na">interval</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2s"</span> <span class="c1"># optional, flush interval mills, over this time, asynchronous threads will flush data.</span>
                       <span class="c1"># The default value is "0s", which means no asynchronous flush thread will be scheduled.</span>
      <span class="na">max-retries</span><span class="pi">:</span> <span class="m">3</span>   <span class="c1"># optional, max retry times if writing records to database failed.</span></code></pre></figure>

  </div>
</div>

<p><strong>Upsert sink:</strong> Flink automatically extracts valid keys from a query. For example, a query <code class="highlighter-rouge">SELECT a, b, c FROM t GROUP BY a, b</code> defines a composite key of the fields <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>. If a JDBC table is used as upsert sink, please make sure keys of the query is one of the unique key sets or primary key of the underlying database. This can guarantee the output result is as expected.</p>

<p><strong>Temporary Join:</strong>  JDBC connector can be used in temporal join as a lookup source. Currently, only sync lookup mode is supported. The lookup cache options (<code class="highlighter-rouge">connector.lookup.cache.max-rows</code> and <code class="highlighter-rouge">connector.lookup.cache.ttl</code>) must all be specified if any of them is specified. The lookup cache is used to improve performance of temporal join JDBC connector by querying the cache first instead of send all requests to remote database. But the returned value might not be the latest if it is from the cache. So it’s a balance between throughput and correctness.</p>

<p><strong>Writing:</strong> As default, the <code class="highlighter-rouge">connector.write.flush.interval</code> is <code class="highlighter-rouge">0s</code> and <code class="highlighter-rouge">connector.write.flush.max-rows</code> is <code class="highlighter-rouge">5000</code>, which means for low traffic queries, the buffered output rows may not be flushed to database for a long time. So the interval configuration is recommended to set.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="hive-connector">Hive Connector</h3>

<p><span class="label label-primary">Source: Batch</span>
<span class="label label-primary">Sink: Batch</span></p>

<p>Please refer to <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hive/">Hive integration</a>.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h2 id="table-formats">Table Formats</h2>

<p>Flink provides a set of table formats that can be used with table connectors.</p>

<p>A format tag indicates the format type for matching with a connector.</p>

<h3 id="csv-format">CSV Format</h3>

<p><span class="label label-info">Format: Serialization Schema</span>
<span class="label label-info">Format: Deserialization Schema</span></p>

<p>The CSV format aims to comply with <a href="https://tools.ietf.org/html/rfc4180">RFC-4180</a> (“Common Format and
MIME Type for Comma-Separated Values (CSV) Files”) proposed by the Internet Engineering Task Force (IETF).</p>

<p>The format allows to read and write CSV data that corresponds to a given format schema. The format schema can be
derived from the desired table schema or defined as a Flink type. Since Flink 1.10, the format will derive
format schema from table schema by default. Therefore, it is no longer necessary to explicitly declare the format schema.</p>

<p>When deriving schema from table schema, the names, types, and fields’ order of the format are determined by the
table’s schema. Time attributes are ignored if their origin is not a field. A <code class="highlighter-rouge">from</code> definition in the table
schema is interpreted as a field renaming in the format.</p>

<p>The CSV format can be used as follows:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'format.type'</span> <span class="o">=</span> <span class="s1">'csv'</span><span class="p">,</span>                      <span class="c1">-- required: specify the schema type</span>

  <span class="s1">'format.field-delimiter'</span> <span class="o">=</span> <span class="s1">';'</span><span class="p">,</span>             <span class="c1">-- optional: field delimiter character (',' by default)</span>

  <span class="s1">'format.line-delimiter'</span> <span class="o">=</span> <span class="n">U</span><span class="o">&amp;</span><span class="s1">'</span><span class="se">\0</span><span class="s1">00D</span><span class="se">\0</span><span class="s1">00A'</span><span class="p">,</span>   <span class="c1">-- optional: line delimiter ("\n" by default, otherwise</span>
                                              <span class="c1">-- "\r" or "\r\n" are allowed), unicode is supported if</span>
                                              <span class="c1">-- the delimiter is an invisible special character,</span>
                                              <span class="c1">-- e.g. U&amp;'\000D' is the unicode representation of carriage return "\r"</span>
                                              <span class="c1">-- e.g. U&amp;'\000A' is the unicode representation of line feed "\n"</span>
  <span class="s1">'format.disable-quote-character'</span> <span class="o">=</span> <span class="s1">'true'</span><span class="p">,</span>  <span class="c1">-- optional: disabled quote character for enclosing field values (false by default)</span>
                                              <span class="c1">-- if true, format.quote-character can not be set</span>
  <span class="s1">'format.quote-character'</span> <span class="o">=</span> <span class="s1">'</span><span class="se">''</span><span class="s1">'</span><span class="p">,</span>            <span class="c1">-- optional: quote character for enclosing field values ('"' by default)</span>
  <span class="s1">'format.allow-comments'</span> <span class="o">=</span> <span class="s1">'true'</span><span class="p">,</span>           <span class="c1">-- optional: ignores comment lines that start with "#"</span>
                                              <span class="c1">-- (disabled by default);</span>
                                              <span class="c1">-- if enabled, make sure to also ignore parse errors to allow empty rows</span>
  <span class="s1">'format.ignore-parse-errors'</span> <span class="o">=</span> <span class="s1">'true'</span><span class="p">,</span>      <span class="c1">-- optional: skip fields and rows with parse errors instead of failing;</span>
                                              <span class="c1">-- fields are set to null in case of errors</span>
  <span class="s1">'format.array-element-delimiter'</span> <span class="o">=</span> <span class="s1">'|'</span><span class="p">,</span>     <span class="c1">-- optional: the array element delimiter string for separating</span>
                                              <span class="c1">-- array and row element values (";" by default)</span>
  <span class="s1">'format.escape-character'</span> <span class="o">=</span> <span class="s1">'</span><span class="se">\\</span><span class="s1">'</span><span class="p">,</span>           <span class="c1">-- optional: escape character for escaping values (disabled by default)</span>
  <span class="s1">'format.null-literal'</span> <span class="o">=</span> <span class="s1">'n/a'</span>               <span class="c1">-- optional: null literal string that is interpreted as a</span>
                                              <span class="c1">-- null value (disabled by default)</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">withFormat</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Csv</span><span class="o">()</span>

    <span class="o">.</span><span class="na">fieldDelimiter</span><span class="o">(</span><span class="sc">';'</span><span class="o">)</span>         <span class="c1">// optional: field delimiter character (',' by default)</span>
    <span class="o">.</span><span class="na">lineDelimiter</span><span class="o">(</span><span class="s">"\r\n"</span><span class="o">)</span>       <span class="c1">// optional: line delimiter ("\n" by default;</span>
                                 <span class="c1">//   otherwise "\r", "\r\n", or "" are allowed)</span>
    <span class="o">.</span><span class="na">disableQuoteCharacter</span><span class="o">()</span>     <span class="c1">// optional: disabled quote character for enclosing field values;</span>
                                 <span class="c1">//   cannot define a quote character and disabled quote character at the same time</span>
    <span class="o">.</span><span class="na">quoteCharacter</span><span class="o">(</span><span class="sc">'\''</span><span class="o">)</span>        <span class="c1">// optional: quote character for enclosing field values ('"' by default)</span>
    <span class="o">.</span><span class="na">allowComments</span><span class="o">()</span>             <span class="c1">// optional: ignores comment lines that start with '#' (disabled by default);</span>
                                 <span class="c1">//   if enabled, make sure to also ignore parse errors to allow empty rows</span>
    <span class="o">.</span><span class="na">ignoreParseErrors</span><span class="o">()</span>         <span class="c1">// optional: skip fields and rows with parse errors instead of failing;</span>
                                 <span class="c1">//   fields are set to null in case of errors</span>
    <span class="o">.</span><span class="na">arrayElementDelimiter</span><span class="o">(</span><span class="s">"|"</span><span class="o">)</span>  <span class="c1">// optional: the array element delimiter string for separating</span>
                                 <span class="c1">//   array and row element values (";" by default)</span>
    <span class="o">.</span><span class="na">escapeCharacter</span><span class="o">(</span><span class="sc">'\\'</span><span class="o">)</span>       <span class="c1">// optional: escape character for escaping values (disabled by default)</span>
    <span class="o">.</span><span class="na">nullLiteral</span><span class="o">(</span><span class="s">"n/a"</span><span class="o">)</span>          <span class="c1">// optional: null literal string that is interpreted as a</span>
                                 <span class="c1">//   null value (disabled by default)</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">with_format</span><span class="p">(</span>
    <span class="n">Csv</span><span class="p">()</span>

    <span class="p">.</span><span class="n">field_delimiter</span><span class="p">(</span><span class="s">';'</span><span class="p">)</span>          <span class="c1"># optional: field delimiter character (',' by default)
</span>    <span class="p">.</span><span class="n">line_delimiter</span><span class="p">(</span><span class="s">"</span><span class="se">\r\n</span><span class="s">"</span><span class="p">)</span>        <span class="c1"># optional: line delimiter ("\n" by default;
</span>                                   <span class="c1">#   otherwise "\r", "\r\n", or "" are allowed)
</span>    <span class="p">.</span><span class="n">quote_character</span><span class="p">(</span><span class="s">'</span><span class="se">\'</span><span class="s">'</span><span class="p">)</span>         <span class="c1"># optional: quote character for enclosing field values ('"' by default)
</span>    <span class="p">.</span><span class="n">allow_comments</span><span class="p">()</span>              <span class="c1"># optional: ignores comment lines that start with '#' (disabled by default);
</span>                                   <span class="c1">#   if enabled, make sure to also ignore parse errors to allow empty rows
</span>    <span class="p">.</span><span class="n">ignore_parse_errors</span><span class="p">()</span>         <span class="c1"># optional: skip fields and rows with parse errors instead of failing;
</span>                                   <span class="c1">#   fields are set to null in case of errors
</span>    <span class="p">.</span><span class="n">array_element_delimiter</span><span class="p">(</span><span class="s">"|"</span><span class="p">)</span>  <span class="c1"># optional: the array element delimiter string for separating
</span>                                   <span class="c1">#   array and row element values (";" by default)
</span>    <span class="p">.</span><span class="n">escape_character</span><span class="p">(</span><span class="s">'</span><span class="se">\\</span><span class="s">'</span><span class="p">)</span>        <span class="c1"># optional: escape character for escaping values (disabled by default)
</span>    <span class="p">.</span><span class="n">null_literal</span><span class="p">(</span><span class="s">"n/a"</span><span class="p">)</span>           <span class="c1"># optional: null literal string that is interpreted as a
</span>                                   <span class="c1">#   null value (disabled by default)
</span><span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">format</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">csv</span>

  <span class="na">field-delimiter</span><span class="pi">:</span> <span class="s2">"</span><span class="s">;"</span>         <span class="c1"># optional: field delimiter character (',' by default)</span>
  <span class="na">line-delimiter</span><span class="pi">:</span> <span class="s2">"</span><span class="se">\r\n</span><span class="s">"</span>       <span class="c1"># optional: line delimiter ("\n" by default;</span>
                               <span class="c1">#   otherwise "\r", "\r\n", or "" are allowed)</span>
  <span class="s">disable-quote-character = </span><span class="no">true</span> <span class="c1"># optional: disabled quote character for enclosing field values (false by default)</span>
                               <span class="c1"># if true, quote-character can not be set</span>
  <span class="na">quote-character</span><span class="pi">:</span> <span class="s2">"</span><span class="s">'"</span>         <span class="c1"># optional: quote character for enclosing field values ('"' by default)</span>
  <span class="na">allow-comments</span><span class="pi">:</span> <span class="no">true</span>         <span class="c1"># optional: ignores comment lines that start with "#" (disabled by default);</span>
                               <span class="c1">#   if enabled, make sure to also ignore parse errors to allow empty rows</span>
  <span class="na">ignore-parse-errors</span><span class="pi">:</span> <span class="no">true</span>    <span class="c1"># optional: skip fields and rows with parse errors instead of failing;</span>
                               <span class="c1">#   fields are set to null in case of errors</span>
  <span class="na">array-element-delimiter</span><span class="pi">:</span> <span class="s2">"</span><span class="s">|"</span> <span class="c1"># optional: the array element delimiter string for separating</span>
                               <span class="c1">#   array and row element values (";" by default)</span>
  <span class="na">escape-character</span><span class="pi">:</span> <span class="s2">"</span><span class="se">\\</span><span class="s">"</span>       <span class="c1"># optional: escape character for escaping values (disabled by default)</span>
  <span class="na">null-literal</span><span class="pi">:</span> <span class="s2">"</span><span class="s">n/a"</span>          <span class="c1"># optional: null literal string that is interpreted as a</span>
                               <span class="c1">#   null value (disabled by default)</span></code></pre></figure>

  </div>
</div>

<p>The following table lists supported types that can be read and written:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Supported Flink SQL Types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">ROW</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">VARCHAR</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">ARRAY[_]</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">INT</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">BIGINT</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">FLOAT</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">DOUBLE</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">BOOLEAN</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">DATE</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">TIME</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">TIMESTAMP</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">DECIMAL</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">NULL</code> (unsupported yet)</td>
    </tr>
  </tbody>
</table>

<p><strong>Numeric types:</strong> Value should be a number but the literal <code class="highlighter-rouge">"null"</code> can also be understood. An empty string is
considered <code class="highlighter-rouge">null</code>. Values are also trimmed (leading/trailing white space). Numbers are parsed using
Java’s <code class="highlighter-rouge">valueOf</code> semantics. Other non-numeric strings may cause a parsing exception.</p>

<p><strong>String and time types:</strong> Value is not trimmed. The literal <code class="highlighter-rouge">"null"</code> can also be understood. Time types
must be formatted according to the Java SQL time format with millisecond precision. For example:
<code class="highlighter-rouge">2018-01-01</code> for date, <code class="highlighter-rouge">20:43:59</code> for time, and <code class="highlighter-rouge">2018-01-01 20:43:59.999</code> for timestamp.</p>

<p><strong>Boolean type:</strong> Value is expected to be a boolean (<code class="highlighter-rouge">"true"</code>, <code class="highlighter-rouge">"false"</code>) string or <code class="highlighter-rouge">"null"</code>. Empty strings are
interpreted as <code class="highlighter-rouge">false</code>. Values are trimmed (leading/trailing white space). Other values result in an exception.</p>

<p><strong>Nested types:</strong> Array and row types are supported for one level of nesting using the array element delimiter.</p>

<p><strong>Primitive byte arrays:</strong> Primitive byte arrays are handled in Base64-encoded representation.</p>

<p><strong>Line endings:</strong> Line endings need to be considered even for row-based connectors (such as Kafka)
to be ignored for unquoted string fields at the end of a row.</p>

<p><strong>Escaping and quoting:</strong> The following table shows examples of how escaping and quoting affect the parsing
of a string using <code class="highlighter-rouge">*</code> for escaping and <code class="highlighter-rouge">'</code> for quoting:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">CSV Field</th>
      <th style="text-align: left">Parsed String</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">123*'4**</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">123'4*</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">'123''4**'</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">123'4*</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">'a;b*'c'</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">a;b'c</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">'a;b''c'</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">a;b'c</code></td>
    </tr>
  </tbody>
</table>

<p>Make sure to add the CSV format as a dependency.</p>

<h3 id="json-format">JSON Format</h3>

<p><span class="label label-info">Format: Serialization Schema</span>
<span class="label label-info">Format: Deserialization Schema</span></p>

<p>The JSON format allows to read and write JSON data that corresponds to a given format schema. The format schema is derived from the desired table schema by default, this requires format schema is equal to the table schema. The names, types, and fields’ order of the format are determined by the table’s schema.</p>

<p>Defining format schema as a JSON schema is deprecated, and may be dropped in future versions.</p>

<p>The JSON format can be used as follows:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'format.type'</span> <span class="o">=</span> <span class="s1">'json'</span><span class="p">,</span>                   <span class="c1">-- required: specify the format type</span>
  <span class="s1">'format.fail-on-missing-field'</span> <span class="o">=</span> <span class="s1">'true'</span><span class="p">,</span>  <span class="c1">-- optional: flag whether to fail if a field is missing or not,</span>
                                            <span class="c1">-- 'false' by default</span>
  <span class="s1">'format.ignore-parse-errors'</span> <span class="o">=</span> <span class="s1">'true'</span><span class="p">,</span>    <span class="c1">-- optional: skip fields and rows with parse errors instead of failing;</span>
                                            <span class="c1">-- fields are set to null in case of errors</span>
  <span class="c1">-- deprecated: define the schema explicitly using JSON schema which parses to DECIMAL and TIMESTAMP.</span>
  <span class="s1">'format.json-schema'</span> <span class="o">=</span>
    <span class="s1">'{
      "type": "object",
      "properties": {
        "lon": {
          "type": "number"
        },
        "rideTime": {
          "type": "string",
          "format": "date-time"
        }
      }
    }'</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">withFormat</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Json</span><span class="o">()</span>
    <span class="o">.</span><span class="na">failOnMissingField</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>   <span class="c1">// optional: flag whether to fail if a field is missing or not, false by default</span>
    <span class="o">.</span><span class="na">ignoreParseErrors</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>    <span class="c1">// optional: skip fields and rows with parse errors instead of failing;</span>
                                <span class="c1">//   fields are set to null in case of errors</span>
    <span class="c1">// deprecated: define the schema explicitly using JSON schema which parses to DECIMAL and TIMESTAMP.</span>
    <span class="o">.</span><span class="na">jsonSchema</span><span class="o">(</span>
      <span class="s">"{"</span> <span class="o">+</span>
      <span class="s">"  type: 'object',"</span> <span class="o">+</span>
      <span class="s">"  properties: {"</span> <span class="o">+</span>
      <span class="s">"    lon: {"</span> <span class="o">+</span>
      <span class="s">"      type: 'number'"</span> <span class="o">+</span>
      <span class="s">"    },"</span> <span class="o">+</span>
      <span class="s">"    rideTime: {"</span> <span class="o">+</span>
      <span class="s">"      type: 'string',"</span> <span class="o">+</span>
      <span class="s">"      format: 'date-time'"</span> <span class="o">+</span>
      <span class="s">"    }"</span> <span class="o">+</span>
      <span class="s">"  }"</span> <span class="o">+</span>
      <span class="s">"}"</span>
    <span class="o">)</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">with_format</span><span class="p">(</span>
    <span class="n">Json</span><span class="p">()</span>
    <span class="p">.</span><span class="n">fail_on_missing_field</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># optional: flag whether to fail if a field is missing or not, False by default
</span>
    <span class="c1"># deprecated: define the schema explicitly using JSON schema which parses to DECIMAL and TIMESTAMP.
</span>    <span class="p">.</span><span class="n">json_schema</span><span class="p">(</span>
        <span class="s">"{"</span>
        <span class="s">"  type: 'object',"</span>
        <span class="s">"  properties: {"</span>
        <span class="s">"    lon: {"</span>
        <span class="s">"      type: 'number'"</span>
        <span class="s">"    },"</span>
        <span class="s">"    rideTime: {"</span>
        <span class="s">"      type: 'string',"</span>
        <span class="s">"      format: 'date-time'"</span>
        <span class="s">"    }"</span>
        <span class="s">"  }"</span>
        <span class="s">"}"</span>
    <span class="p">)</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">format</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">json</span>
  <span class="na">fail-on-missing-field</span><span class="pi">:</span> <span class="no">true</span>   <span class="c1"># optional: flag whether to fail if a field is missing or not, false by default</span>

  <span class="c1"># deprecated: define the schema explicitly using JSON schema which parses to DECIMAL and TIMESTAMP.</span>
  <span class="na">json-schema</span><span class="pi">:</span> <span class="pi">&gt;</span>
    <span class="s">{</span>
      <span class="s">type: 'object',</span>
      <span class="s">properties: {</span>
        <span class="s">lon: {</span>
          <span class="s">type: 'number'</span>
        <span class="s">},</span>
        <span class="s">rideTime: {</span>
          <span class="s">type: 'string',</span>
          <span class="s">format: 'date-time'</span>
        <span class="s">}</span>
      <span class="s">}</span>
    <span class="s">}</span></code></pre></figure>

  </div>
</div>

<p>The following table shows the mapping of JSON schema types to Flink SQL types:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">JSON schema</th>
      <th style="text-align: left">Flink SQL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">object</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">ROW</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">boolean</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">BOOLEAN</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">array</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">ARRAY[_]</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">number</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">DECIMAL</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">integer</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">DECIMAL</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">string</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">STRING</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">string</code> with <code class="highlighter-rouge">format: date-time</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">TIMESTAMP</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">string</code> with <code class="highlighter-rouge">format: date</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">DATE</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">string</code> with <code class="highlighter-rouge">format: time</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">TIME</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">string</code> with <code class="highlighter-rouge">encoding: base64</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">ARRAY[TINYINT]</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">null</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">NULL</code> (unsupported yet)</td>
    </tr>
  </tbody>
</table>

<p>Currently, Flink supports only a subset of the <a href="http://json-schema.org/">JSON schema specification</a> <code class="highlighter-rouge">draft-07</code>. Union types (as well as <code class="highlighter-rouge">allOf</code>, <code class="highlighter-rouge">anyOf</code>, <code class="highlighter-rouge">not</code>) are not supported yet. <code class="highlighter-rouge">oneOf</code> and arrays of types are only supported for specifying nullability.</p>

<p>Simple references that link to a common definition in the document are supported as shown in the more complex example below:</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
  </span><span class="nl">"definitions"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"address"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"street_address"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="nl">"city"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="nl">"state"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="nl">"required"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="s2">"street_address"</span><span class="p">,</span><span class="w">
        </span><span class="s2">"city"</span><span class="p">,</span><span class="w">
        </span><span class="s2">"state"</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"billing_address"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"$ref"</span><span class="p">:</span><span class="w"> </span><span class="s2">"#/definitions/address"</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"shipping_address"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"$ref"</span><span class="p">:</span><span class="w"> </span><span class="s2">"#/definitions/address"</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"optional_address"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"oneOf"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"null"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
          </span><span class="nl">"$ref"</span><span class="p">:</span><span class="w"> </span><span class="s2">"#/definitions/address"</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<p><strong>Missing Field Handling:</strong> By default, a missing JSON field is set to <code class="highlighter-rouge">null</code>. You can enable strict JSON parsing that will cancel the source (and query) if a field is missing.</p>

<p>Make sure to add the JSON format as a dependency.</p>

<h3 id="apache-avro-format">Apache Avro Format</h3>

<p><span class="label label-info">Format: Serialization Schema</span>
<span class="label label-info">Format: Deserialization Schema</span></p>

<p>The <a href="https://avro.apache.org/">Apache Avro</a> format allows to read and write Avro data that corresponds to a given format schema. The format schema can be defined either as a fully qualified class name of an Avro specific record or as an Avro schema string. If a class name is used, the class must be available in the classpath during runtime.</p>

<p>The Avro format can be used as follows:</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'format.type'</span> <span class="o">=</span> <span class="s1">'avro'</span><span class="p">,</span>                                 <span class="c1">-- required: specify the schema type</span>
  <span class="s1">'format.record-class'</span> <span class="o">=</span> <span class="s1">'org.organization.types.User'</span><span class="p">,</span>  <span class="c1">-- required: define the schema either by using an Avro specific record class</span>

  <span class="s1">'format.avro-schema'</span> <span class="o">=</span>                                  <span class="c1">-- or by using an Avro schema</span>
    <span class="s1">'{
      "type": "record",
      "name": "test",
      "fields" : [
        {"name": "a", "type": "long"},
        {"name": "b", "type": "string"}
      ]
    }'</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">withFormat</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">Avro</span><span class="o">()</span>

    <span class="c1">// required: define the schema either by using an Avro specific record class</span>
    <span class="o">.</span><span class="na">recordClass</span><span class="o">(</span><span class="nc">User</span><span class="o">.</span><span class="na">class</span><span class="o">)</span>

    <span class="c1">// or by using an Avro schema</span>
    <span class="o">.</span><span class="na">avroSchema</span><span class="o">(</span>
      <span class="s">"{"</span> <span class="o">+</span>
      <span class="s">"  \"type\": \"record\","</span> <span class="o">+</span>
      <span class="s">"  \"name\": \"test\","</span> <span class="o">+</span>
      <span class="s">"  \"fields\" : ["</span> <span class="o">+</span>
      <span class="s">"    {\"name\": \"a\", \"type\": \"long\"},"</span> <span class="o">+</span>
      <span class="s">"    {\"name\": \"b\", \"type\": \"string\"}"</span> <span class="o">+</span>
      <span class="s">"  ]"</span> <span class="o">+</span>
      <span class="s">"}"</span>
    <span class="o">)</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">with_format</span><span class="p">(</span>
    <span class="n">Avro</span><span class="p">()</span>

    <span class="c1"># required: define the schema either by using an Avro specific record class
</span>    <span class="p">.</span><span class="n">record_class</span><span class="p">(</span><span class="s">"full.qualified.user.class.name"</span><span class="p">)</span>

    <span class="c1"># or by using an Avro schema
</span>    <span class="p">.</span><span class="n">avro_schema</span><span class="p">(</span>
        <span class="s">"{"</span>
        <span class="s">"  </span><span class="se">\"</span><span class="s">type</span><span class="se">\"</span><span class="s">: </span><span class="se">\"</span><span class="s">record</span><span class="se">\"</span><span class="s">,"</span>
        <span class="s">"  </span><span class="se">\"</span><span class="s">name</span><span class="se">\"</span><span class="s">: </span><span class="se">\"</span><span class="s">test</span><span class="se">\"</span><span class="s">,"</span>
        <span class="s">"  </span><span class="se">\"</span><span class="s">fields</span><span class="se">\"</span><span class="s"> : ["</span>
        <span class="s">"    {</span><span class="se">\"</span><span class="s">name</span><span class="se">\"</span><span class="s">: </span><span class="se">\"</span><span class="s">a</span><span class="se">\"</span><span class="s">, </span><span class="se">\"</span><span class="s">type</span><span class="se">\"</span><span class="s">: </span><span class="se">\"</span><span class="s">long</span><span class="se">\"</span><span class="s">},"</span>
        <span class="s">"    {</span><span class="se">\"</span><span class="s">name</span><span class="se">\"</span><span class="s">: </span><span class="se">\"</span><span class="s">b</span><span class="se">\"</span><span class="s">, </span><span class="se">\"</span><span class="s">type</span><span class="se">\"</span><span class="s">: </span><span class="se">\"</span><span class="s">string</span><span class="se">\"</span><span class="s">}"</span>
        <span class="s">"  ]"</span>
        <span class="s">"}"</span>
    <span class="p">)</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">format</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">avro</span>

  <span class="c1"># required: define the schema either by using an Avro specific record class</span>
  <span class="na">record-class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">org.organization.types.User"</span>

  <span class="c1"># or by using an Avro schema</span>
  <span class="na">avro-schema</span><span class="pi">:</span> <span class="pi">&gt;</span>
    <span class="s">{</span>
      <span class="s">"type": "record",</span>
      <span class="s">"name": "test",</span>
      <span class="s">"fields" : [</span>
        <span class="s">{"name": "a", "type": "long"},</span>
        <span class="s">{"name": "b", "type": "string"}</span>
      <span class="s">]</span>
    <span class="s">}</span></code></pre></figure>

  </div>
</div>

<p>Avro types are mapped to the corresponding SQL data types. Union types are only supported for specifying nullability otherwise they are converted to an <code class="highlighter-rouge">ANY</code> type. The following table shows the mapping:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Avro schema</th>
      <th style="text-align: left">Flink SQL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">record</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">ROW</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">enum</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">VARCHAR</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">array</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">ARRAY[_]</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">map</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">MAP[VARCHAR, _]</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">union</code></td>
      <td style="text-align: left">non-null type or <code class="highlighter-rouge">ANY</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">fixed</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">ARRAY[TINYINT]</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">string</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">VARCHAR</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">bytes</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">ARRAY[TINYINT]</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">int</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">INT</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">long</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">BIGINT</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">float</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">FLOAT</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">double</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">DOUBLE</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">boolean</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">BOOLEAN</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">int</code> with <code class="highlighter-rouge">logicalType: date</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">DATE</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">int</code> with <code class="highlighter-rouge">logicalType: time-millis</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">TIME</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">int</code> with <code class="highlighter-rouge">logicalType: time-micros</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">INT</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">long</code> with <code class="highlighter-rouge">logicalType: timestamp-millis</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">TIMESTAMP</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">long</code> with <code class="highlighter-rouge">logicalType: timestamp-micros</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">BIGINT</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">bytes</code> with <code class="highlighter-rouge">logicalType: decimal</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">DECIMAL</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">fixed</code> with <code class="highlighter-rouge">logicalType: decimal</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">DECIMAL</code></td>
    </tr>
    <tr>
      <td style="text-align: left"><code class="highlighter-rouge">null</code></td>
      <td style="text-align: left"><code class="highlighter-rouge">NULL</code> (unsupported yet)</td>
    </tr>
  </tbody>
</table>

<p>Avro uses <a href="http://www.joda.org/joda-time/">Joda-Time</a> for representing logical date and time types in specific record classes. The Joda-Time dependency is not part of Flink’s distribution. Therefore, make sure that Joda-Time is in your classpath together with your specific record class during runtime. Avro formats specified via a schema string do not require Joda-Time to be present.</p>

<p>Make sure to add the Apache Avro dependency.</p>

<h3 id="old-csv-format">Old CSV Format</h3>

<p><span class="label label-danger">Attention</span> For prototyping purposes only!</p>

<p>The old CSV format allows to read and write comma-separated rows using the filesystem connector.
The format schema is derived from the desired table schema.</p>

<p>This format describes Flink’s non-standard CSV table source/sink. In the future, the format will be
replaced by a proper RFC-compliant version. Use the RFC-compliant CSV format when writing to Kafka.
Use the old one for stream/batch filesystem operations for now.</p>

<div class="codetabs">
  <div data-lang="DDL">

    <figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">MyUserTable</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">'format.type'</span> <span class="o">=</span> <span class="s1">'csv'</span><span class="p">,</span>                  <span class="c1">-- required: specify the schema type</span>

  <span class="s1">'format.field-delimiter'</span> <span class="o">=</span> <span class="s1">','</span><span class="p">,</span>         <span class="c1">-- optional: string delimiter "," by default</span>
  <span class="s1">'format.line-delimiter'</span> <span class="o">=</span> <span class="n">U</span><span class="o">&amp;</span><span class="s1">'</span><span class="se">\0</span><span class="s1">00A'</span><span class="p">,</span>    <span class="c1">-- optional: string delimiter line feed by default, unicode is</span>
                                          <span class="c1">-- supported if the delimiter is an invisible special character,</span>
                                          <span class="c1">-- e.g. U&amp;'\000A' is the unicode representation of line feed "\n"</span>
  <span class="s1">'format.quote-character'</span> <span class="o">=</span> <span class="s1">'"'</span><span class="p">,</span>         <span class="c1">-- optional: single character for string values, empty by default</span>
  <span class="s1">'format.comment-prefix'</span> <span class="o">=</span> <span class="s1">'#'</span><span class="p">,</span>          <span class="c1">-- optional: string to indicate comments, empty by default</span>
  <span class="s1">'format.ignore-first-line'</span> <span class="o">=</span> <span class="s1">'false'</span><span class="p">,</span>   <span class="c1">-- optional: boolean flag to ignore the first line,</span>
                                          <span class="c1">-- by default it is not skipped</span>
  <span class="s1">'format.ignore-parse-errors'</span> <span class="o">=</span> <span class="s1">'true'</span>   <span class="c1">-- optional: skip records with parse error instead of failing by default</span>
<span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="Java/Scala">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="o">.</span><span class="na">withFormat</span><span class="o">(</span>
  <span class="k">new</span> <span class="nf">OldCsv</span><span class="o">()</span>
    <span class="o">.</span><span class="na">fieldDelimiter</span><span class="o">(</span><span class="s">","</span><span class="o">)</span>              <span class="c1">// optional: string delimiter "," by default</span>
    <span class="o">.</span><span class="na">lineDelimiter</span><span class="o">(</span><span class="s">"\n"</span><span class="o">)</span>              <span class="c1">// optional: string delimiter "\n" by default</span>
    <span class="o">.</span><span class="na">quoteCharacter</span><span class="o">(</span><span class="sc">'"'</span><span class="o">)</span>              <span class="c1">// optional: single character for string values, empty by default</span>
    <span class="o">.</span><span class="na">commentPrefix</span><span class="o">(</span><span class="sc">'#'</span><span class="o">)</span>               <span class="c1">// optional: string to indicate comments, empty by default</span>
    <span class="o">.</span><span class="na">ignoreFirstLine</span><span class="o">()</span>                <span class="c1">// optional: ignore the first line, by default it is not skipped</span>
    <span class="o">.</span><span class="na">ignoreParseErrors</span><span class="o">()</span>              <span class="c1">// optional: skip records with parse error instead of failing by default</span>
<span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">.</span><span class="n">with_format</span><span class="p">(</span>
    <span class="n">OldCsv</span><span class="p">()</span>
    <span class="p">.</span><span class="n">field_delimiter</span><span class="p">(</span><span class="s">","</span><span class="p">)</span>              <span class="c1"># optional: string delimiter "," by default
</span>    <span class="p">.</span><span class="n">line_delimiter</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>              <span class="c1"># optional: string delimiter "\n" by default
</span>    <span class="p">.</span><span class="n">quote_character</span><span class="p">(</span><span class="s">'"'</span><span class="p">)</span>              <span class="c1"># optional: single character for string values, empty by default
</span>    <span class="p">.</span><span class="n">comment_prefix</span><span class="p">(</span><span class="s">'#'</span><span class="p">)</span>               <span class="c1"># optional: string to indicate comments, empty by default
</span>    <span class="p">.</span><span class="n">ignore_first_line</span><span class="p">()</span>               <span class="c1"># optional: ignore the first line, by default it is not skipped
</span>    <span class="p">.</span><span class="n">ignore_parse_errors</span><span class="p">()</span>             <span class="c1"># optional: skip records with parse error instead of failing by default
</span><span class="p">)</span></code></pre></figure>

  </div>

  <div data-lang="YAML">

    <figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="na">format</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">csv</span>
  <span class="na">field-delimiter</span><span class="pi">:</span> <span class="s2">"</span><span class="s">,"</span>       <span class="c1"># optional: string delimiter "," by default</span>
  <span class="na">line-delimiter</span><span class="pi">:</span> <span class="s2">"</span><span class="se">\n</span><span class="s">"</span>       <span class="c1"># optional: string delimiter "\n" by default</span>
  <span class="na">quote-character</span><span class="pi">:</span> <span class="s1">'</span><span class="s">"'</span>       <span class="c1"># optional: single character for string values, empty by default</span>
  <span class="na">comment-prefix</span><span class="pi">:</span> <span class="s1">'</span><span class="s">#'</span>        <span class="c1"># optional: string to indicate comments, empty by default</span>
  <span class="na">ignore-first-line</span><span class="pi">:</span> <span class="no">false</span>   <span class="c1"># optional: boolean flag to ignore the first line, by default it is not skipped</span>
  <span class="na">ignore-parse-errors</span><span class="pi">:</span> <span class="no">true</span>  <span class="c1"># optional: skip records with parse error instead of failing by default</span></code></pre></figure>

  </div>
</div>

<p>The old CSV format is included in Flink and does not require additional dependencies.</p>

<p><span class="label label-danger">Attention</span> The old CSV format for writing rows is limited at the moment.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h2 id="further-tablesources-and-tablesinks">Further TableSources and TableSinks</h2>

<p>The following table sources and sinks have not yet been migrated (or have not been migrated entirely) to the new unified interfaces.</p>

<p>These are the additional <code class="highlighter-rouge">TableSource</code>s which are provided with Flink:</p>

<table>
  <tbody>
    <tr>
      <td><strong>Class name</strong></td>
      <td><strong>Maven dependency</strong></td>
      <td><strong>Batch?</strong></td>
      <td><strong>Streaming?</strong></td>
      <td><strong>Description</strong></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">OrcTableSource</code></td>
      <td><code class="highlighter-rouge">flink-orc</code></td>
      <td>Y</td>
      <td>N</td>
      <td>A <code class="highlighter-rouge">TableSource</code> for ORC files.</td>
    </tr>
  </tbody>
</table>

<p>These are the additional <code class="highlighter-rouge">TableSink</code>s which are provided with Flink:</p>

<table>
  <tbody>
    <tr>
      <td><strong>Class name</strong></td>
      <td><strong>Maven dependency</strong></td>
      <td><strong>Batch?</strong></td>
      <td><strong>Streaming?</strong></td>
      <td><strong>Description</strong></td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">CsvTableSink</code></td>
      <td><code class="highlighter-rouge">flink-table</code></td>
      <td>Y</td>
      <td>Append</td>
      <td>A simple sink for CSV files.</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">CassandraAppendTableSink</code></td>
      <td><code class="highlighter-rouge">flink-connector-cassandra</code></td>
      <td>N</td>
      <td>Append</td>
      <td>Writes a Table to a Cassandra table.</td>
    </tr>
  </tbody>
</table>

<h3 id="orctablesource">OrcTableSource</h3>

<p>The <code class="highlighter-rouge">OrcTableSource</code> reads <a href="https://orc.apache.org">ORC files</a>. ORC is a file format for structured data and stores the data in a compressed, columnar representation. ORC is very storage efficient and supports projection and filter push-down.</p>

<p>An <code class="highlighter-rouge">OrcTableSource</code> is created as shown below:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="c1">// create Hadoop Configuration</span>
<span class="nc">Configuration</span> <span class="n">config</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">();</span>

<span class="nc">OrcTableSource</span> <span class="n">orcTableSource</span> <span class="o">=</span> <span class="nc">OrcTableSource</span><span class="o">.</span><span class="na">builder</span><span class="o">()</span>
  <span class="c1">// path to ORC file(s). NOTE: By default, directories are recursively scanned.</span>
  <span class="o">.</span><span class="na">path</span><span class="o">(</span><span class="s">"file:///path/to/data"</span><span class="o">)</span>
  <span class="c1">// schema of ORC files</span>
  <span class="o">.</span><span class="na">forOrcSchema</span><span class="o">(</span><span class="s">"struct&lt;name:string,addresses:array&lt;struct&lt;street:string,zip:smallint&gt;&gt;&gt;"</span><span class="o">)</span>
  <span class="c1">// Hadoop configuration</span>
  <span class="o">.</span><span class="na">withConfiguration</span><span class="o">(</span><span class="n">config</span><span class="o">)</span>
  <span class="c1">// build OrcTableSource</span>
  <span class="o">.</span><span class="na">build</span><span class="o">();</span></code></pre></figure>

  </div>

  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">// create Hadoop Configuration</span>
<span class="k">val</span> <span class="nv">config</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Configuration</span><span class="o">()</span>

<span class="k">val</span> <span class="nv">orcTableSource</span> <span class="k">=</span> <span class="nv">OrcTableSource</span><span class="o">.</span><span class="py">builder</span><span class="o">()</span>
  <span class="c1">// path to ORC file(s). NOTE: By default, directories are recursively scanned.</span>
  <span class="o">.</span><span class="py">path</span><span class="o">(</span><span class="s">"file:///path/to/data"</span><span class="o">)</span>
  <span class="c1">// schema of ORC files</span>
  <span class="o">.</span><span class="py">forOrcSchema</span><span class="o">(</span><span class="s">"struct&lt;name:string,addresses:array&lt;struct&lt;street:string,zip:smallint&gt;&gt;&gt;"</span><span class="o">)</span>
  <span class="c1">// Hadoop configuration</span>
  <span class="o">.</span><span class="py">withConfiguration</span><span class="o">(</span><span class="n">config</span><span class="o">)</span>
  <span class="c1">// build OrcTableSource</span>
  <span class="o">.</span><span class="py">build</span><span class="o">()</span></code></pre></figure>

  </div>
</div>

<p><strong>Note:</strong> The <code class="highlighter-rouge">OrcTableSource</code> does not support ORC’s <code class="highlighter-rouge">Union</code> type yet.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>

<h3 id="csvtablesink">CsvTableSink</h3>

<p>The <code class="highlighter-rouge">CsvTableSink</code> emits a <code class="highlighter-rouge">Table</code> to one or more CSV files.</p>

<p>The sink only supports append-only streaming tables. It cannot be used to emit a <code class="highlighter-rouge">Table</code> that is continuously updated. See the <a href="./streaming/dynamic_tables.html#table-to-stream-conversion">documentation on Table to Stream conversions</a> for details. When emitting a streaming table, rows are written at least once (if checkpointing is enabled) and the <code class="highlighter-rouge">CsvTableSink</code> does not split output files into bucket files but continuously writes to the same files.</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">CsvTableSink</span> <span class="n">sink</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">CsvTableSink</span><span class="o">(</span>
    <span class="n">path</span><span class="o">,</span>                  <span class="c1">// output path</span>
    <span class="s">"|"</span><span class="o">,</span>                   <span class="c1">// optional: delimit files by '|'</span>
    <span class="mi">1</span><span class="o">,</span>                     <span class="c1">// optional: write to a single file</span>
    <span class="nc">WriteMode</span><span class="o">.</span><span class="na">OVERWRITE</span><span class="o">);</span>  <span class="c1">// optional: override existing files</span>

<span class="n">tableEnv</span><span class="o">.</span><span class="na">registerTableSink</span><span class="o">(</span>
  <span class="s">"csvOutputTable"</span><span class="o">,</span>
  <span class="c1">// specify table schema</span>
  <span class="k">new</span> <span class="nc">String</span><span class="o">[]{</span><span class="s">"f0"</span><span class="o">,</span> <span class="s">"f1"</span><span class="o">},</span>
  <span class="k">new</span> <span class="nc">TypeInformation</span><span class="o">[]{</span><span class="nc">Types</span><span class="o">.</span><span class="na">STRING</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="na">INT</span><span class="o">},</span>
  <span class="n">sink</span><span class="o">);</span>

<span class="nc">Table</span> <span class="n">table</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">table</span><span class="o">.</span><span class="na">executeInsert</span><span class="o">(</span><span class="s">"csvOutputTable"</span><span class="o">);</span></code></pre></figure>

  </div>

  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">sink</span><span class="k">:</span> <span class="kt">CsvTableSink</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">CsvTableSink</span><span class="o">(</span>
    <span class="n">path</span><span class="o">,</span>                             <span class="c1">// output path</span>
    <span class="n">fieldDelim</span> <span class="k">=</span> <span class="s">"|"</span><span class="o">,</span>                 <span class="c1">// optional: delimit files by '|'</span>
    <span class="n">numFiles</span> <span class="k">=</span> <span class="mi">1</span><span class="o">,</span>                     <span class="c1">// optional: write to a single file</span>
    <span class="n">writeMode</span> <span class="k">=</span> <span class="nv">WriteMode</span><span class="o">.</span><span class="py">OVERWRITE</span><span class="o">)</span>  <span class="c1">// optional: override existing files</span>

<span class="nv">tableEnv</span><span class="o">.</span><span class="py">registerTableSink</span><span class="o">(</span>
  <span class="s">"csvOutputTable"</span><span class="o">,</span>
  <span class="c1">// specify table schema</span>
  <span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">"f0"</span><span class="o">,</span> <span class="s">"f1"</span><span class="o">),</span>
  <span class="nc">Array</span><span class="o">[</span><span class="kt">TypeInformation</span><span class="o">[</span><span class="k">_</span><span class="o">]](</span><span class="nv">Types</span><span class="o">.</span><span class="py">STRING</span><span class="o">,</span> <span class="nv">Types</span><span class="o">.</span><span class="py">INT</span><span class="o">),</span>
  <span class="n">sink</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="o">???</span>
<span class="nv">table</span><span class="o">.</span><span class="py">executeInsert</span><span class="o">(</span><span class="s">"csvOutputTable"</span><span class="o">)</span></code></pre></figure>

  </div>

  <div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">field_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"f0"</span><span class="p">,</span> <span class="s">"f1"</span><span class="p">]</span>
<span class="n">field_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">DataTypes</span><span class="p">.</span><span class="n">STRING</span><span class="p">(),</span> <span class="n">DataTypes</span><span class="p">.</span><span class="n">INT</span><span class="p">()]</span>

<span class="n">sink</span> <span class="o">=</span> <span class="n">CsvTableSink</span><span class="p">(</span>
    <span class="n">field_names</span><span class="p">,</span>
    <span class="n">field_types</span><span class="p">,</span>
    <span class="n">path</span><span class="p">,</span>                 <span class="c1"># output path
</span>    <span class="s">"|"</span><span class="p">,</span>                  <span class="c1"># optional: delimit files by '|'
</span>    <span class="mi">1</span><span class="p">,</span>                    <span class="c1"># optional: write to a single file
</span>    <span class="n">WriteMode</span><span class="p">.</span><span class="n">OVERWRITE</span>   <span class="c1"># optional: override existing files
</span><span class="p">)</span>

<span class="n">table_env</span><span class="p">.</span><span class="n">register_table_sink</span><span class="p">(</span>
    <span class="s">"csvOutputTable"</span><span class="p">,</span>
    <span class="n">sink</span>
<span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="p">...</span>
<span class="n">table</span><span class="p">.</span><span class="n">execute_insert</span><span class="p">(</span><span class="s">"csvOutputTable"</span><span class="p">).</span><span class="n">wait</span><span class="p">()</span></code></pre></figure>

  </div>
</div>

<h3 id="cassandraappendtablesink">CassandraAppendTableSink</h3>

<p>The <code class="highlighter-rouge">CassandraAppendTableSink</code> emits a <code class="highlighter-rouge">Table</code> to a Cassandra table. The sink only supports append-only streaming tables. It cannot be used to emit a <code class="highlighter-rouge">Table</code> that is continuously updated. See the <a href="./streaming/dynamic_tables.html#table-to-stream-conversion">documentation on Table to Stream conversions</a> for details.</p>

<p>The <code class="highlighter-rouge">CassandraAppendTableSink</code> inserts all rows at least once into the Cassandra table if checkpointing is enabled. However, you can specify the query as upsert query.</p>

<p>To use the <code class="highlighter-rouge">CassandraAppendTableSink</code>, you have to add the Cassandra connector dependency (<code>flink-connector-cassandra</code>) to your project. The example below shows how to use the <code class="highlighter-rouge">CassandraAppendTableSink</code>.</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">ClusterBuilder</span> <span class="n">builder</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// configure Cassandra cluster connection</span>

<span class="nc">CassandraAppendTableSink</span> <span class="n">sink</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">CassandraAppendTableSink</span><span class="o">(</span>
  <span class="n">builder</span><span class="o">,</span>
  <span class="c1">// the query must match the schema of the table</span>
  <span class="s">"INSERT INTO flink.myTable (id, name, value) VALUES (?, ?, ?)"</span><span class="o">);</span>

<span class="n">tableEnv</span><span class="o">.</span><span class="na">registerTableSink</span><span class="o">(</span>
  <span class="s">"cassandraOutputTable"</span><span class="o">,</span>
  <span class="c1">// specify table schema</span>
  <span class="k">new</span> <span class="nc">String</span><span class="o">[]{</span><span class="s">"id"</span><span class="o">,</span> <span class="s">"name"</span><span class="o">,</span> <span class="s">"value"</span><span class="o">},</span>
  <span class="k">new</span> <span class="nc">TypeInformation</span><span class="o">[]{</span><span class="nc">Types</span><span class="o">.</span><span class="na">INT</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="na">STRING</span><span class="o">,</span> <span class="nc">Types</span><span class="o">.</span><span class="na">DOUBLE</span><span class="o">},</span>
  <span class="n">sink</span><span class="o">);</span>

<span class="nc">Table</span> <span class="n">table</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">table</span><span class="o">.</span><span class="na">insertInto</span><span class="o">(</span><span class="n">cassandraOutputTable</span><span class="o">);</span></code></pre></figure>

  </div>

  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">builder</span><span class="k">:</span> <span class="kt">ClusterBuilder</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// configure Cassandra cluster connection</span>

<span class="k">val</span> <span class="nv">sink</span><span class="k">:</span> <span class="kt">CassandraAppendTableSink</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">CassandraAppendTableSink</span><span class="o">(</span>
  <span class="n">builder</span><span class="o">,</span>
  <span class="c1">// the query must match the schema of the table</span>
  <span class="s">"INSERT INTO flink.myTable (id, name, value) VALUES (?, ?, ?)"</span><span class="o">)</span>

<span class="nv">tableEnv</span><span class="o">.</span><span class="py">registerTableSink</span><span class="o">(</span>
  <span class="s">"cassandraOutputTable"</span><span class="o">,</span>
  <span class="c1">// specify table schema</span>
  <span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">"id"</span><span class="o">,</span> <span class="s">"name"</span><span class="o">,</span> <span class="s">"value"</span><span class="o">),</span>
  <span class="nc">Array</span><span class="o">[</span><span class="kt">TypeInformation</span><span class="o">[</span><span class="k">_</span><span class="o">]](</span><span class="nv">Types</span><span class="o">.</span><span class="py">INT</span><span class="o">,</span> <span class="nv">Types</span><span class="o">.</span><span class="py">STRING</span><span class="o">,</span> <span class="nv">Types</span><span class="o">.</span><span class="py">DOUBLE</span><span class="o">),</span>
  <span class="n">sink</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">table</span><span class="k">:</span> <span class="kt">Table</span> <span class="o">=</span> <span class="o">???</span>
<span class="nv">table</span><span class="o">.</span><span class="py">insertInto</span><span class="o">(</span><span class="n">cassandraOutputTable</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>



<div class="footer">
  <a href="https://cwiki.apache.org/confluence/display/FLINK/Flink+Translation+Specifications" target="_blank">
    
      想参与贡献翻译？
    
  </a>
</div>


        </div>
      </div>
    </div><!-- /.container -->

    <!-- default code tab -->
    <script>var defaultCodeTab = "";</script>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/js/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.1.0/anchor.min.js"></script>
    <script src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/js/flink.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- Disqus -->
    
  </body>
</html>
