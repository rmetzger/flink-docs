<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink 1.12 Documentation: File Sink</title>
    <link rel="shortcut icon" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/favicon.ico" type="image/x-icon">
    <link rel="icon" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="//ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/file_sink.html">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/flink.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/syntax.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/codetabs.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/font-awesome/css/font-awesome.min.css">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    

    <!-- Main content. -->
    <div class="container">
      
      <div class="row">
        <div class="col-lg-3" id="sidenavcol">
          <div class="sidenav-logo">
  <p><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/"><img class="bottom" alt="Apache Flink" src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/img/navbar-brand-logo.jpg"></a> v1.12</p>
</div>
<ul id="sidenav">
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/"><i class="fa fa-home title" aria-hidden="true"></i> Home</a></li><hr class="section-break"></hr>
<li><a href="#collapse-2" data-toggle="collapse"><i class="fa fa-rocket title appetizer" aria-hidden="true"></i> Try Flink<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-2"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/local_installation.html">Local Installation</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/datastream_api.html">Fraud Detection with the DataStream API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/table_api.html">Real Time Reporting with the Table API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/flink-operations-playground.html">Flink Operations Playground</a></li>
</ul></div></li>
<li><a href="#collapse-8" data-toggle="collapse"><i class="fa fa-hand-paper-o title appetizer" aria-hidden="true"></i> Learn Flink<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-8"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/datastream_api.html">Intro to the DataStream API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/etl.html">Data Pipelines & ETL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/streaming_analytics.html">Streaming Analytics</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/event_driven.html">Event-driven Applications</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/fault_tolerance.html">Fault Tolerance</a></li>
</ul></div></li>
<li><a href="#collapse-15" data-toggle="collapse"><i class="fa fa-map-o title appetizer" aria-hidden="true"></i> Concepts<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-15"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/index.html">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/stateful-stream-processing.html">Stateful Stream Processing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/timely-stream-processing.html">Timely Stream Processing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/flink-architecture.html">Flink Architecture</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/glossary.html">Glossary</a></li>
</ul></div></li><hr class="section-break"></hr>
<li><a href="#collapse-21" data-toggle="collapse"><i class="fa fa-code title maindish" aria-hidden="true"></i> Application Development<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-21"><ul>
<li><a href="#collapse-22" data-toggle="collapse">DataStream API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-22"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/datastream_api.html">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/datastream_execution_mode.html">Execution Mode (Batch/Streaming)</a></li>
<li><a href="#collapse-24" data-toggle="collapse">Event Time<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-24"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_time.html">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamps_watermarks.html">Generating Watermarks</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamp_extractors.html">Builtin Watermark Generators</a></li>
</ul></div></li>
<li><a href="#collapse-28" data-toggle="collapse">State & Fault Tolerance<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-28"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state.html">Working with State</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/broadcast_state.html">The Broadcast State Pattern</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/checkpointing.html">Checkpointing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/queryable_state.html">Queryable State</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state_backends.html">State Backends</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/schema_evolution.html">State Schema Evolution</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/custom_serialization.html">Custom State Serialization</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/user_defined_functions.html">User-Defined Functions</a></li>
<li><a href="#collapse-38" data-toggle="collapse">Operators<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-38"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/windows.html">Windows</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/joining.html">Joining</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/process_function.html">Process Function</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/asyncio.html">Async I/O</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html">Data Sources</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/side_output.html">Side Outputs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/application_parameters.html">Handling Application Parameters</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/testing.html">Testing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/experimental.html">Experimental Features</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/scala_api_extensions.html">Scala API Extensions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/java_lambdas.html">Java Lambda Expressions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html">Project Configuration</a></li>
</ul></div></li>
<li><a href="#collapse-53" data-toggle="collapse">DataSet API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-53"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/dataset_transformations.html">Transformations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/iterations.html">Iterations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/zip_elements_guide.html">Zipping Elements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/hadoop_compatibility.html">Hadoop Compatibility</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/local_execution.html">Local Execution</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/cluster_execution.html">Cluster Execution</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/examples.html">Batch Examples</a></li>
</ul></div></li>
<li><a href="#collapse-62" data-toggle="collapse">Table API & SQL<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-62"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html">Concepts & Common API</a></li>
<li><a href="#collapse-64" data-toggle="collapse">Streaming Concepts<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-64"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/dynamic_tables.html">Dynamic Tables</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/time_attributes.html">Time Attributes</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/joins.html">Joins in Continuous Queries</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/temporal_tables.html">Temporal Tables</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/match_recognize.html">Detecting Patterns</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/query_configuration.html">Query Configuration</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/types.html">Data Types</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/tableApi.html">Table API</a></li>
<li><a href="#collapse-74" data-toggle="collapse">SQL<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-74"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/queries.html">Queries</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/create.html">CREATE Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/drop.html">DROP Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/alter.html">ALTER Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/insert.html">INSERT Statement</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/hints.html">SQL Hints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/describe.html">DESCRIBE Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/explain.html">EXPLAIN Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/use.html">USE Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/show.html">SHOW Statements</a></li>
</ul></div></li>
<li><a href="#collapse-86" data-toggle="collapse">Functions<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-86"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/functions/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/functions/systemFunctions.html">System (Built-in) Functions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/functions/udfs.html">User-defined Functions</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/modules.html">Modules</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/catalogs.html">Catalogs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sqlClient.html">SQL Client</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/config.html">Configuration</a></li>
<li><a href="#collapse-94" data-toggle="collapse">Performance Tuning<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-94"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/tuning/streaming_aggregation_optimization.html">Streaming Aggregation</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sourceSinks.html">User-defined Sources & Sinks</a></li>
</ul></div></li>
<li><a href="#collapse-99" data-toggle="collapse">Python API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-99"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/installation.html">Installation</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table_api_tutorial.html">Table API Tutorial</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/datastream_tutorial.html">DataStream API Tutorial</a></li>
<li><a href="#collapse-103" data-toggle="collapse">Table API User's Guide<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-103"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/intro_to_table_api.html">Intro to the Python Table API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/table_environment.html">TableEnvironment</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/operations.html">Operations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/python_types.html">Data Types</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/built_in_functions.html">System (Built-in) Functions</a></li>
<li><a href="#collapse-109" data-toggle="collapse">User Defined Functions<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-109"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/udfs/python_udfs.html">General User-defined Functions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/udfs/vectorized_python_udfs.html">Vectorized User-defined Functions</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/conversion_of_pandas.html">Conversions between PyFlink Table and Pandas DataFrame</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/dependency_management.html">Dependency Management</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/sql.html">SQL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/catalogs.html">Catalogs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/metrics.html">Metrics</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/python_table_api_connectors.html">Connectors</a></li>
</ul></div></li>
<li><a href="#collapse-120" data-toggle="collapse">DataStream API User's Guide<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-120"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/datastream-api-users-guide/data_types.html">Data Types</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/datastream-api-users-guide/operators.html">Operators</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/datastream-api-users-guide/dependency_management.html">Dependency Management</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/python_config.html">Configuration</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/environment_variables.html">Environment Variables</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/faq.html">FAQ</a></li>
</ul></div></li>
<li><a href="#collapse-129" data-toggle="collapse">Data Types & Serialization<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-129"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/types_serialization.html">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/custom_serializers.html">Custom Serializers</a></li>
</ul></div></li>
<li><a href="#collapse-132" data-toggle="collapse">Managing Execution<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-132"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/execution_configuration.html">Execution Configuration</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/packaging.html">Program Packaging</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/parallel.html">Parallel Execution</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/execution_plans.html">Execution Plans</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/task_failure_recovery.html">Task Failure Recovery</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/migration.html">API Migration Guides</a></li>
</ul></div></li>
<li><a href="#collapse-141" data-toggle="collapse"><i class="fa fa-book title maindish" aria-hidden="true"></i> Libraries<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-141"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html">Event Processing (CEP)</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/state_processor_api.html">State Processor API</a></li>
<li><a href="#collapse-144" data-toggle="collapse">Graphs: Gelly<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-144"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/graph_api.html">Graph API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/iterative_graph_processing.html">Iterative Graph Processing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/library_methods.html">Library Methods</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/graph_algorithms.html">Graph Algorithms</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/graph_generators.html">Graph Generators</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/bipartite_graph.html">Bipartite Graph</a></li>
</ul></div></li>
</ul></div></li>
<li><a href="#collapse-153" data-toggle="collapse"class="active"><i class="fa fa-random title maindish" aria-hidden="true"></i> Connectors</a><div class="collapse in" id="collapse-153"><ul>
<li><a href="#collapse-154" data-toggle="collapse"class="active">DataStream Connectors</a><div class="collapse in" id="collapse-154"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/guarantees.html">Fault Tolerance Guarantees</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html">Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/cassandra.html">Cassandra</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kinesis.html">Kinesis</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/elasticsearch.html">Elasticsearch</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/file_sink.html" class="active">File Sink</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/streamfile_sink.html">Streaming File Sink</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/rabbitmq.html">RabbitMQ</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/nifi.html">NiFi</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/pubsub.html">Google Cloud PubSub</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/twitter.html">Twitter</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/jdbc.html">JDBC</a></li>
</ul></div></li>
<li><a href="#collapse-168" data-toggle="collapse">Table & SQL Connectors<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-168"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/">Overview</a></li>
<li><a href="#collapse-169" data-toggle="collapse">Formats<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-169"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/csv.html">CSV</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/json.html">JSON</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/avro-confluent.html">Confluent Avro</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/avro.html">Avro</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/debezium.html">Debezium</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/canal.html">Canal</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/maxwell.html">Maxwell</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/parquet.html">Parquet</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/orc.html">Orc</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/raw.html">Raw</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html">Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/upsert-kafka.html">Upsert Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kinesis.html">Kinesis</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/jdbc.html">JDBC</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/elasticsearch.html">Elasticsearch</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html">FileSystem</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hbase.html">HBase</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/datagen.html">DataGen</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/print.html">Print</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/blackhole.html">BlackHole</a></li>
<li><a href="#collapse-191" data-toggle="collapse">Hive<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-191"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_catalog.html">Hive Catalog</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_dialect.html">Hive Dialect</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_read_write.html">Hive Read & Write</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_functions.html">Hive Functions</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/downloads.html">Download</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/connectors.html">DataSet Connectors</a></li>
</ul></div></li>
<li><a href="#collapse-201" data-toggle="collapse"><i class="fa fa-sliders title maindish" aria-hidden="true"></i> Deployment<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-201"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/">Overview</a></li>
<li><a href="#collapse-202" data-toggle="collapse">Resource Providers<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-202"><ul>
<li><a href="#collapse-203" data-toggle="collapse">Standalone<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-203"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/local.html">Local Cluster</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/docker.html">Docker</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/kubernetes.html">Kubernetes</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/native_kubernetes.html">Native Kubernetes</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/yarn.html">YARN</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/mesos.html">Mesos</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html">Configuration</a></li>
<li><a href="#collapse-213" data-toggle="collapse">Memory Configuration<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-213"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup.html">Set up Flink's Process Memory</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup_tm.html">Set up TaskManager Memory</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup_jobmanager.html">Set up JobManager Memory</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_tuning.html">Memory tuning guide</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_trouble.html">Troubleshooting</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_migration.html">Migration Guide</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/cli.html">Command-Line Interface</a></li>
<li><a href="#collapse-222" data-toggle="collapse">File Systems<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-222"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/common.html">Common Configurations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/s3.html">Amazon S3</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/oss.html">Aliyun OSS</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/azure.html">Azure Blob Storage</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/plugins.html">Plugins</a></li>
</ul></div></li>
<li><a href="#collapse-229" data-toggle="collapse">High Availability (HA)<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-229"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/zookeeper_ha.html">ZooKeeper HA Services</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/kubernetes_ha.html">Kubernetes HA Services</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/metric_reporters.html">Metric Reporters</a></li>
<li><a href="#collapse-234" data-toggle="collapse">Security<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-234"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/security/security-ssl.html">SSL Setup</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/security/security-kerberos.html">Kerberos</a></li>
</ul></div></li>
<li><a href="#collapse-238" data-toggle="collapse">REPLs<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-238"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/repls/python_shell.html">Python REPL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/repls/scala_shell.html">Scala REPL</a></li>
</ul></div></li>
<li><a href="#collapse-242" data-toggle="collapse">Advanced<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-242"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/advanced/external_resources.html">External Resources</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/advanced/historyserver.html">History Server</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/advanced/logging.html">Logging</a></li>
</ul></div></li>
</ul></div></li>
<li><a href="#collapse-248" data-toggle="collapse"><i class="fa fa-cogs title maindish" aria-hidden="true"></i> Operations<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-248"><ul>
<li><a href="#collapse-249" data-toggle="collapse">State & Fault Tolerance<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-249"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/checkpoints.html">Checkpoints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/savepoints.html">Savepoints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html">State Backends</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/large_state_tuning.html">Tuning Checkpoints and Large State</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/metrics.html">Metrics</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/rest_api.html">REST API</a></li>
<li><a href="#collapse-257" data-toggle="collapse">Debugging<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-257"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/debugging/debugging_event_time.html">Debugging Windows & Event Time</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/debugging/debugging_classloading.html">Debugging Classloading</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/debugging/application_profiling.html">Application Profiling & Debugging</a></li>
</ul></div></li>
<li><a href="#collapse-262" data-toggle="collapse">Monitoring<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-262"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/monitoring/checkpoint_monitoring.html">Monitoring Checkpointing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/monitoring/back_pressure.html">Monitoring Back Pressure</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/upgrading.html">Upgrading Applications and Flink Versions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/production_ready.html">Production Readiness Checklist</a></li>
</ul></div></li><hr class="section-break"></hr>
<li><a href="#collapse-269" data-toggle="collapse"><i class="fa fa-cogs title dessert" aria-hidden="true"></i> Flink Development<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-269"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/flinkDev/ide_setup.html">Importing Flink into an IDE</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/flinkDev/building.html">Building Flink from Source</a></li>
</ul></div></li>
<li><a href="#collapse-273" data-toggle="collapse"><i class="fa fa-book title dessert" aria-hidden="true"></i> Internals<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-273"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/internals/job_scheduling.html">Jobs and Scheduling</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/internals/task_lifecycle.html">Task Lifecycle</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/internals/filesystems.html">File Systems</a></li>
</ul></div></li>
  <li class="divider"></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java"><i class="fa fa-external-link title" aria-hidden="true"></i> Javadocs</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/scala/index.html#org.apache.flink.api.scala.package"><i class="fa fa-external-link title" aria-hidden="true"></i> Scaladocs</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/python"><i class="fa fa-external-link title" aria-hidden="true"></i> Pythondocs</a></li>
  <li><a href="http://flink.apache.org"><i class="fa fa-external-link title" aria-hidden="true"></i> Project Page</a></li>
</ul>

<div class="sidenav-search-box">
  <form class="navbar-form" role="search" action="//ci.apache.org/projects/flink/flink-docs-release-1.12/search-results.html">
    <div class="form-group">
      <input type="text" class="form-control" size="16px" name="q" placeholder="Search">
    </div>
    <button type="submit" class="btn btn-default">Go</button>
  </form>
</div>

<div class="sidenav-versions">
  <div class="dropdown">
    <button class="btn btn-default dropdown-toggle" type="button" data-toggle="dropdown">Pick Docs Version<span class="caret"></span></button>
    <ul class="dropdown-menu">
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.11">v1.11</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.10">v1.10</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.9">v1.9</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.8">v1.8</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.7">v1.7</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.6">v1.6</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.5">v1.5</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.4">v1.4</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.3">v1.3</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.2">v1.2</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.1">v1.1</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.0">v1.0</a></li>
    </ul>
  </div>
</div>

<div class="sidenav-languages"><!-- link to the Chinese home page when current is blog page -->
    <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/file_sink.html">
      <button type="submit" class="btn btn-default">中文版</button>
    </a>
</div>

        </div>
        <div class="col-lg-9 content" id="contentcol">

          

<ol class="breadcrumb">
  
    <li><i class="fa fa-random title maindish" aria-hidden="true"></i> Connectors</li>
  
    <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/">DataStream Connectors</a></li>
  
    <li class="active">File Sink</li>
</ol>

<h1>File Sink</h1>




<ul id="markdown-toc">
  <li><a href="#file-formats" id="markdown-toc-file-formats">File Formats</a>    <ul>
      <li><a href="#row-encoded-formats" id="markdown-toc-row-encoded-formats">Row-encoded Formats</a></li>
      <li><a href="#bulk-encoded-formats" id="markdown-toc-bulk-encoded-formats">Bulk-encoded Formats</a></li>
    </ul>
  </li>
  <li><a href="#bucket-assignment" id="markdown-toc-bucket-assignment">Bucket Assignment</a></li>
  <li><a href="#rolling-policy" id="markdown-toc-rolling-policy">Rolling Policy</a></li>
  <li><a href="#part-file-lifecycle" id="markdown-toc-part-file-lifecycle">Part file lifecycle</a>    <ul>
      <li><a href="#part-file-configuration" id="markdown-toc-part-file-configuration">Part file configuration</a></li>
    </ul>
  </li>
  <li><a href="#important-considerations" id="markdown-toc-important-considerations">Important Considerations</a>    <ul>
      <li><a href="#general" id="markdown-toc-general">General</a></li>
      <li><a href="#batch-specific" id="markdown-toc-batch-specific">BATCH-specific</a></li>
      <li><a href="#s3-specific" id="markdown-toc-s3-specific">S3-specific</a></li>
    </ul>
  </li>
</ul>

<p>This connector provides a unified Sink for <code class="highlighter-rouge">BATCH</code> and <code class="highlighter-rouge">STREAMING</code> that writes partitioned files to filesystems
supported by the <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/">Flink <code class="highlighter-rouge">FileSystem</code> abstraction</a>. This filesystem
connector provides the same guarantees for both <code class="highlighter-rouge">BATCH</code> and <code class="highlighter-rouge">STREAMING</code> and it is an evolution of the 
existing <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/streamfile_sink.html">Streaming File Sink</a> which was designed for providing exactly-once semantics for <code class="highlighter-rouge">STREAMING</code> execution.</p>

<p>The file sink writes incoming data into buckets. Given that the incoming streams can be unbounded,
data in each bucket is organized into part files of finite size. The bucketing behaviour is fully configurable
with a default time-based bucketing where we start writing a new bucket every hour. This means that each resulting
bucket will contain files with records received during 1 hour intervals from the stream.</p>

<p>Data within the bucket directories is split into part files. Each bucket will contain at least one part file for
each subtask of the sink that has received data for that bucket. Additional part files will be created according to the configurable
rolling policy. For <code class="highlighter-rouge">Row-encoded Formats</code> (see <a href="#file-formats">File Formats</a>) the default policy rolls part files based
on size, a timeout that specifies the maximum duration for which a file can be open, and a maximum inactivity
timeout after which the file is closed. For <code class="highlighter-rouge">Bulk-encoded Formats</code> we roll on every checkpoint and the user can 
specify additional conditions based on size or time.</p>

<div class="alert alert-info">
     <b>IMPORTANT:</b> Checkpointing needs to be enabled when using the `FileSink` in `STREAMING` mode. Part files 
     can only be finalized on successful checkpoints. If checkpointing is disabled, part files will forever stay 
     in the `in-progress` or the `pending` state, and cannot be safely read by downstream systems.
 </div>

<p><img src="//ci.apache.org/projects/flink/flink-docs-release-1.12/fig/streamfilesink_bucketing.png" class="center" style="width: 100%;" /></p>

<h2 id="file-formats">File Formats</h2>

<p>The <code class="highlighter-rouge">FileSink</code> supports both row-wise and bulk encoding formats, such as <a href="http://parquet.apache.org">Apache Parquet</a>.
These two variants come with their respective builders that can be created with the following static methods:</p>

<ul>
  <li>Row-encoded sink: <code class="highlighter-rouge">FileSink.forRowFormat(basePath, rowEncoder)</code></li>
  <li>Bulk-encoded sink: <code class="highlighter-rouge">FileSink.forBulkFormat(basePath, bulkWriterFactory)</code></li>
</ul>

<p>When creating either a row or a bulk encoded sink we have to specify the base path where the buckets will be
stored and the encoding logic for our data.</p>

<p>Please check out the JavaDoc for <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/connector/file/sink/FileSink.html">FileSink</a>
for all the configuration options and more documentation about the implementation of the different data formats.</p>

<h3 id="row-encoded-formats">Row-encoded Formats</h3>

<p>Row-encoded formats need to specify an <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/api/common/serialization/Encoder.html">Encoder</a>
that is used for serializing individual rows to the <code class="highlighter-rouge">OutputStream</code> of the in-progress part files.</p>

<p>In addition to the bucket assigner, the <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/connector/file/sink/FileSink.RowFormatBuilder.html">RowFormatBuilder</a> allows the user to specify:</p>

<ul>
  <li>Custom <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicy.html">RollingPolicy</a> : Rolling policy to override the DefaultRollingPolicy</li>
  <li>bucketCheckInterval (default = 1 min) : Millisecond interval for checking time based rolling policies</li>
</ul>

<p>Basic usage for writing String elements thus looks like this:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.api.common.serialization.SimpleStringEncoder</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.core.fs.Path</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy</span><span class="o">;</span>

<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="o">...;</span>

<span class="kd">final</span> <span class="nc">FileSink</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">sink</span> <span class="o">=</span> <span class="nc">FileSink</span>
    <span class="o">.</span><span class="na">forRowFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">),</span> <span class="k">new</span> <span class="nc">SimpleStringEncoder</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;(</span><span class="s">"UTF-8"</span><span class="o">))</span>
    <span class="o">.</span><span class="na">withRollingPolicy</span><span class="o">(</span>
        <span class="nc">DefaultRollingPolicy</span><span class="o">.</span><span class="na">builder</span><span class="o">()</span>
            <span class="o">.</span><span class="na">withRolloverInterval</span><span class="o">(</span><span class="nc">TimeUnit</span><span class="o">.</span><span class="na">MINUTES</span><span class="o">.</span><span class="na">toMillis</span><span class="o">(</span><span class="mi">15</span><span class="o">))</span>
            <span class="o">.</span><span class="na">withInactivityInterval</span><span class="o">(</span><span class="nc">TimeUnit</span><span class="o">.</span><span class="na">MINUTES</span><span class="o">.</span><span class="na">toMillis</span><span class="o">(</span><span class="mi">5</span><span class="o">))</span>
            <span class="o">.</span><span class="na">withMaxPartSize</span><span class="o">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">)</span>
            <span class="o">.</span><span class="na">build</span><span class="o">())</span>
	<span class="o">.</span><span class="na">build</span><span class="o">();</span>

<span class="n">input</span><span class="o">.</span><span class="na">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.api.common.serialization.SimpleStringEncoder</span>
<span class="k">import</span> <span class="nn">org.apache.flink.core.fs.Path</span>
<span class="k">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span>
<span class="k">import</span> <span class="nn">org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy</span>

<span class="k">val</span> <span class="nv">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="nv">sink</span><span class="k">:</span> <span class="kt">FileSink</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">FileSink</span>
    <span class="o">.</span><span class="py">forRowFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">),</span> <span class="k">new</span> <span class="nc">SimpleStringEncoder</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">"UTF-8"</span><span class="o">))</span>
    <span class="o">.</span><span class="py">withRollingPolicy</span><span class="o">(</span>
        <span class="nv">DefaultRollingPolicy</span><span class="o">.</span><span class="py">builder</span><span class="o">()</span>
            <span class="o">.</span><span class="py">withRolloverInterval</span><span class="o">(</span><span class="nv">TimeUnit</span><span class="o">.</span><span class="py">MINUTES</span><span class="o">.</span><span class="py">toMillis</span><span class="o">(</span><span class="mi">15</span><span class="o">))</span>
            <span class="o">.</span><span class="py">withInactivityInterval</span><span class="o">(</span><span class="nv">TimeUnit</span><span class="o">.</span><span class="py">MINUTES</span><span class="o">.</span><span class="py">toMillis</span><span class="o">(</span><span class="mi">5</span><span class="o">))</span>
            <span class="o">.</span><span class="py">withMaxPartSize</span><span class="o">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">)</span>
            <span class="o">.</span><span class="py">build</span><span class="o">())</span>
    <span class="o">.</span><span class="py">build</span><span class="o">()</span>

<span class="nv">input</span><span class="o">.</span><span class="py">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>This example creates a simple sink that assigns records to the default one hour time buckets. It also specifies
a rolling policy that rolls the in-progress part file on any of the following 3 conditions:</p>

<ul>
  <li>It contains at least 15 minutes worth of data</li>
  <li>It hasn’t received new records for the last 5 minutes</li>
  <li>The file size has reached 1 GB (after writing the last record)</li>
</ul>

<h3 id="bulk-encoded-formats">Bulk-encoded Formats</h3>

<p>Bulk-encoded sinks are created similarly to the row-encoded ones, but instead of
specifying an <code class="highlighter-rouge">Encoder</code>, we have to specify a <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/api/common/serialization/BulkWriter.Factory.html">BulkWriter.Factory</a>.
The <code class="highlighter-rouge">BulkWriter</code> logic defines how new elements are added and flushed, and how a batch of records
is finalized for further encoding purposes.</p>

<p>Flink comes with four built-in BulkWriter factories:</p>

<ul>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/parquet/ParquetWriterFactory.html">ParquetWriterFactory</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/avro/AvroWriterFactory.html">AvroWriterFactory</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/sequencefile/SequenceFileWriterFactory.html">SequenceFileWriterFactory</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/compress/CompressWriterFactory.html">CompressWriterFactory</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/orc/writer/OrcBulkWriterFactory.html">OrcBulkWriterFactory</a></li>
</ul>

<div class="alert alert-info">
     <b>IMPORTANT:</b> Bulk Formats can only have a rolling policy that extends the `CheckpointRollingPolicy`. 
     The latter rolls on every checkpoint. A policy can roll additionally based on size or processing time.
</div>

<h4 id="parquet-format">Parquet format</h4>

<p>Flink contains built in convenience methods for creating Parquet writer factories for Avro data. These methods
and their associated documentation can be found in the <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/parquet/avro/ParquetAvroWriters.html">ParquetAvroWriters</a> class.</p>

<p>For writing to other Parquet compatible data formats, users need to create the ParquetWriterFactory with a custom implementation of the <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/parquet/ParquetBuilder.html">ParquetBuilder</a> interface.</p>

<p>To use the Parquet bulk encoder in your application you need to add the following dependency:</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-parquet_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

<p>A <code class="highlighter-rouge">FileSink</code> that writes Avro data to Parquet format can be created like this:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.formats.parquet.avro.ParquetAvroWriters</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.avro.Schema</span><span class="o">;</span>


<span class="nc">Schema</span> <span class="n">schema</span> <span class="o">=</span> <span class="o">...;</span>
<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">GenericRecord</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="o">...;</span>

<span class="kd">final</span> <span class="nc">FileSink</span><span class="o">&lt;</span><span class="nc">GenericRecord</span><span class="o">&gt;</span> <span class="n">sink</span> <span class="o">=</span> <span class="nc">FileSink</span>
	<span class="o">.</span><span class="na">forBulkFormat</span><span class="o">(</span><span class="n">outputBasePath</span><span class="o">,</span> <span class="nc">ParquetAvroWriters</span><span class="o">.</span><span class="na">forGenericRecord</span><span class="o">(</span><span class="n">schema</span><span class="o">))</span>
	<span class="o">.</span><span class="na">build</span><span class="o">();</span>

<span class="n">input</span><span class="o">.</span><span class="na">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.formats.parquet.avro.ParquetAvroWriters</span>
<span class="k">import</span> <span class="nn">org.apache.avro.Schema</span>

<span class="k">val</span> <span class="nv">schema</span><span class="k">:</span> <span class="kt">Schema</span> <span class="o">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">GenericRecord</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="nv">sink</span><span class="k">:</span> <span class="kt">FileSink</span><span class="o">[</span><span class="kt">GenericRecord</span><span class="o">]</span> <span class="k">=</span> <span class="nc">FileSink</span>
    <span class="o">.</span><span class="py">forBulkFormat</span><span class="o">(</span><span class="n">outputBasePath</span><span class="o">,</span> <span class="nv">ParquetAvroWriters</span><span class="o">.</span><span class="py">forGenericRecord</span><span class="o">(</span><span class="n">schema</span><span class="o">))</span>
    <span class="o">.</span><span class="py">build</span><span class="o">()</span>

<span class="nv">input</span><span class="o">.</span><span class="py">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>Similarly, a <code class="highlighter-rouge">FileSink</code> that writes Protobuf data to Parquet format can be created like this:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.formats.parquet.protobuf.ParquetProtoWriters</span><span class="o">;</span>

<span class="c1">// ProtoRecord is a generated protobuf Message class.</span>
<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">ProtoRecord</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="o">...;</span>

<span class="kd">final</span> <span class="nc">FileSink</span><span class="o">&lt;</span><span class="nc">ProtoRecord</span><span class="o">&gt;</span> <span class="n">sink</span> <span class="o">=</span> <span class="nc">FileSink</span>
	<span class="o">.</span><span class="na">forBulkFormat</span><span class="o">(</span><span class="n">outputBasePath</span><span class="o">,</span> <span class="nc">ParquetProtoWriters</span><span class="o">.</span><span class="na">forType</span><span class="o">(</span><span class="nc">ProtoRecord</span><span class="o">.</span><span class="na">class</span><span class="o">))</span>
	<span class="o">.</span><span class="na">build</span><span class="o">();</span>

<span class="n">input</span><span class="o">.</span><span class="na">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.formats.parquet.protobuf.ParquetProtoWriters</span>

<span class="c1">// ProtoRecord is a generated protobuf Message class.</span>
<span class="k">val</span> <span class="nv">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">ProtoRecord</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="nv">sink</span><span class="k">:</span> <span class="kt">FileSink</span><span class="o">[</span><span class="kt">ProtoRecord</span><span class="o">]</span> <span class="k">=</span> <span class="nc">FileSink</span>
    <span class="o">.</span><span class="py">forBulkFormat</span><span class="o">(</span><span class="n">outputBasePath</span><span class="o">,</span> <span class="nv">ParquetProtoWriters</span><span class="o">.</span><span class="py">forType</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">ProtoRecord</span><span class="o">]))</span>
    <span class="o">.</span><span class="py">build</span><span class="o">()</span>

<span class="nv">input</span><span class="o">.</span><span class="py">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<h4 id="avro-format">Avro format</h4>

<p>Flink also provides built-in support for writing data into Avro files. A list of convenience methods to create
Avro writer factories and their associated documentation can be found in the 
<a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/avro/AvroWriters.html">AvroWriters</a> class.</p>

<p>To use the Avro writers in your application you need to add the following dependency:</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-avro<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

<p>A <code class="highlighter-rouge">FileSink</code> that writes data to Avro files can be created like this:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.formats.avro.AvroWriters</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.avro.Schema</span><span class="o">;</span>


<span class="nc">Schema</span> <span class="n">schema</span> <span class="o">=</span> <span class="o">...;</span>
<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">GenericRecord</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="o">...;</span>

<span class="kd">final</span> <span class="nc">FileSink</span><span class="o">&lt;</span><span class="nc">GenericRecord</span><span class="o">&gt;</span> <span class="n">sink</span> <span class="o">=</span> <span class="nc">FileSink</span>
	<span class="o">.</span><span class="na">forBulkFormat</span><span class="o">(</span><span class="n">outputBasePath</span><span class="o">,</span> <span class="nc">AvroWriters</span><span class="o">.</span><span class="na">forGenericRecord</span><span class="o">(</span><span class="n">schema</span><span class="o">))</span>
	<span class="o">.</span><span class="na">build</span><span class="o">();</span>

<span class="n">input</span><span class="o">.</span><span class="na">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.formats.avro.AvroWriters</span>
<span class="k">import</span> <span class="nn">org.apache.avro.Schema</span>

<span class="k">val</span> <span class="nv">schema</span><span class="k">:</span> <span class="kt">Schema</span> <span class="o">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">GenericRecord</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="nv">sink</span><span class="k">:</span> <span class="kt">FileSink</span><span class="o">[</span><span class="kt">GenericRecord</span><span class="o">]</span> <span class="k">=</span> <span class="nc">FileSink</span>
    <span class="o">.</span><span class="py">forBulkFormat</span><span class="o">(</span><span class="n">outputBasePath</span><span class="o">,</span> <span class="nv">AvroWriters</span><span class="o">.</span><span class="py">forGenericRecord</span><span class="o">(</span><span class="n">schema</span><span class="o">))</span>
    <span class="o">.</span><span class="py">build</span><span class="o">()</span>

<span class="nv">input</span><span class="o">.</span><span class="py">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>For creating customized Avro writers, e.g. enabling compression, users need to create the <code class="highlighter-rouge">AvroWriterFactory</code>
with a custom implementation of the <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/avro/AvroBuilder.html">AvroBuilder</a> interface:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">AvroWriterFactory</span><span class="o">&lt;?&gt;</span> <span class="n">factory</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">AvroWriterFactory</span><span class="o">&lt;&gt;((</span><span class="nc">AvroBuilder</span><span class="o">&lt;</span><span class="nc">Address</span><span class="o">&gt;)</span> <span class="n">out</span> <span class="o">-&gt;</span> <span class="o">{</span>
	<span class="nc">Schema</span> <span class="n">schema</span> <span class="o">=</span> <span class="nc">ReflectData</span><span class="o">.</span><span class="na">get</span><span class="o">().</span><span class="na">getSchema</span><span class="o">(</span><span class="nc">Address</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
	<span class="nc">DatumWriter</span><span class="o">&lt;</span><span class="nc">Address</span><span class="o">&gt;</span> <span class="n">datumWriter</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ReflectDatumWriter</span><span class="o">&lt;&gt;(</span><span class="n">schema</span><span class="o">);</span>

	<span class="nc">DataFileWriter</span><span class="o">&lt;</span><span class="nc">Address</span><span class="o">&gt;</span> <span class="n">dataFileWriter</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">DataFileWriter</span><span class="o">&lt;&gt;(</span><span class="n">datumWriter</span><span class="o">);</span>
	<span class="n">dataFileWriter</span><span class="o">.</span><span class="na">setCodec</span><span class="o">(</span><span class="nc">CodecFactory</span><span class="o">.</span><span class="na">snappyCodec</span><span class="o">());</span>
	<span class="n">dataFileWriter</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">schema</span><span class="o">,</span> <span class="n">out</span><span class="o">);</span>
	<span class="k">return</span> <span class="n">dataFileWriter</span><span class="o">;</span>
<span class="o">});</span>

<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">Address</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">stream</span><span class="o">.</span><span class="na">sinkTo</span><span class="o">(</span><span class="nc">FileSink</span><span class="o">.</span><span class="na">forBulkFormat</span><span class="o">(</span>
	<span class="n">outputBasePath</span><span class="o">,</span>
	<span class="n">factory</span><span class="o">).</span><span class="na">build</span><span class="o">());</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">factory</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">AvroWriterFactory</span><span class="o">[</span><span class="kt">Address</span><span class="o">](</span><span class="k">new</span> <span class="nc">AvroBuilder</span><span class="o">[</span><span class="kt">Address</span><span class="o">]()</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">def</span> <span class="nf">createWriter</span><span class="o">(</span><span class="n">out</span><span class="k">:</span> <span class="kt">OutputStream</span><span class="o">)</span><span class="k">:</span> <span class="kt">DataFileWriter</span><span class="o">[</span><span class="kt">Address</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
        <span class="k">val</span> <span class="nv">schema</span> <span class="k">=</span> <span class="nv">ReflectData</span><span class="o">.</span><span class="py">get</span><span class="o">.</span><span class="py">getSchema</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">Address</span><span class="o">])</span>
        <span class="k">val</span> <span class="nv">datumWriter</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ReflectDatumWriter</span><span class="o">[</span><span class="kt">Address</span><span class="o">](</span><span class="n">schema</span><span class="o">)</span>

        <span class="k">val</span> <span class="nv">dataFileWriter</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DataFileWriter</span><span class="o">[</span><span class="kt">Address</span><span class="o">](</span><span class="n">datumWriter</span><span class="o">)</span>
        <span class="nv">dataFileWriter</span><span class="o">.</span><span class="py">setCodec</span><span class="o">(</span><span class="nv">CodecFactory</span><span class="o">.</span><span class="py">snappyCodec</span><span class="o">)</span>
        <span class="nv">dataFileWriter</span><span class="o">.</span><span class="py">create</span><span class="o">(</span><span class="n">schema</span><span class="o">,</span> <span class="n">out</span><span class="o">)</span>
        <span class="n">dataFileWriter</span>
    <span class="o">}</span>
<span class="o">})</span>

<span class="k">val</span> <span class="nv">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Address</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="nv">stream</span><span class="o">.</span><span class="py">sinkTo</span><span class="o">(</span><span class="nv">FileSink</span><span class="o">.</span><span class="py">forBulkFormat</span><span class="o">(</span>
    <span class="n">outputBasePath</span><span class="o">,</span>
    <span class="n">factory</span><span class="o">).</span><span class="py">build</span><span class="o">());</span></code></pre></figure>

  </div>
</div>

<h4 id="orc-format">ORC Format</h4>

<p>To enable the data to be bulk encoded in ORC format, Flink offers <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/formats/orc/writers/OrcBulkWriterFactory.html">OrcBulkWriterFactory</a> 
which takes a concrete implementation of <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/orc/vector/Vectorizer.html">Vectorizer</a>.</p>

<p>Like any other columnar format that encodes data in bulk fashion, Flink’s <code class="highlighter-rouge">OrcBulkWriter</code> writes the input elements in batches. It uses 
ORC’s <code class="highlighter-rouge">VectorizedRowBatch</code> to achieve this.</p>

<p>Since the input element has to be transformed to a <code class="highlighter-rouge">VectorizedRowBatch</code>, users have to extend the abstract <code class="highlighter-rouge">Vectorizer</code> 
class and override the <code class="highlighter-rouge">vectorize(T element, VectorizedRowBatch batch)</code> method. As you can see, the method provides an 
instance of <code class="highlighter-rouge">VectorizedRowBatch</code> to be used directly by the users so users just have to write the logic to transform the 
input <code class="highlighter-rouge">element</code> to <code class="highlighter-rouge">ColumnVectors</code> and set them in the provided <code class="highlighter-rouge">VectorizedRowBatch</code> instance.</p>

<p>For example, if the input element is of type <code class="highlighter-rouge">Person</code> which looks like:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">class</span> <span class="nc">Person</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="kd">final</span> <span class="nc">String</span> <span class="n">name</span><span class="o">;</span>
    <span class="kd">private</span> <span class="kd">final</span> <span class="kt">int</span> <span class="n">age</span><span class="o">;</span>
    <span class="o">...</span>
<span class="o">}</span></code></pre></figure>

  </div>
</div>

<p>Then a child implementation to convert the element of type <code class="highlighter-rouge">Person</code> and set them in the <code class="highlighter-rouge">VectorizedRowBatch</code> can be like:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.hive.ql.exec.vector.LongColumnVector</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.io.IOException</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.io.Serializable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.nio.charset.StandardCharsets</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonVectorizer</span> <span class="kd">extends</span> <span class="nc">Vectorizer</span><span class="o">&lt;</span><span class="nc">Person</span><span class="o">&gt;</span> <span class="kd">implements</span> <span class="nc">Serializable</span> <span class="o">{</span>	
	<span class="kd">public</span> <span class="nf">PersonVectorizer</span><span class="o">(</span><span class="nc">String</span> <span class="n">schema</span><span class="o">)</span> <span class="o">{</span>
		<span class="kd">super</span><span class="o">(</span><span class="n">schema</span><span class="o">);</span>
	<span class="o">}</span>
	<span class="nd">@Override</span>
	<span class="kd">public</span> <span class="kt">void</span> <span class="nf">vectorize</span><span class="o">(</span><span class="nc">Person</span> <span class="n">element</span><span class="o">,</span> <span class="nc">VectorizedRowBatch</span> <span class="n">batch</span><span class="o">)</span> <span class="kd">throws</span> <span class="nc">IOException</span> <span class="o">{</span>
		<span class="nc">BytesColumnVector</span> <span class="n">nameColVector</span> <span class="o">=</span> <span class="o">(</span><span class="nc">BytesColumnVector</span><span class="o">)</span> <span class="n">batch</span><span class="o">.</span><span class="na">cols</span><span class="o">[</span><span class="mi">0</span><span class="o">];</span>
		<span class="nc">LongColumnVector</span> <span class="n">ageColVector</span> <span class="o">=</span> <span class="o">(</span><span class="nc">LongColumnVector</span><span class="o">)</span> <span class="n">batch</span><span class="o">.</span><span class="na">cols</span><span class="o">[</span><span class="mi">1</span><span class="o">];</span>
		<span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="na">size</span><span class="o">++;</span>
		<span class="n">nameColVector</span><span class="o">.</span><span class="na">setVal</span><span class="o">(</span><span class="n">row</span><span class="o">,</span> <span class="n">element</span><span class="o">.</span><span class="na">getName</span><span class="o">().</span><span class="na">getBytes</span><span class="o">(</span><span class="nc">StandardCharsets</span><span class="o">.</span><span class="na">UTF_8</span><span class="o">));</span>
		<span class="n">ageColVector</span><span class="o">.</span><span class="na">vector</span><span class="o">[</span><span class="n">row</span><span class="o">]</span> <span class="o">=</span> <span class="n">element</span><span class="o">.</span><span class="na">getAge</span><span class="o">();</span>
	<span class="o">}</span>
<span class="o">}</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">java.nio.charset.StandardCharsets</span>
<span class="k">import</span> <span class="nn">org.apache.hadoop.hive.ql.exec.vector.</span><span class="o">{</span><span class="nc">BytesColumnVector</span><span class="o">,</span> <span class="nc">LongColumnVector</span><span class="o">}</span>

<span class="k">class</span> <span class="nc">PersonVectorizer</span><span class="o">(</span><span class="n">schema</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">Vectorizer</span><span class="o">[</span><span class="kt">Person</span><span class="o">](</span><span class="n">schema</span><span class="o">)</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="nf">vectorize</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">Person</span><span class="o">,</span> <span class="n">batch</span><span class="k">:</span> <span class="kt">VectorizedRowBatch</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="nv">nameColVector</span> <span class="k">=</span> <span class="nv">batch</span><span class="o">.</span><span class="py">cols</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="py">asInstanceOf</span><span class="o">[</span><span class="kt">BytesColumnVector</span><span class="o">]</span>
    <span class="k">val</span> <span class="nv">ageColVector</span> <span class="k">=</span> <span class="nv">batch</span><span class="o">.</span><span class="py">cols</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="py">asInstanceOf</span><span class="o">[</span><span class="kt">LongColumnVector</span><span class="o">]</span>
    <span class="nv">nameColVector</span><span class="o">.</span><span class="py">setVal</span><span class="o">(</span><span class="nv">batch</span><span class="o">.</span><span class="py">size</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="nv">element</span><span class="o">.</span><span class="py">getName</span><span class="o">.</span><span class="py">getBytes</span><span class="o">(</span><span class="nv">StandardCharsets</span><span class="o">.</span><span class="py">UTF_8</span><span class="o">))</span>
    <span class="nv">ageColVector</span><span class="o">.</span><span class="py">vector</span><span class="o">(</span><span class="nv">batch</span><span class="o">.</span><span class="py">size</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="k">=</span> <span class="nv">element</span><span class="o">.</span><span class="py">getAge</span>
  <span class="o">}</span>

<span class="o">}</span></code></pre></figure>

  </div>
</div>

<p>To use the ORC bulk encoder in an application, users need to add the following dependency:</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-orc_2.11<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

<p>And then a <code class="highlighter-rouge">FileSink</code> that writes data in ORC format can be created like this:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.orc.writer.OrcBulkWriterFactory</span><span class="o">;</span>

<span class="nc">String</span> <span class="n">schema</span> <span class="o">=</span> <span class="s">"struct&lt;_col0:string,_col1:int&gt;"</span><span class="o">;</span>
<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">Person</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="o">...;</span>

<span class="kd">final</span> <span class="nc">OrcBulkWriterFactory</span><span class="o">&lt;</span><span class="nc">Person</span><span class="o">&gt;</span> <span class="n">writerFactory</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">OrcBulkWriterFactory</span><span class="o">&lt;&gt;(</span><span class="k">new</span> <span class="nc">PersonVectorizer</span><span class="o">(</span><span class="n">schema</span><span class="o">));</span>

<span class="kd">final</span> <span class="nc">FileSink</span><span class="o">&lt;</span><span class="nc">Person</span><span class="o">&gt;</span> <span class="n">sink</span> <span class="o">=</span> <span class="nc">FileSink</span>
	<span class="o">.</span><span class="na">forBulkFormat</span><span class="o">(</span><span class="n">outputBasePath</span><span class="o">,</span> <span class="n">writerFactory</span><span class="o">)</span>
	<span class="o">.</span><span class="na">build</span><span class="o">();</span>

<span class="n">input</span><span class="o">.</span><span class="na">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.orc.writer.OrcBulkWriterFactory</span>

<span class="k">val</span> <span class="nv">schema</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="s">"struct&lt;_col0:string,_col1:int&gt;"</span>
<span class="k">val</span> <span class="nv">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">Person</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">writerFactory</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">OrcBulkWriterFactory</span><span class="o">(</span><span class="k">new</span> <span class="nc">PersonVectorizer</span><span class="o">(</span><span class="n">schema</span><span class="o">));</span>

<span class="k">val</span> <span class="nv">sink</span><span class="k">:</span> <span class="kt">FileSink</span><span class="o">[</span><span class="kt">Person</span><span class="o">]</span> <span class="k">=</span> <span class="nc">FileSink</span>
    <span class="o">.</span><span class="py">forBulkFormat</span><span class="o">(</span><span class="n">outputBasePath</span><span class="o">,</span> <span class="n">writerFactory</span><span class="o">)</span>
    <span class="o">.</span><span class="py">build</span><span class="o">()</span>

<span class="nv">input</span><span class="o">.</span><span class="py">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>OrcBulkWriterFactory can also take Hadoop <code class="highlighter-rouge">Configuration</code> and <code class="highlighter-rouge">Properties</code> so that a custom Hadoop configuration and ORC 
writer properties can be provided.</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">String</span> <span class="n">schema</span> <span class="o">=</span> <span class="o">...;</span>
<span class="nc">Configuration</span> <span class="n">conf</span> <span class="o">=</span> <span class="o">...;</span>
<span class="nc">Properties</span> <span class="n">writerProperties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>

<span class="n">writerProps</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"orc.compress"</span><span class="o">,</span> <span class="s">"LZ4"</span><span class="o">);</span>
<span class="c1">// Other ORC supported properties can also be set similarly.</span>

<span class="kd">final</span> <span class="nc">OrcBulkWriterFactory</span><span class="o">&lt;</span><span class="nc">Person</span><span class="o">&gt;</span> <span class="n">writerFactory</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">OrcBulkWriterFactory</span><span class="o">&lt;&gt;(</span>
    <span class="k">new</span> <span class="nf">PersonVectorizer</span><span class="o">(</span><span class="n">schema</span><span class="o">),</span> <span class="n">writerProperties</span><span class="o">,</span> <span class="n">conf</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">schema</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">conf</span><span class="k">:</span> <span class="kt">Configuration</span> <span class="o">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">writerProperties</span><span class="k">:</span> <span class="kt">Properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>

<span class="nv">writerProps</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"orc.compress"</span><span class="o">,</span> <span class="s">"LZ4"</span><span class="o">)</span>
<span class="c1">// Other ORC supported properties can also be set similarly.</span>

<span class="k">val</span> <span class="nv">writerFactory</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">OrcBulkWriterFactory</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">PersonVectorizer</span><span class="o">(</span><span class="n">schema</span><span class="o">),</span> <span class="n">writerProperties</span><span class="o">,</span> <span class="n">conf</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>The complete list of ORC writer properties can be found <a href="https://orc.apache.org/docs/hive-config.html">here</a>.</p>

<p>Users who want to add user metadata to the ORC files can do so by calling <code class="highlighter-rouge">addUserMetadata(...)</code> inside the overriding 
<code class="highlighter-rouge">vectorize(...)</code> method.</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">PersonVectorizer</span> <span class="kd">extends</span> <span class="nc">Vectorizer</span><span class="o">&lt;</span><span class="nc">Person</span><span class="o">&gt;</span> <span class="kd">implements</span> <span class="nc">Serializable</span> <span class="o">{</span>	
	<span class="nd">@Override</span>
	<span class="kd">public</span> <span class="kt">void</span> <span class="nf">vectorize</span><span class="o">(</span><span class="nc">Person</span> <span class="n">element</span><span class="o">,</span> <span class="nc">VectorizedRowBatch</span> <span class="n">batch</span><span class="o">)</span> <span class="kd">throws</span> <span class="nc">IOException</span> <span class="o">{</span>
		<span class="o">...</span>
		<span class="nc">String</span> <span class="n">metadataKey</span> <span class="o">=</span> <span class="o">...;</span>
		<span class="nc">ByteBuffer</span> <span class="n">metadataValue</span> <span class="o">=</span> <span class="o">...;</span>
		<span class="k">this</span><span class="o">.</span><span class="na">addUserMetadata</span><span class="o">(</span><span class="n">metadataKey</span><span class="o">,</span> <span class="n">metadataValue</span><span class="o">);</span>
	<span class="o">}</span>
<span class="o">}</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">class</span> <span class="nc">PersonVectorizer</span><span class="o">(</span><span class="n">schema</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">Vectorizer</span><span class="o">[</span><span class="kt">Person</span><span class="o">](</span><span class="n">schema</span><span class="o">)</span> <span class="o">{</span>

  <span class="k">override</span> <span class="k">def</span> <span class="nf">vectorize</span><span class="o">(</span><span class="n">element</span><span class="k">:</span> <span class="kt">Person</span><span class="o">,</span> <span class="n">batch</span><span class="k">:</span> <span class="kt">VectorizedRowBatch</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="o">...</span>
    <span class="k">val</span> <span class="nv">metadataKey</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">val</span> <span class="nv">metadataValue</span><span class="k">:</span> <span class="kt">ByteBuffer</span> <span class="o">=</span> <span class="o">...</span>
    <span class="nf">addUserMetadata</span><span class="o">(</span><span class="n">metadataKey</span><span class="o">,</span> <span class="n">metadataValue</span><span class="o">)</span>
  <span class="o">}</span>

<span class="o">}</span></code></pre></figure>

  </div>
</div>

<h4 id="hadoop-sequencefile-format">Hadoop SequenceFile format</h4>

<p>To use the <code class="highlighter-rouge">SequenceFile</code> bulk encoder in your application you need to add the following dependency:</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
  <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>flink-sequence-file<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

<p>A simple <code class="highlighter-rouge">SequenceFile</code> writer can be created like this:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.flink.configuration.GlobalConfiguration</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.conf.Configuration</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.LongWritable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.SequenceFile</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.Text</span><span class="o">;</span>


<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">Tuple2</span><span class="o">&lt;</span><span class="nc">LongWritable</span><span class="o">,</span> <span class="nc">Text</span><span class="o">&gt;&gt;</span> <span class="n">input</span> <span class="o">=</span> <span class="o">...;</span>
<span class="nc">Configuration</span> <span class="n">hadoopConf</span> <span class="o">=</span> <span class="nc">HadoopUtils</span><span class="o">.</span><span class="na">getHadoopConfiguration</span><span class="o">(</span><span class="nc">GlobalConfiguration</span><span class="o">.</span><span class="na">loadConfiguration</span><span class="o">());</span>
<span class="kd">final</span> <span class="nc">FileSink</span><span class="o">&lt;</span><span class="nc">Tuple2</span><span class="o">&lt;</span><span class="nc">LongWritable</span><span class="o">,</span> <span class="nc">Text</span><span class="o">&gt;&gt;</span> <span class="n">sink</span> <span class="o">=</span> <span class="nc">FileSink</span>
  <span class="o">.</span><span class="na">forBulkFormat</span><span class="o">(</span>
    <span class="n">outputBasePath</span><span class="o">,</span>
    <span class="k">new</span> <span class="nc">SequenceFileWriterFactory</span><span class="o">&lt;&gt;(</span><span class="n">hadoopConf</span><span class="o">,</span> <span class="nc">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="nc">Text</span><span class="o">.</span><span class="na">class</span><span class="o">))</span>
	<span class="o">.</span><span class="na">build</span><span class="o">();</span>

<span class="n">input</span><span class="o">.</span><span class="na">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.flink.connector.file.sink.FileSink</span><span class="o">;</span>
<span class="k">import</span> <span class="nn">org.apache.flink.configuration.GlobalConfiguration</span>
<span class="k">import</span> <span class="nn">org.apache.hadoop.conf.Configuration</span>
<span class="k">import</span> <span class="nn">org.apache.hadoop.io.LongWritable</span>
<span class="k">import</span> <span class="nn">org.apache.hadoop.io.SequenceFile</span>
<span class="k">import</span> <span class="nn">org.apache.hadoop.io.Text</span><span class="o">;</span>

<span class="k">val</span> <span class="nv">input</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[(</span><span class="kt">LongWritable</span>, <span class="kt">Text</span><span class="o">)]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="nv">hadoopConf</span><span class="k">:</span> <span class="kt">Configuration</span> <span class="o">=</span> <span class="nv">HadoopUtils</span><span class="o">.</span><span class="py">getHadoopConfiguration</span><span class="o">(</span><span class="nv">GlobalConfiguration</span><span class="o">.</span><span class="py">loadConfiguration</span><span class="o">())</span>
<span class="k">val</span> <span class="nv">sink</span><span class="k">:</span> <span class="kt">FileSink</span><span class="o">[(</span><span class="kt">LongWritable</span>, <span class="kt">Text</span><span class="o">)]</span> <span class="k">=</span> <span class="nc">FileSink</span>
  <span class="o">.</span><span class="py">forBulkFormat</span><span class="o">(</span>
    <span class="n">outputBasePath</span><span class="o">,</span>
    <span class="k">new</span> <span class="nc">SequenceFileWriterFactory</span><span class="o">(</span><span class="n">hadoopConf</span><span class="o">,</span> <span class="nv">LongWritable</span><span class="o">.</span><span class="py">class</span><span class="o">,</span> <span class="nv">Text</span><span class="o">.</span><span class="py">class</span><span class="o">))</span>
	<span class="o">.</span><span class="py">build</span><span class="o">()</span>

<span class="nv">input</span><span class="o">.</span><span class="py">sinkTo</span><span class="o">(</span><span class="n">sink</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>The <code class="highlighter-rouge">SequenceFileWriterFactory</code> supports additional constructor parameters to specify compression settings.</p>

<h2 id="bucket-assignment">Bucket Assignment</h2>

<p>The bucketing logic defines how the data will be structured into subdirectories inside the base output directory.</p>

<p>Both row and bulk formats (see <a href="#file-formats">File Formats</a>) use the <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/bucketassigners/DateTimeBucketAssigner.html">DateTimeBucketAssigner</a> as the default assigner.
By default the <code class="highlighter-rouge">DateTimeBucketAssigner</code> creates hourly buckets based on the system default timezone
with the following format: <code class="highlighter-rouge">yyyy-MM-dd--HH</code>. Both the date format (<em>i.e.</em> bucket size) and timezone can be
configured manually.</p>

<p>We can specify a custom <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketAssigner.html">BucketAssigner</a> by calling <code class="highlighter-rouge">.withBucketAssigner(assigner)</code> on the format builders.</p>

<p>Flink comes with two built-in BucketAssigners:</p>

<ul>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/bucketassigners/DateTimeBucketAssigner.html">DateTimeBucketAssigner</a> : Default time based assigner</li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/bucketassigners/BasePathBucketAssigner.html">BasePathBucketAssigner</a> : Assigner that stores all part files in the base path (single global bucket)</li>
</ul>

<h2 id="rolling-policy">Rolling Policy</h2>

<p>The <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/RollingPolicy.html">RollingPolicy</a> defines when a given in-progress part file will be closed and moved to the pending and later to finished state.
Part files in the “finished” state are the ones that are ready for viewing and are guaranteed to contain valid data that will not be reverted in case of failure.
In <code class="highlighter-rouge">STREAMING</code> mode, the Rolling Policy in combination with the checkpointing interval (pending files become finished on the next checkpoint) control how quickly
part files become available for downstream readers and also the size and number of these parts. In <code class="highlighter-rouge">BATCH</code> mode, part-files become visible at the end of the job but 
the rolling policy can control their maximum size.</p>

<p>Flink comes with two built-in RollingPolicies:</p>

<ul>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/rollingpolicies/DefaultRollingPolicy.html">DefaultRollingPolicy</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/rollingpolicies/OnCheckpointRollingPolicy.html">OnCheckpointRollingPolicy</a></li>
</ul>

<h2 id="part-file-lifecycle">Part file lifecycle</h2>

<p>In order to use the output of the <code class="highlighter-rouge">FileSink</code> in downstream systems, we need to understand the naming and lifecycle of the output files produced.</p>

<p>Part files can be in one of three states:</p>
<ol>
  <li><strong>In-progress</strong> : The part file that is currently being written to is in-progress</li>
  <li><strong>Pending</strong> : Closed (due to the specified rolling policy) in-progress files that are waiting to be committed</li>
  <li><strong>Finished</strong> : On successful checkpoints (<code class="highlighter-rouge">STREAMING</code>) or at the end of input (<code class="highlighter-rouge">BATCH</code>) pending files transition to “Finished”</li>
</ol>

<p>Only finished files are safe to read by downstream systems as those are guaranteed to not be modified later.</p>

<p>Each writer subtask will have a single in-progress part file at any given time for every active bucket, but there can be several pending and finished files.</p>

<p><strong>Part file example</strong></p>

<p>To better understand the lifecycle of these files let’s look at a simple example with 2 sink subtasks:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>└── 2019-08-25--12
    ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334
    └── part-81fc4980-a6af-41c8-9937-9939408a734b-0.inprogress.ea65a428-a1d0-4a0b-bbc5-7a436a75e575
</code></pre></div></div>

<p>When the part file <code class="highlighter-rouge">part-81fc4980-a6af-41c8-9937-9939408a734b-0</code> is rolled (let’s say it becomes too large), it becomes pending but it is not renamed. The sink then opens a new part file: <code class="highlighter-rouge">part-81fc4980-a6af-41c8-9937-9939408a734b-1</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>└── 2019-08-25--12
    ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334
    ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0.inprogress.ea65a428-a1d0-4a0b-bbc5-7a436a75e575
    └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11
</code></pre></div></div>

<p>As <code class="highlighter-rouge">part-81fc4980-a6af-41c8-9937-9939408a734b-0</code> is now pending completion, after the next successful checkpoint, it is finalized:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>└── 2019-08-25--12
    ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334
    ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0
    └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11
</code></pre></div></div>

<p>New buckets are created as dictated by the bucketing policy, and this doesn’t affect currently in-progress files:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>└── 2019-08-25--12
    ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334
    ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0
    └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11
└── 2019-08-25--13
    └── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.2b475fec-1482-4dea-9946-eb4353b475f1
</code></pre></div></div>

<p>Old buckets can still receive new records as the bucketing policy is evaluated on a per-record basis.</p>

<h3 id="part-file-configuration">Part file configuration</h3>

<p>Finished files can be distinguished from the in-progress ones by their naming scheme only.</p>

<p>By default, the file naming strategy is as follows:</p>
<ul>
  <li><strong>In-progress / Pending</strong>: <code class="highlighter-rouge">part-&lt;uid&gt;-&lt;partFileIndex&gt;.inprogress.uid</code></li>
  <li><strong>Finished:</strong> <code class="highlighter-rouge">part-&lt;uid&gt;-&lt;partFileIndex&gt;</code>
where <code class="highlighter-rouge">uid</code> is a random id assigned to a subtask of the sink when the subtask is instantiated. This <code class="highlighter-rouge">uid</code> is not fault-tolerant 
so it is regenerated when the subtask recovers from a failure.</li>
</ul>

<p>Flink allows the user to specify a prefix and/or a suffix for his/her part files. 
This can be done using an <code class="highlighter-rouge">OutputFileConfig</code>. 
For example for a prefix “prefix” and a suffix “.ext” the sink will create the following files:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>└── 2019-08-25--12
    ├── prefix-4005733d-a830-4323-8291-8866de98b582-0.ext
    ├── prefix-4005733d-a830-4323-8291-8866de98b582-1.ext.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334
    ├── prefix-81fc4980-a6af-41c8-9937-9939408a734b-0.ext
    └── prefix-81fc4980-a6af-41c8-9937-9939408a734b-1.ext.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11
</code></pre></div></div>

<p>The user can specify an <code class="highlighter-rouge">OutputFileConfig</code> in the following way:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">OutputFileConfig</span> <span class="n">config</span> <span class="o">=</span> <span class="nc">OutputFileConfig</span>
 <span class="o">.</span><span class="na">builder</span><span class="o">()</span>
 <span class="o">.</span><span class="na">withPartPrefix</span><span class="o">(</span><span class="s">"prefix"</span><span class="o">)</span>
 <span class="o">.</span><span class="na">withPartSuffix</span><span class="o">(</span><span class="s">".ext"</span><span class="o">)</span>
 <span class="o">.</span><span class="na">build</span><span class="o">();</span>
            
<span class="nc">FileSink</span><span class="o">&lt;</span><span class="nc">Tuple2</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">,</span> <span class="nc">Integer</span><span class="o">&gt;&gt;</span> <span class="n">sink</span> <span class="o">=</span> <span class="nc">FileSink</span>
 <span class="o">.</span><span class="na">forRowFormat</span><span class="o">((</span><span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">),</span> <span class="k">new</span> <span class="nc">SimpleStringEncoder</span><span class="o">&lt;&gt;(</span><span class="s">"UTF-8"</span><span class="o">))</span>
 <span class="o">.</span><span class="na">withBucketAssigner</span><span class="o">(</span><span class="k">new</span> <span class="nc">KeyBucketAssigner</span><span class="o">())</span>
 <span class="o">.</span><span class="na">withRollingPolicy</span><span class="o">(</span><span class="nc">OnCheckpointRollingPolicy</span><span class="o">.</span><span class="na">build</span><span class="o">())</span>
 <span class="o">.</span><span class="na">withOutputFileConfig</span><span class="o">(</span><span class="n">config</span><span class="o">)</span>
 <span class="o">.</span><span class="na">build</span><span class="o">();</span>
			</code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">config</span> <span class="k">=</span> <span class="nc">OutputFileConfig</span>
 <span class="o">.</span><span class="py">builder</span><span class="o">()</span>
 <span class="o">.</span><span class="py">withPartPrefix</span><span class="o">(</span><span class="s">"prefix"</span><span class="o">)</span>
 <span class="o">.</span><span class="py">withPartSuffix</span><span class="o">(</span><span class="s">".ext"</span><span class="o">)</span>
 <span class="o">.</span><span class="py">build</span><span class="o">()</span>
            
<span class="k">val</span> <span class="nv">sink</span> <span class="k">=</span> <span class="nc">FileSink</span>
 <span class="o">.</span><span class="py">forRowFormat</span><span class="o">(</span><span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">),</span> <span class="k">new</span> <span class="nc">SimpleStringEncoder</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">"UTF-8"</span><span class="o">))</span>
 <span class="o">.</span><span class="py">withBucketAssigner</span><span class="o">(</span><span class="k">new</span> <span class="nc">KeyBucketAssigner</span><span class="o">())</span>
 <span class="o">.</span><span class="py">withRollingPolicy</span><span class="o">(</span><span class="nv">OnCheckpointRollingPolicy</span><span class="o">.</span><span class="py">build</span><span class="o">())</span>
 <span class="o">.</span><span class="py">withOutputFileConfig</span><span class="o">(</span><span class="n">config</span><span class="o">)</span>
 <span class="o">.</span><span class="py">build</span><span class="o">()</span>
			</code></pre></figure>

  </div>
</div>

<h2 id="important-considerations">Important Considerations</h2>

<h3 id="general">General</h3>

<p><span class="label label-danger">Important Note 1</span>: When using Hadoop &lt; 2.7, please use
the <code class="highlighter-rouge">OnCheckpointRollingPolicy</code> which rolls part files on every checkpoint. The reason is that if part files “traverse”
the checkpoint interval, then, upon recovery from a failure the <code class="highlighter-rouge">FileSink</code> may use the <code class="highlighter-rouge">truncate()</code> method of the 
filesystem to discard uncommitted data from the in-progress file. This method is not supported by pre-2.7 Hadoop versions 
and Flink will throw an exception.</p>

<p><span class="label label-danger">Important Note 2</span>: Given that Flink sinks and UDFs in general do not differentiate between
normal job termination (<em>e.g.</em> finite input stream) and termination due to failure, upon normal termination of a job, the last 
in-progress files will not be transitioned to the “finished” state.</p>

<p><span class="label label-danger">Important Note 3</span>: Flink and the <code class="highlighter-rouge">FileSink</code> never overwrites committed data.
Given this, when trying to restore from an old checkpoint/savepoint which assumes an in-progress file which was committed
by subsequent successful checkpoints, the <code class="highlighter-rouge">FileSink</code> will refuse to resume and will throw an exception as it cannot locate the 
in-progress file.</p>

<p><span class="label label-danger">Important Note 4</span>: Currently, the <code class="highlighter-rouge">FileSink</code> only supports three filesystems: 
HDFS, S3, and Local. Flink will throw an exception when using an unsupported filesystem at runtime.</p>

<h3 id="batch-specific">BATCH-specific</h3>

<p><span class="label label-danger">Important Note 1</span>: Although the <code class="highlighter-rouge">Writer</code> is executed with the user-specified
parallelism, the <code class="highlighter-rouge">Committer</code> is executed with parallelism equal to 1.</p>

<p><span class="label label-danger">Important Note 2</span>: Pending files are committed, i.e. transition to <code class="highlighter-rouge">Finished</code> 
state, after the whole input has been processed.</p>

<p><span class="label label-danger">Important Note 3</span>: When High-Availability is activated, if a <code class="highlighter-rouge">JobManager</code> 
failure happens while the <code class="highlighter-rouge">Committers</code> are committing, then we may have duplicates. This is going to be fixed in<br />
future Flink versions 
(see progress in <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-147%3A+Support+Checkpoints+After+Tasks+Finished">FLIP-147</a>).</p>

<h3 id="s3-specific">S3-specific</h3>

<p><span class="label label-danger">Important Note 1</span>: For S3, the <code class="highlighter-rouge">FileSink</code>
supports only the <a href="https://hadoop.apache.org/">Hadoop-based</a> FileSystem implementation, not
the implementation based on <a href="https://prestodb.io/">Presto</a>. In case your job uses the
<code class="highlighter-rouge">FileSink</code> to write to S3 but you want to use the Presto-based one for checkpointing,
it is advised to use explicitly <em>“s3a://”</em> (for Hadoop) as the scheme for the target path of
the sink and <em>“s3p://”</em> for checkpointing (for Presto). Using <em>“s3://”</em> for both the sink
and checkpointing may lead to unpredictable behavior, as both implementations “listen” to that scheme.</p>

<p><span class="label label-danger">Important Note 2</span>: To guarantee exactly-once semantics while
being efficient, the <code class="highlighter-rouge">FileSink</code> uses the <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html">Multi-part Upload</a>
feature of S3 (MPU from now on). This feature allows to upload files in independent chunks (thus the “multi-part”)
which can be combined into the original file when all the parts of the MPU are successfully uploaded.
For inactive MPUs, S3 supports a bucket lifecycle rule that the user can use to abort multipart uploads
that don’t complete within a specified number of days after being initiated. This implies that if you set this rule
aggressively and take a savepoint with some part-files being not fully uploaded, their associated MPUs may time-out
before the job is restarted. This will result in your job not being able to restore from that savepoint as the
pending part-files are no longer there and Flink will fail with an exception as it tries to fetch them and fails.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>



<div class="footer">
  <a href="https://cwiki.apache.org/confluence/display/FLINK/Flink+Translation+Specifications" target="_blank">
    
      Want to contribute translation?
    
  </a>
</div>


        </div>
      </div>
    </div><!-- /.container -->

    <!-- default code tab -->
    <script>var defaultCodeTab = "";</script>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/js/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.1.0/anchor.min.js"></script>
    <script src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/js/flink.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- Disqus -->
    
  </body>
</html>
