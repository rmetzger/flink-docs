<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Apache Flink 1.12 Documentation: Apache Kafka Connector</title>
    <link rel="shortcut icon" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/favicon.ico" type="image/x-icon">
    <link rel="icon" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="//ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html">

    <!-- Bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/flink.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/syntax.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/css/codetabs.css">
    <link rel="stylesheet" href="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/font-awesome/css/font-awesome.min.css">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    

    <!-- Main content. -->
    <div class="container">
      
      <div class="row">
        <div class="col-lg-3" id="sidenavcol">
          <div class="sidenav-logo">
  <p><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/"><img class="bottom" alt="Apache Flink" src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/img/navbar-brand-logo.jpg"></a> v1.12</p>
</div>
<ul id="sidenav">
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/"><i class="fa fa-home title" aria-hidden="true"></i> Home</a></li><hr class="section-break"></hr>
<li><a href="#collapse-2" data-toggle="collapse"><i class="fa fa-rocket title appetizer" aria-hidden="true"></i> Try Flink<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-2"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/local_installation.html">Local Installation</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/datastream_api.html">Fraud Detection with the DataStream API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/table_api.html">Real Time Reporting with the Table API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/try-flink/flink-operations-playground.html">Flink Operations Playground</a></li>
</ul></div></li>
<li><a href="#collapse-8" data-toggle="collapse"><i class="fa fa-hand-paper-o title appetizer" aria-hidden="true"></i> Learn Flink<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-8"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/datastream_api.html">Intro to the DataStream API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/etl.html">Data Pipelines & ETL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/streaming_analytics.html">Streaming Analytics</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/event_driven.html">Event-driven Applications</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/learn-flink/fault_tolerance.html">Fault Tolerance</a></li>
</ul></div></li>
<li><a href="#collapse-15" data-toggle="collapse"><i class="fa fa-map-o title appetizer" aria-hidden="true"></i> Concepts<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-15"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/index.html">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/stateful-stream-processing.html">Stateful Stream Processing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/timely-stream-processing.html">Timely Stream Processing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/flink-architecture.html">Flink Architecture</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/glossary.html">Glossary</a></li>
</ul></div></li><hr class="section-break"></hr>
<li><a href="#collapse-21" data-toggle="collapse"><i class="fa fa-code title maindish" aria-hidden="true"></i> Application Development<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-21"><ul>
<li><a href="#collapse-22" data-toggle="collapse">DataStream API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-22"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/datastream_api.html">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/datastream_execution_mode.html">Execution Mode (Batch/Streaming)</a></li>
<li><a href="#collapse-24" data-toggle="collapse">Event Time<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-24"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_time.html">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamps_watermarks.html">Generating Watermarks</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamp_extractors.html">Builtin Watermark Generators</a></li>
</ul></div></li>
<li><a href="#collapse-28" data-toggle="collapse">State & Fault Tolerance<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-28"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state.html">Working with State</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/broadcast_state.html">The Broadcast State Pattern</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/checkpointing.html">Checkpointing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/queryable_state.html">Queryable State</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state_backends.html">State Backends</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/schema_evolution.html">State Schema Evolution</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/custom_serialization.html">Custom State Serialization</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/user_defined_functions.html">User-Defined Functions</a></li>
<li><a href="#collapse-38" data-toggle="collapse">Operators<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-38"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/windows.html">Windows</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/joining.html">Joining</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/process_function.html">Process Function</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/asyncio.html">Async I/O</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/sources.html">Data Sources</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/side_output.html">Side Outputs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/application_parameters.html">Handling Application Parameters</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/testing.html">Testing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/experimental.html">Experimental Features</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/scala_api_extensions.html">Scala API Extensions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/java_lambdas.html">Java Lambda Expressions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html">Project Configuration</a></li>
</ul></div></li>
<li><a href="#collapse-53" data-toggle="collapse">DataSet API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-53"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/dataset_transformations.html">Transformations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/iterations.html">Iterations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/zip_elements_guide.html">Zipping Elements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/hadoop_compatibility.html">Hadoop Compatibility</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/local_execution.html">Local Execution</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/cluster_execution.html">Cluster Execution</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/examples.html">Batch Examples</a></li>
</ul></div></li>
<li><a href="#collapse-62" data-toggle="collapse">Table API & SQL<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-62"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html">Concepts & Common API</a></li>
<li><a href="#collapse-64" data-toggle="collapse">Streaming Concepts<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-64"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/dynamic_tables.html">Dynamic Tables</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/time_attributes.html">Time Attributes</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/joins.html">Joins in Continuous Queries</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/temporal_tables.html">Temporal Tables</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/match_recognize.html">Detecting Patterns</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/query_configuration.html">Query Configuration</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/types.html">Data Types</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/tableApi.html">Table API</a></li>
<li><a href="#collapse-74" data-toggle="collapse">SQL<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-74"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/queries.html">Queries</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/create.html">CREATE Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/drop.html">DROP Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/alter.html">ALTER Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/insert.html">INSERT Statement</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/hints.html">SQL Hints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/describe.html">DESCRIBE Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/explain.html">EXPLAIN Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/use.html">USE Statements</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/show.html">SHOW Statements</a></li>
</ul></div></li>
<li><a href="#collapse-86" data-toggle="collapse">Functions<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-86"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/functions/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/functions/systemFunctions.html">System (Built-in) Functions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/functions/udfs.html">User-defined Functions</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/modules.html">Modules</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/catalogs.html">Catalogs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sqlClient.html">SQL Client</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/config.html">Configuration</a></li>
<li><a href="#collapse-94" data-toggle="collapse">Performance Tuning<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-94"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/tuning/streaming_aggregation_optimization.html">Streaming Aggregation</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sourceSinks.html">User-defined Sources & Sinks</a></li>
</ul></div></li>
<li><a href="#collapse-99" data-toggle="collapse">Python API<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-99"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/installation.html">Installation</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table_api_tutorial.html">Table API Tutorial</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/datastream_tutorial.html">DataStream API Tutorial</a></li>
<li><a href="#collapse-103" data-toggle="collapse">Table API User's Guide<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-103"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/intro_to_table_api.html">Intro to the Python Table API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/table_environment.html">TableEnvironment</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/operations.html">Operations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/python_types.html">Data Types</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/built_in_functions.html">System (Built-in) Functions</a></li>
<li><a href="#collapse-109" data-toggle="collapse">User Defined Functions<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-109"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/udfs/python_udfs.html">General User-defined Functions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/udfs/vectorized_python_udfs.html">Vectorized User-defined Functions</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/conversion_of_pandas.html">Conversions between PyFlink Table and Pandas DataFrame</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/dependency_management.html">Dependency Management</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/sql.html">SQL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/catalogs.html">Catalogs</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/metrics.html">Metrics</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/table-api-users-guide/python_table_api_connectors.html">Connectors</a></li>
</ul></div></li>
<li><a href="#collapse-120" data-toggle="collapse">DataStream API User's Guide<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-120"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/datastream-api-users-guide/data_types.html">Data Types</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/datastream-api-users-guide/operators.html">Operators</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/datastream-api-users-guide/dependency_management.html">Dependency Management</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/python_config.html">Configuration</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/environment_variables.html">Environment Variables</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/faq.html">FAQ</a></li>
</ul></div></li>
<li><a href="#collapse-129" data-toggle="collapse">Data Types & Serialization<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-129"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/types_serialization.html">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/custom_serializers.html">Custom Serializers</a></li>
</ul></div></li>
<li><a href="#collapse-132" data-toggle="collapse">Managing Execution<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-132"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/execution_configuration.html">Execution Configuration</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/packaging.html">Program Packaging</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/parallel.html">Parallel Execution</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/execution_plans.html">Execution Plans</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/task_failure_recovery.html">Task Failure Recovery</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/migration.html">API Migration Guides</a></li>
</ul></div></li>
<li><a href="#collapse-141" data-toggle="collapse"><i class="fa fa-book title maindish" aria-hidden="true"></i> Libraries<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-141"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/cep.html">Event Processing (CEP)</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/state_processor_api.html">State Processor API</a></li>
<li><a href="#collapse-144" data-toggle="collapse">Graphs: Gelly<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-144"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/graph_api.html">Graph API</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/iterative_graph_processing.html">Iterative Graph Processing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/library_methods.html">Library Methods</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/graph_algorithms.html">Graph Algorithms</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/graph_generators.html">Graph Generators</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/libs/gelly/bipartite_graph.html">Bipartite Graph</a></li>
</ul></div></li>
</ul></div></li>
<li><a href="#collapse-153" data-toggle="collapse"class="active"><i class="fa fa-random title maindish" aria-hidden="true"></i> Connectors</a><div class="collapse in" id="collapse-153"><ul>
<li><a href="#collapse-154" data-toggle="collapse"class="active">DataStream Connectors</a><div class="collapse in" id="collapse-154"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/guarantees.html">Fault Tolerance Guarantees</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html" class="active">Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/cassandra.html">Cassandra</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kinesis.html">Kinesis</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/elasticsearch.html">Elasticsearch</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/file_sink.html">File Sink</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/streamfile_sink.html">Streaming File Sink</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/rabbitmq.html">RabbitMQ</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/nifi.html">NiFi</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/pubsub.html">Google Cloud PubSub</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/twitter.html">Twitter</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/jdbc.html">JDBC</a></li>
</ul></div></li>
<li><a href="#collapse-168" data-toggle="collapse">Table & SQL Connectors<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-168"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/">Overview</a></li>
<li><a href="#collapse-169" data-toggle="collapse">Formats<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-169"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/csv.html">CSV</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/json.html">JSON</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/avro-confluent.html">Confluent Avro</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/avro.html">Avro</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/debezium.html">Debezium</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/canal.html">Canal</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/maxwell.html">Maxwell</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/parquet.html">Parquet</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/orc.html">Orc</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/raw.html">Raw</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html">Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/upsert-kafka.html">Upsert Kafka</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kinesis.html">Kinesis</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/jdbc.html">JDBC</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/elasticsearch.html">Elasticsearch</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html">FileSystem</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hbase.html">HBase</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/datagen.html">DataGen</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/print.html">Print</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/blackhole.html">BlackHole</a></li>
<li><a href="#collapse-191" data-toggle="collapse">Hive<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-191"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_catalog.html">Hive Catalog</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_dialect.html">Hive Dialect</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_read_write.html">Hive Read & Write</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/hive_functions.html">Hive Functions</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/downloads.html">Download</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/connectors.html">DataSet Connectors</a></li>
</ul></div></li>
<li><a href="#collapse-201" data-toggle="collapse"><i class="fa fa-sliders title maindish" aria-hidden="true"></i> Deployment<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-201"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/">Overview</a></li>
<li><a href="#collapse-202" data-toggle="collapse">Resource Providers<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-202"><ul>
<li><a href="#collapse-203" data-toggle="collapse">Standalone<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-203"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/local.html">Local Cluster</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/docker.html">Docker</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/kubernetes.html">Kubernetes</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/native_kubernetes.html">Native Kubernetes</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/yarn.html">YARN</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/mesos.html">Mesos</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html">Configuration</a></li>
<li><a href="#collapse-213" data-toggle="collapse">Memory Configuration<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-213"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup.html">Set up Flink's Process Memory</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup_tm.html">Set up TaskManager Memory</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup_jobmanager.html">Set up JobManager Memory</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_tuning.html">Memory tuning guide</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_trouble.html">Troubleshooting</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_migration.html">Migration Guide</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/cli.html">Command-Line Interface</a></li>
<li><a href="#collapse-222" data-toggle="collapse">File Systems<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-222"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/common.html">Common Configurations</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/s3.html">Amazon S3</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/oss.html">Aliyun OSS</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/azure.html">Azure Blob Storage</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/filesystems/plugins.html">Plugins</a></li>
</ul></div></li>
<li><a href="#collapse-229" data-toggle="collapse">High Availability (HA)<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-229"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/">Overview</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/zookeeper_ha.html">ZooKeeper HA Services</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/kubernetes_ha.html">Kubernetes HA Services</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/metric_reporters.html">Metric Reporters</a></li>
<li><a href="#collapse-234" data-toggle="collapse">Security<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-234"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/security/security-ssl.html">SSL Setup</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/security/security-kerberos.html">Kerberos</a></li>
</ul></div></li>
<li><a href="#collapse-238" data-toggle="collapse">REPLs<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-238"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/repls/python_shell.html">Python REPL</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/repls/scala_shell.html">Scala REPL</a></li>
</ul></div></li>
<li><a href="#collapse-242" data-toggle="collapse">Advanced<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-242"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/advanced/external_resources.html">External Resources</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/advanced/historyserver.html">History Server</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/advanced/logging.html">Logging</a></li>
</ul></div></li>
</ul></div></li>
<li><a href="#collapse-248" data-toggle="collapse"><i class="fa fa-cogs title maindish" aria-hidden="true"></i> Operations<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-248"><ul>
<li><a href="#collapse-249" data-toggle="collapse">State & Fault Tolerance<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-249"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/checkpoints.html">Checkpoints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/savepoints.html">Savepoints</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html">State Backends</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/large_state_tuning.html">Tuning Checkpoints and Large State</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/metrics.html">Metrics</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/rest_api.html">REST API</a></li>
<li><a href="#collapse-257" data-toggle="collapse">Debugging<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-257"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/debugging/debugging_event_time.html">Debugging Windows & Event Time</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/debugging/debugging_classloading.html">Debugging Classloading</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/debugging/application_profiling.html">Application Profiling & Debugging</a></li>
</ul></div></li>
<li><a href="#collapse-262" data-toggle="collapse">Monitoring<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-262"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/monitoring/checkpoint_monitoring.html">Monitoring Checkpointing</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/monitoring/back_pressure.html">Monitoring Back Pressure</a></li>
</ul></div></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/upgrading.html">Upgrading Applications and Flink Versions</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/production_ready.html">Production Readiness Checklist</a></li>
</ul></div></li><hr class="section-break"></hr>
<li><a href="#collapse-269" data-toggle="collapse"><i class="fa fa-cogs title dessert" aria-hidden="true"></i> Flink Development<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-269"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/flinkDev/ide_setup.html">Importing Flink into an IDE</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/flinkDev/building.html">Building Flink from Source</a></li>
</ul></div></li>
<li><a href="#collapse-273" data-toggle="collapse"><i class="fa fa-book title dessert" aria-hidden="true"></i> Internals<i class="fa fa-caret-down pull-right" aria-hidden="true" style="padding-top: 4px"></i></a><div class="collapse" id="collapse-273"><ul>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/internals/job_scheduling.html">Jobs and Scheduling</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/internals/task_lifecycle.html">Task Lifecycle</a></li>
<li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/internals/filesystems.html">File Systems</a></li>
</ul></div></li>
  <li class="divider"></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/java"><i class="fa fa-external-link title" aria-hidden="true"></i> Javadocs</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/scala/index.html#org.apache.flink.api.scala.package"><i class="fa fa-external-link title" aria-hidden="true"></i> Scaladocs</a></li>
  <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/api/python"><i class="fa fa-external-link title" aria-hidden="true"></i> Pythondocs</a></li>
  <li><a href="http://flink.apache.org"><i class="fa fa-external-link title" aria-hidden="true"></i> Project Page</a></li>
</ul>

<div class="sidenav-search-box">
  <form class="navbar-form" role="search" action="//ci.apache.org/projects/flink/flink-docs-release-1.12/search-results.html">
    <div class="form-group">
      <input type="text" class="form-control" size="16px" name="q" placeholder="Search">
    </div>
    <button type="submit" class="btn btn-default">Go</button>
  </form>
</div>

<div class="sidenav-versions">
  <div class="dropdown">
    <button class="btn btn-default dropdown-toggle" type="button" data-toggle="dropdown">Pick Docs Version<span class="caret"></span></button>
    <ul class="dropdown-menu">
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.11">v1.11</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.10">v1.10</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.9">v1.9</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.8">v1.8</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.7">v1.7</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.6">v1.6</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.5">v1.5</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.4">v1.4</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.3">v1.3</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.2">v1.2</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.1">v1.1</a></li>
      <li><a href="http://ci.apache.org/projects/flink/flink-docs-release-1.0">v1.0</a></li>
    </ul>
  </div>
</div>

<div class="sidenav-languages"><!-- link to the Chinese home page when current is blog page -->
    <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html">
      <button type="submit" class="btn btn-default">中文版</button>
    </a>
</div>

        </div>
        <div class="col-lg-9 content" id="contentcol">

          

<ol class="breadcrumb">
  
    <li><i class="fa fa-random title maindish" aria-hidden="true"></i> Connectors</li>
  
    <li><a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/">DataStream Connectors</a></li>
  
    <li class="active">Kafka</li>
</ol>

<h1>Apache Kafka Connector</h1>




<p>Flink provides an <a href="https://kafka.apache.org">Apache Kafka</a> connector for reading data from and writing data to Kafka topics with exactly-once guarantees.</p>

<ul id="markdown-toc">
  <li><a href="#dependency" id="markdown-toc-dependency">Dependency</a></li>
  <li><a href="#kafka-consumer" id="markdown-toc-kafka-consumer">Kafka Consumer</a>    <ul>
      <li><a href="#the-deserializationschema" id="markdown-toc-the-deserializationschema">The <code class="highlighter-rouge">DeserializationSchema</code></a></li>
      <li><a href="#kafka-consumers-start-position-configuration" id="markdown-toc-kafka-consumers-start-position-configuration">Kafka Consumers Start Position Configuration</a></li>
      <li><a href="#kafka-consumers-and-fault-tolerance" id="markdown-toc-kafka-consumers-and-fault-tolerance">Kafka Consumers and Fault Tolerance</a></li>
      <li><a href="#kafka-consumers-topic-and-partition-discovery" id="markdown-toc-kafka-consumers-topic-and-partition-discovery">Kafka Consumers Topic and Partition Discovery</a></li>
      <li><a href="#kafka-consumers-offset-committing-behaviour-configuration" id="markdown-toc-kafka-consumers-offset-committing-behaviour-configuration">Kafka Consumers Offset Committing Behaviour Configuration</a></li>
      <li><a href="#kafka-consumers-and-timestamp-extractionwatermark-emission" id="markdown-toc-kafka-consumers-and-timestamp-extractionwatermark-emission">Kafka Consumers and Timestamp Extraction/Watermark Emission</a></li>
    </ul>
  </li>
  <li><a href="#kafka-producer" id="markdown-toc-kafka-producer">Kafka Producer</a></li>
  <li><a href="#the-serializationschema" id="markdown-toc-the-serializationschema">The <code class="highlighter-rouge">SerializationSchema</code></a>    <ul>
      <li><a href="#kafka-producers-and-fault-tolerance" id="markdown-toc-kafka-producers-and-fault-tolerance">Kafka Producers and Fault Tolerance</a></li>
    </ul>
  </li>
  <li><a href="#kafka-connector-metrics" id="markdown-toc-kafka-connector-metrics">Kafka Connector Metrics</a></li>
  <li><a href="#enabling-kerberos-authentication" id="markdown-toc-enabling-kerberos-authentication">Enabling Kerberos Authentication</a></li>
  <li><a href="#upgrading-to-the-latest-connector-version" id="markdown-toc-upgrading-to-the-latest-connector-version">Upgrading to the Latest Connector Version</a></li>
  <li><a href="#troubleshooting" id="markdown-toc-troubleshooting">Troubleshooting</a>    <ul>
      <li><a href="#data-loss" id="markdown-toc-data-loss">Data loss</a></li>
      <li><a href="#unknowntopicorpartitionexception" id="markdown-toc-unknowntopicorpartitionexception">UnknownTopicOrPartitionException</a></li>
    </ul>
  </li>
</ul>

<h2 id="dependency">Dependency</h2>

<p>Apache Flink ships with a universal Kafka connector which attempts to track the latest version of the Kafka client.
The version of the client it uses may change between Flink releases.
Modern Kafka clients are backwards compatible with broker versions 0.10.0 or later.
For details on Kafka compatibility, please refer to the official <a href="https://kafka.apache.org/protocol.html#protocol_compatibility">Kafka documentation</a>.</p>

<div class="codetabs">
  <div data-lang="universal">

    <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
	<span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
	<span class="nt">&lt;artifactId&gt;</span>flink-connector-kafka_2.11<span class="nt">&lt;/artifactId&gt;</span>
	<span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

  </div>
</div>

<p>Flink’s streaming connectors are not currently part of the binary distribution.
See how to link with them for cluster execution <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/project-configuration.html">here</a>.</p>

<h2 id="kafka-consumer">Kafka Consumer</h2>

<p>Flink’s Kafka consumer - <code class="highlighter-rouge">FlinkKafkaConsumer</code> provides access to read from one or more Kafka topics.</p>

<p>The constructor accepts the following arguments:</p>

<ol>
  <li>The topic name / list of topic names</li>
  <li>A DeserializationSchema / KafkaDeserializationSchema for deserializing the data from Kafka</li>
  <li>Properties for the Kafka consumer.
  The following properties are required:
    <ul>
      <li>“bootstrap.servers” (comma separated list of Kafka brokers)</li>
      <li>“group.id” the id of the consumer group</li>
    </ul>
  </li>
</ol>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">);</span>
<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span>
	<span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">));</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">stream</span> <span class="k">=</span> <span class="n">env</span>
    <span class="o">.</span><span class="py">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">))</span></code></pre></figure>

  </div>
</div>

<h3 id="the-deserializationschema">The <code class="highlighter-rouge">DeserializationSchema</code></h3>

<p>The Flink Kafka Consumer needs to know how to turn the binary data in Kafka into Java/Scala objects.
The <code class="highlighter-rouge">KafkaDeserializationSchema</code> allows users to specify such a schema. The <code class="highlighter-rouge">T deserialize(ConsumerRecord&lt;byte[], byte[]&gt; record)</code> method gets called for each Kafka message, passing the value from Kafka.</p>

<p>For convenience, Flink provides the following schemas out of the box:</p>

<ol>
  <li>
    <p><code class="highlighter-rouge">TypeInformationSerializationSchema</code> (and <code class="highlighter-rouge">TypeInformationKeyValueSerializationSchema</code>) which creates
 a schema based on a Flink’s <code class="highlighter-rouge">TypeInformation</code>. This is useful if the data is both written and read by Flink.
 This schema is a performant Flink-specific alternative to other generic serialization approaches.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">JsonDeserializationSchema</code> (and <code class="highlighter-rouge">JSONKeyValueDeserializationSchema</code>) which turns the serialized JSON
 into an ObjectNode object, from which fields can be accessed using <code class="highlighter-rouge">objectNode.get("field").as(Int/String/...)()</code>.
 The KeyValue objectNode contains a “key” and “value” field which contain all fields, as well as
 an optional “metadata” field that exposes the offset/partition/topic for this message.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">AvroDeserializationSchema</code> which reads data serialized with Avro format using a statically provided schema. It can
 infer the schema from Avro generated classes (<code class="highlighter-rouge">AvroDeserializationSchema.forSpecific(...)</code>) or it can work with <code class="highlighter-rouge">GenericRecords</code>
 with a manually provided schema (with <code class="highlighter-rouge">AvroDeserializationSchema.forGeneric(...)</code>). This deserialization schema expects that
 the serialized records DO NOT contain embedded schema.</p>

    <ul>
      <li>There is also a version of this schema available that can lookup the writer’s schema (schema which was used to write the record) in
<a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Confluent Schema Registry</a>. Using these deserialization schema
record will be read with the schema that was retrieved from Schema Registry and transformed to a statically provided( either through 
<code class="highlighter-rouge">ConfluentRegistryAvroDeserializationSchema.forGeneric(...)</code> or <code class="highlighter-rouge">ConfluentRegistryAvroDeserializationSchema.forSpecific(...)</code>).</li>
    </ul>

    <p><br />To use this deserialization schema one has to add the following additional dependency:</p>
  </li>
</ol>

<div class="codetabs">
  <div data-lang="AvroDeserializationSchema">

    <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>flink-avro<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

  </div>
  <div data-lang="ConfluentRegistryAvroDeserializationSchema">

    <figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.flink<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>flink-avro-confluent-registry<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>1.12.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span></code></pre></figure>

  </div>
</div>

<p>When encountering a corrupted message that cannot be deserialized for any reason the deserialization schema should return null which will result in the record being skipped.
Due to the consumer’s fault tolerance (see below sections for more details), failing the job on the corrupted message will let the consumer attempt to deserialize the message again.
Therefore, if deserialization still fails, the consumer will fall into a non-stop restart and fail loop on that corrupted message.</p>

<h3 id="kafka-consumers-start-position-configuration">Kafka Consumers Start Position Configuration</h3>

<p>The Flink Kafka Consumer allows configuring how the start positions for Kafka partitions are determined.</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="nc">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">myConsumer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(...);</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromEarliest</span><span class="o">();</span>     <span class="c1">// start from the earliest record possible</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromLatest</span><span class="o">();</span>       <span class="c1">// start from the latest record</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromTimestamp</span><span class="o">(...);</span> <span class="c1">// start from specified epoch timestamp (milliseconds)</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromGroupOffsets</span><span class="o">();</span> <span class="c1">// the default behaviour</span>

<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">);</span>
<span class="o">...</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">env</span> <span class="k">=</span> <span class="nv">StreamExecutionEnvironment</span><span class="o">.</span><span class="py">getExecutionEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="nv">myConsumer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">[</span><span class="kt">String</span><span class="o">](...)</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromEarliest</span><span class="o">()</span>      <span class="c1">// start from the earliest record possible</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromLatest</span><span class="o">()</span>        <span class="c1">// start from the latest record</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromTimestamp</span><span class="o">(...)</span>  <span class="c1">// start from specified epoch timestamp (milliseconds)</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromGroupOffsets</span><span class="o">()</span>  <span class="c1">// the default behaviour</span>

<span class="k">val</span> <span class="nv">stream</span> <span class="k">=</span> <span class="nv">env</span><span class="o">.</span><span class="py">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">)</span>
<span class="o">...</span></code></pre></figure>

  </div>
</div>

<p>All versions of the Flink Kafka Consumer have the above explicit configuration methods for start position.</p>

<ul>
  <li><code class="highlighter-rouge">setStartFromGroupOffsets</code> (default behaviour): Start reading partitions from
 the consumer group’s (<code class="highlighter-rouge">group.id</code> setting in the consumer properties) committed
 offsets in Kafka brokers. If offsets could not be
 found for a partition, the <code class="highlighter-rouge">auto.offset.reset</code> setting in the properties will be used.</li>
  <li><code class="highlighter-rouge">setStartFromEarliest()</code> / <code class="highlighter-rouge">setStartFromLatest()</code>: Start from the earliest / latest
 record. Under these modes, committed offsets in Kafka will be ignored and
 not used as starting positions. If offsets become out of range for a partition,
 the <code class="highlighter-rouge">auto.offset.reset</code> setting in the properties will be used.</li>
  <li><code class="highlighter-rouge">setStartFromTimestamp(long)</code>: Start from the specified timestamp. For each partition, the record
 whose timestamp is larger than or equal to the specified timestamp will be used as the start position.
 If a partition’s latest record is earlier than the timestamp, the partition will simply be read
 from the latest record. Under this mode, committed offsets in Kafka will be ignored and not used as
 starting positions.</li>
</ul>

<p>You can also specify the exact offsets the consumer should start from for each partition:</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">Map</span><span class="o">&lt;</span><span class="nc">KafkaTopicPartition</span><span class="o">,</span> <span class="nc">Long</span><span class="o">&gt;</span> <span class="n">specificStartOffsets</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">&lt;&gt;();</span>
<span class="n">specificStartOffsets</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">0</span><span class="o">),</span> <span class="mi">23L</span><span class="o">);</span>
<span class="n">specificStartOffsets</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">1</span><span class="o">),</span> <span class="mi">31L</span><span class="o">);</span>
<span class="n">specificStartOffsets</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="mi">43L</span><span class="o">);</span>

<span class="n">myConsumer</span><span class="o">.</span><span class="na">setStartFromSpecificOffsets</span><span class="o">(</span><span class="n">specificStartOffsets</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">specificStartOffsets</span> <span class="k">=</span> <span class="k">new</span> <span class="nv">java</span><span class="o">.</span><span class="py">util</span><span class="o">.</span><span class="py">HashMap</span><span class="o">[</span><span class="kt">KafkaTopicPartition</span>, <span class="kt">java.lang.Long</span><span class="o">]()</span>
<span class="nv">specificStartOffsets</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">0</span><span class="o">),</span> <span class="mi">23L</span><span class="o">)</span>
<span class="nv">specificStartOffsets</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">1</span><span class="o">),</span> <span class="mi">31L</span><span class="o">)</span>
<span class="nv">specificStartOffsets</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="k">new</span> <span class="nc">KafkaTopicPartition</span><span class="o">(</span><span class="s">"myTopic"</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="mi">43L</span><span class="o">)</span>

<span class="nv">myConsumer</span><span class="o">.</span><span class="py">setStartFromSpecificOffsets</span><span class="o">(</span><span class="n">specificStartOffsets</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>The above example configures the consumer to start from the specified offsets for
partitions 0, 1, and 2 of topic <code class="highlighter-rouge">myTopic</code>. The offset values should be the
next record that the consumer should read for each partition. Note that
if the consumer needs to read a partition which does not have a specified
offset within the provided offsets map, it will fallback to the default
group offsets behaviour (i.e. <code class="highlighter-rouge">setStartFromGroupOffsets()</code>) for that
particular partition.</p>

<p>Note that these start position configuration methods do not affect the start position when the job is
automatically restored from a failure or manually restored using a savepoint.
On restore, the start position of each Kafka partition is determined by the
offsets stored in the savepoint or checkpoint
(please see the next section for information about checkpointing to enable
fault tolerance for the consumer).</p>

<h3 id="kafka-consumers-and-fault-tolerance">Kafka Consumers and Fault Tolerance</h3>

<p>With Flink’s checkpointing enabled, the Flink Kafka Consumer will consume records from a topic and periodically checkpoint all
its Kafka offsets, together with the state of other operations. In case of a job failure, Flink will restore
the streaming program to the state of the latest checkpoint and re-consume the records from Kafka, starting from the offsets that were
stored in the checkpoint.</p>

<p>The interval of drawing checkpoints therefore defines how much the program may have to go back at most, in case of a failure.
To use fault tolerant Kafka Consumers, checkpointing of the topology needs to be enabled in the <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html#execution-checkpointing-interval">job</a>.</p>

<p>If checkpointing is disabled, the Kafka consumer will periodically commit the offsets to Zookeeper.</p>

<h3 id="kafka-consumers-topic-and-partition-discovery">Kafka Consumers Topic and Partition Discovery</h3>

<h4 id="partition-discovery">Partition discovery</h4>

<p>The Flink Kafka Consumer supports discovering dynamically created Kafka partitions, and consumes them with
exactly-once guarantees. All partitions discovered after the initial retrieval of partition metadata (i.e., when the
job starts running) will be consumed from the earliest possible offset.</p>

<p>By default, partition discovery is disabled. To enable it, set a non-negative value
for <code class="highlighter-rouge">flink.partition-discovery.interval-millis</code> in the provided properties config,
representing the discovery interval in milliseconds.</p>

<h4 id="topic-discovery">Topic discovery</h4>

<p>The Kafka Consumer is also capable of discovering topics by matching topic names using regular expressions.</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">final</span> <span class="nc">StreamExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="nc">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

<span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">);</span>

<span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">myConsumer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(</span>
    <span class="n">java</span><span class="o">.</span><span class="na">util</span><span class="o">.</span><span class="na">regex</span><span class="o">.</span><span class="na">Pattern</span><span class="o">.</span><span class="na">compile</span><span class="o">(</span><span class="s">"test-topic-[0-9]"</span><span class="o">),</span>
    <span class="k">new</span> <span class="nf">SimpleStringSchema</span><span class="o">(),</span>
    <span class="n">properties</span><span class="o">);</span>

<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">);</span>
<span class="o">...</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">env</span> <span class="k">=</span> <span class="nv">StreamExecutionEnvironment</span><span class="o">.</span><span class="py">getExecutionEnvironment</span><span class="o">()</span>

<span class="k">val</span> <span class="nv">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">myConsumer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span>
  <span class="nv">java</span><span class="o">.</span><span class="py">util</span><span class="o">.</span><span class="py">regex</span><span class="o">.</span><span class="py">Pattern</span><span class="o">.</span><span class="py">compile</span><span class="o">(</span><span class="s">"test-topic-[0-9]"</span><span class="o">),</span>
  <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">,</span>
  <span class="n">properties</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">stream</span> <span class="k">=</span> <span class="nv">env</span><span class="o">.</span><span class="py">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">)</span>
<span class="o">...</span></code></pre></figure>

  </div>
</div>

<p>In the above example, all topics with names that match the specified regular expression
(starting with <code class="highlighter-rouge">test-topic-</code> and ending with a single digit) will be subscribed by the consumer
when the job starts running.</p>

<p>To allow the consumer to discover dynamically created topics after the job started running,
set a non-negative value for <code class="highlighter-rouge">flink.partition-discovery.interval-millis</code>. This allows
the consumer to discover partitions of new topics with names that also match the specified
pattern.</p>

<h3 id="kafka-consumers-offset-committing-behaviour-configuration">Kafka Consumers Offset Committing Behaviour Configuration</h3>

<p>The Flink Kafka Consumer allows configuring the behaviour of how offsets
are committed back to Kafka brokers. Note that the
Flink Kafka Consumer does not rely on the committed offsets for fault
tolerance guarantees. The committed offsets are only a means to expose
the consumer’s progress for monitoring purposes.</p>

<p>The way to configure offset commit behaviour is different, depending on
whether checkpointing is enabled for the job.</p>

<ul>
  <li>
    <p><em>Checkpointing disabled:</em> if checkpointing is disabled, the Flink Kafka
 Consumer relies on the automatic periodic offset committing capability
 of the internally used Kafka clients. Therefore, to disable or enable offset
 committing, simply set the <code class="highlighter-rouge">enable.auto.commit</code> / <code class="highlighter-rouge">auto.commit.interval.ms</code> keys to appropriate values
 in the provided <code class="highlighter-rouge">Properties</code> configuration.</p>
  </li>
  <li>
    <p><em>Checkpointing enabled:</em> if checkpointing is enabled, the Flink Kafka
 Consumer will commit the offsets stored in the checkpointed states when
 the checkpoints are completed. This ensures that the committed offsets
 in Kafka brokers is consistent with the offsets in the checkpointed states.
 Users can choose to disable or enable offset committing by calling the
 <code class="highlighter-rouge">setCommitOffsetsOnCheckpoints(boolean)</code> method on the consumer (by default,
 the behaviour is <code class="highlighter-rouge">true</code>).
 Note that in this scenario, the automatic periodic offset committing
 settings in <code class="highlighter-rouge">Properties</code> is completely ignored.</p>
  </li>
</ul>

<h3 id="kafka-consumers-and-timestamp-extractionwatermark-emission">Kafka Consumers and Timestamp Extraction/Watermark Emission</h3>

<p>In many scenarios, the timestamp of a record is embedded in the record itself, or the metadata of the <code class="highlighter-rouge">ConsumerRecord</code>.
In addition, users may want to emit watermarks either periodically, or irregularly, e.g. based on
special records in the Kafka stream that contain the current event-time watermark. For these cases, the Flink Kafka
Consumer allows the specification of a <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_time.html">watermark strategy</a>.</p>

<p>You can specify your custom strategy as described
<a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamps_watermarks.html">here</a>, or use one from the
<a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamp_extractors.html">predefined ones</a>.</p>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">);</span>

<span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">myConsumer</span> <span class="o">=</span>
    <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">&lt;&gt;(</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">);</span>
<span class="n">myConsumer</span><span class="o">.</span><span class="na">assignTimestampsAndWatermarks</span><span class="o">(</span>
    <span class="nc">WatermarkStrategy</span><span class="o">.</span>
        <span class="o">.</span><span class="na">forBoundedOutOfOrderness</span><span class="o">(</span><span class="nc">Duration</span><span class="o">.</span><span class="na">ofSeconds</span><span class="o">(</span><span class="mi">20</span><span class="o">)));</span>

<span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">()</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"group.id"</span><span class="o">,</span> <span class="s">"test"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">myConsumer</span> <span class="k">=</span>
    <span class="k">new</span> <span class="nc">FlinkKafkaConsumer</span><span class="o">(</span><span class="s">"topic"</span><span class="o">,</span> <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span> <span class="n">properties</span><span class="o">);</span>
<span class="nv">myConsumer</span><span class="o">.</span><span class="py">assignTimestampsAndWatermarks</span><span class="o">(</span>
    <span class="nc">WatermarkStrategy</span><span class="o">.</span>
        <span class="o">.</span><span class="py">forBoundedOutOfOrderness</span><span class="o">(</span><span class="nv">Duration</span><span class="o">.</span><span class="py">ofSeconds</span><span class="o">(</span><span class="mi">20</span><span class="o">)))</span>

<span class="k">val</span> <span class="nv">stream</span> <span class="k">=</span> <span class="nv">env</span><span class="o">.</span><span class="py">addSource</span><span class="o">(</span><span class="n">myConsumer</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p><strong>Note</strong>: If a watermark assigner depends on records read from Kafka to advance its watermarks
(which is commonly the case), all topics and partitions need to have a continuous stream of records.
Otherwise, the watermarks of the whole application cannot advance and all time-based operations,
such as time windows or functions with timers, cannot make progress. A single idle Kafka partition causes this behavior.
Consider setting appropriate <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamps_watermarks.html#dealing-with-idle-sources">idelness timeouts</a> to mitigate this issue.</p>

<h2 id="kafka-producer">Kafka Producer</h2>

<p>Flink’s Kafka Producer - <code class="highlighter-rouge">FlinkKafkaProducer</code> allows writing a stream of records to one or more Kafka topics.</p>

<p>The constructor accepts the following arguments:</p>

<ol>
  <li>A default output topic where events should be written</li>
  <li>A SerializationSchema / KafkaSerializationSchema for serializing data into Kafka</li>
  <li>Properties for the Kafka client. The following properties are required:
    <ul>
      <li>“bootstrap.servers” (comma separated list of Kafka brokers)</li>
    </ul>
  </li>
  <li>A fault-tolerance semantic</li>
</ol>

<div class="codetabs">
  <div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="nc">DataStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="o">...</span>

<span class="nc">Properties</span> <span class="n">properties</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">properties</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">);</span>

<span class="nc">FlinkKafkaProducer</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">myProducer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaProducer</span><span class="o">&lt;&gt;(</span>
        <span class="s">"my-topic"</span><span class="o">,</span>                  <span class="c1">// target topic</span>
        <span class="k">new</span> <span class="nf">SimpleStringSchema</span><span class="o">(),</span>    <span class="c1">// serialization schema</span>
        <span class="n">properties</span><span class="o">,</span>                  <span class="c1">// producer config</span>
        <span class="nc">FlinkKafkaProducer</span><span class="o">.</span><span class="na">Semantic</span><span class="o">.</span><span class="na">EXACTLY_ONCE</span><span class="o">);</span> <span class="c1">// fault-tolerance</span>

<span class="n">stream</span><span class="o">.</span><span class="na">addSink</span><span class="o">(</span><span class="n">myProducer</span><span class="o">);</span></code></pre></figure>

  </div>
  <div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">stream</span><span class="k">:</span> <span class="kt">DataStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="k">val</span> <span class="nv">properties</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Properties</span>
<span class="nv">properties</span><span class="o">.</span><span class="py">setProperty</span><span class="o">(</span><span class="s">"bootstrap.servers"</span><span class="o">,</span> <span class="s">"localhost:9092"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">myProducer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">FlinkKafkaProducer</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span>
        <span class="s">"my-topic"</span><span class="o">,</span>                  <span class="c1">// target topic</span>
        <span class="k">new</span> <span class="nc">SimpleStringSchema</span><span class="o">(),</span>    <span class="c1">// serialization schema</span>
        <span class="n">properties</span><span class="o">,</span>                  <span class="c1">// producer config</span>
        <span class="nv">FlinkKafkaProducer</span><span class="o">.</span><span class="py">Semantic</span><span class="o">.</span><span class="py">EXACTLY_ONCE</span><span class="o">)</span> <span class="c1">// fault-tolerance</span>

<span class="nv">stream</span><span class="o">.</span><span class="py">addSink</span><span class="o">(</span><span class="n">myProducer</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<h2 id="the-serializationschema">The <code class="highlighter-rouge">SerializationSchema</code></h2>

<p>The Flink Kafka Producer needs to know how to turn Java/Scala objects into binary data.
The <code class="highlighter-rouge">KafkaSerializationSchema</code> allows users to specify such a schema.
The <code class="highlighter-rouge">ProducerRecord&lt;byte[], byte[]&gt; serialize(T element, @Nullable Long timestamp)</code> method gets called for each record, generating a <code class="highlighter-rouge">ProducerRecord</code> that is written to Kafka.</p>

<p>The gives users fine-grained control over how data is written out to Kafka. 
Through the producer record you can:</p>
<ul>
  <li>Set header values</li>
  <li>Define keys for each record</li>
  <li>Specify custom partitioning of data</li>
</ul>

<h3 id="kafka-producers-and-fault-tolerance">Kafka Producers and Fault Tolerance</h3>

<p>With Flink’s checkpointing enabled, the <code class="highlighter-rouge">FlinkKafkaProducer</code> can provide
exactly-once delivery guarantees.</p>

<p>Besides enabling Flink’s checkpointing, you can also choose three different modes of operating
chosen by passing appropriate <code class="highlighter-rouge">semantic</code> parameter to the <code class="highlighter-rouge">FlinkKafkaProducer</code>:</p>

<ul>
  <li><code class="highlighter-rouge">Semantic.NONE</code>: Flink will not guarantee anything. Produced records can be lost or they can
 be duplicated.</li>
  <li><code class="highlighter-rouge">Semantic.AT_LEAST_ONCE</code> (default setting): This guarantees that no records will be lost (although they can be duplicated).</li>
  <li><code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code>: Kafka transactions will be used to provide exactly-once semantic. Whenever you write
 to Kafka using transactions, do not forget about setting desired <code class="highlighter-rouge">isolation.level</code> (<code class="highlighter-rouge">read_committed</code>
 or <code class="highlighter-rouge">read_uncommitted</code> - the latter one is the default value) for any application consuming records
 from Kafka.</li>
</ul>

<h5 id="caveats">Caveats</h5>

<p><code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code> mode relies on the ability to commit transactions
that were started before taking a checkpoint, after recovering from the said checkpoint. If the time
between Flink application crash and completed restart is larger than Kafka’s transaction timeout
there will be data loss (Kafka will automatically abort transactions that exceeded timeout time).
Having this in mind, please configure your transaction timeout appropriately to your expected down
times.</p>

<p>Kafka brokers by default have <code class="highlighter-rouge">transaction.max.timeout.ms</code> set to 15 minutes. This property will
not allow to set transaction timeouts for the producers larger than it’s value.
<code class="highlighter-rouge">FlinkKafkaProducer</code> by default sets the <code class="highlighter-rouge">transaction.timeout.ms</code> property in producer config to
1 hour, thus <code class="highlighter-rouge">transaction.max.timeout.ms</code> should be increased before using the
<code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code> mode.</p>

<p>In <code class="highlighter-rouge">read_committed</code> mode of <code class="highlighter-rouge">KafkaConsumer</code>, any transactions that were not finished
(neither aborted nor completed) will block all reads from the given Kafka topic past any
un-finished transaction. In other words after following sequence of events:</p>

<ol>
  <li>User started <code class="highlighter-rouge">transaction1</code> and written some records using it</li>
  <li>User started <code class="highlighter-rouge">transaction2</code> and written some further records using it</li>
  <li>User committed <code class="highlighter-rouge">transaction2</code></li>
</ol>

<p>Even if records from <code class="highlighter-rouge">transaction2</code> are already committed, they will not be visible to
the consumers until <code class="highlighter-rouge">transaction1</code> is committed or aborted. This has two implications:</p>

<ul>
  <li>First of all, during normal working of Flink applications, user can expect a delay in visibility
 of the records produced into Kafka topics, equal to average time between completed checkpoints.</li>
  <li>Secondly in case of Flink application failure, topics into which this application was writing,
 will be blocked for the readers until the application restarts or the configured transaction 
 timeout time will pass. This remark only applies for the cases when there are multiple
 agents/applications writing to the same Kafka topic.</li>
</ul>

<p><strong>Note</strong>:  <code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code> mode uses a fixed size pool of KafkaProducers
per each <code class="highlighter-rouge">FlinkKafkaProducer</code> instance. One of each of those producers is used per one
checkpoint. If the number of concurrent checkpoints exceeds the pool size, <code class="highlighter-rouge">FlinkKafkaProducer</code>
will throw an exception and will fail the whole application. Please configure max pool size and max
number of concurrent checkpoints accordingly.</p>

<p><strong>Note</strong>: <code class="highlighter-rouge">Semantic.EXACTLY_ONCE</code> takes all possible measures to not leave any lingering transactions
that would block the consumers from reading from Kafka topic more then it is necessary. However in the
event of failure of Flink application before first checkpoint, after restarting such application there
is no information in the system about previous pool sizes. Thus it is unsafe to scale down Flink
application before first checkpoint completes, by factor larger than <code class="highlighter-rouge">FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR</code>.</p>

<h2 id="kafka-connector-metrics">Kafka Connector Metrics</h2>

<p>Flink’s Kafka connectors provide some metrics through Flink’s <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/metrics.html">metrics system</a> to analyze
the behavior of the connector.
The producers export Kafka’s internal metrics through Flink’s metric system for all supported versions.
The Kafka documentation lists all exported metrics in its <a href="http://kafka.apache.org/documentation/#selector_monitoring">documentation</a>.</p>

<p>In addition to these metrics, all consumers expose the <code class="highlighter-rouge">current-offsets</code> and <code class="highlighter-rouge">committed-offsets</code> for each topic partition.
The <code class="highlighter-rouge">current-offsets</code> refers to the current offset in the partition. This refers to the offset of the last element that
we retrieved and emitted successfully. The <code class="highlighter-rouge">committed-offsets</code> is the last committed offset.</p>

<p>The Kafka Consumers in Flink commit the offsets back to the Kafka brokers.
If checkpointing is disabled, offsets are committed periodically.
With checkpointing, the commit happens once all operators in the streaming topology have confirmed that they’ve created a checkpoint of their state. 
This provides users with at-least-once semantics for the offsets committed to Zookeeper or the broker. For offsets checkpointed to Flink, the system 
provides exactly once guarantees.</p>

<p>The offsets committed to ZK or the broker can also be used to track the read progress of the Kafka consumer. The difference between
the committed offset and the most recent offset in each partition is called the <em>consumer lag</em>. If the Flink topology is consuming
the data slower from the topic than new data is added, the lag will increase and the consumer will fall behind.
For large production deployments we recommend monitoring that metric to avoid increasing latency.</p>

<h2 id="enabling-kerberos-authentication">Enabling Kerberos Authentication</h2>

<p>Flink provides first-class support through the Kafka connector to authenticate to a Kafka installation
configured for Kerberos. Simply configure Flink in <code class="highlighter-rouge">flink-conf.yaml</code> to enable Kerberos authentication for Kafka like so:</p>

<ol>
  <li>Configure Kerberos credentials by setting the following -
    <ul>
      <li><code class="highlighter-rouge">security.kerberos.login.use-ticket-cache</code>: By default, this is <code class="highlighter-rouge">true</code> and Flink will attempt to use Kerberos credentials in ticket caches managed by <code class="highlighter-rouge">kinit</code>.
 Note that when using the Kafka connector in Flink jobs deployed on YARN, Kerberos authorization using ticket caches will not work.
 This is also the case when deploying using Mesos, as authorization using ticket cache is not supported for Mesos deployments.</li>
      <li><code class="highlighter-rouge">security.kerberos.login.keytab</code> and <code class="highlighter-rouge">security.kerberos.login.principal</code>: To use Kerberos keytabs instead, set values for both of these properties.</li>
    </ul>
  </li>
  <li>Append <code class="highlighter-rouge">KafkaClient</code> to <code class="highlighter-rouge">security.kerberos.login.contexts</code>: This tells Flink to provide the configured Kerberos credentials to the Kafka login context to be used for Kafka authentication.</li>
</ol>

<p>Once Kerberos-based Flink security is enabled, you can authenticate to Kafka with either the Flink Kafka Consumer or Producer
by simply including the following two settings in the provided properties configuration that is passed to the internal Kafka client:</p>

<ul>
  <li>Set <code class="highlighter-rouge">security.protocol</code> to <code class="highlighter-rouge">SASL_PLAINTEXT</code> (default <code class="highlighter-rouge">NONE</code>): The protocol used to communicate to Kafka brokers.
When using standalone Flink deployment, you can also use <code class="highlighter-rouge">SASL_SSL</code>; please see how to configure the Kafka client for SSL <a href="https://kafka.apache.org/documentation/#security_configclients">here</a>.</li>
  <li>Set <code class="highlighter-rouge">sasl.kerberos.service.name</code> to <code class="highlighter-rouge">kafka</code> (default <code class="highlighter-rouge">kafka</code>): The value for this should match the <code class="highlighter-rouge">sasl.kerberos.service.name</code> used for Kafka broker configurations.
A mismatch in service name between client and server configuration will cause the authentication to fail.</li>
</ul>

<p>For more information on Flink configuration for Kerberos security, please see <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html">here</a>.
You can also find <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/security/security-kerberos.html">here</a> further details on how Flink internally setups Kerberos-based security.</p>

<h2 id="upgrading-to-the-latest-connector-version">Upgrading to the Latest Connector Version</h2>

<p>The generic upgrade steps are outlined in <a href="//ci.apache.org/projects/flink/flink-docs-release-1.12/ops/upgrading.html">upgrading jobs and Flink versions
guide</a>. For Kafka, you additionally need
to follow these steps:</p>

<ul>
  <li>Do not upgrade Flink and the Kafka Connector version at the same time.</li>
  <li>Make sure you have a <code class="highlighter-rouge">group.id</code> configured for your Consumer.</li>
  <li>Set <code class="highlighter-rouge">setCommitOffsetsOnCheckpoints(true)</code> on the consumer so that read
offsets are committed to Kafka. It’s important to do this before stopping and
taking the savepoint. You might have to do a stop/restart cycle on the old
connector version to enable this setting.</li>
  <li>Set <code class="highlighter-rouge">setStartFromGroupOffsets(true)</code> on the consumer so that we get read
offsets from Kafka. This will only take effect when there is no read offset
in Flink state, which is why the next step is very important.</li>
  <li>Change the assigned <code class="highlighter-rouge">uid</code> of your source/sink. This makes sure the new
source/sink doesn’t read state from the old source/sink operators.</li>
  <li>Start the new job with <code class="highlighter-rouge">--allow-non-restored-state</code> because we still have the
state of the previous connector version in the savepoint.</li>
</ul>

<h2 id="troubleshooting">Troubleshooting</h2>

<div class="alert alert-warning">
If you have a problem with Kafka when using Flink, keep in mind that Flink only wraps
<a href="https://kafka.apache.org/documentation/#consumerapi">KafkaConsumer</a> or
<a href="https://kafka.apache.org/documentation/#producerapi">KafkaProducer</a>
and your problem might be independent of Flink and sometimes can be solved by upgrading Kafka brokers,
reconfiguring Kafka brokers or reconfiguring <tt>KafkaConsumer</tt> or <tt>KafkaProducer</tt> in Flink.
Some examples of common problems are listed below.
</div>

<h3 id="data-loss">Data loss</h3>

<p>Depending on your Kafka configuration, even after Kafka acknowledges
writes you can still experience data loss. In particular keep in mind about the following properties
in Kafka config:</p>

<ul>
  <li><code class="highlighter-rouge">acks</code></li>
  <li><code class="highlighter-rouge">log.flush.interval.messages</code></li>
  <li><code class="highlighter-rouge">log.flush.interval.ms</code></li>
  <li><code class="highlighter-rouge">log.flush.*</code></li>
</ul>

<p>Default values for the above options can easily lead to data loss.
Please refer to the Kafka documentation for more explanation.</p>

<h3 id="unknowntopicorpartitionexception">UnknownTopicOrPartitionException</h3>

<p>One possible cause of this error is when a new leader election is taking place,
for example after or during restarting a Kafka broker.
This is a retriable exception, so Flink job should be able to restart and resume normal operation.
It also can be circumvented by changing <code class="highlighter-rouge">retries</code> property in the producer settings.
However this might cause reordering of messages,
which in turn if undesired can be circumvented by setting <code class="highlighter-rouge">max.in.flight.requests.per.connection</code> to 1.</p>

<p><a href="#top" class="top pull-right"><span class="glyphicon glyphicon-chevron-up"></span> Back to top</a></p>



<div class="footer">
  <a href="https://cwiki.apache.org/confluence/display/FLINK/Flink+Translation+Specifications" target="_blank">
    
      Want to contribute translation?
    
  </a>
</div>


        </div>
      </div>
    </div><!-- /.container -->

    <!-- default code tab -->
    <script>var defaultCodeTab = "";</script>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/js/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.1.0/anchor.min.js"></script>
    <script src="//ci.apache.org/projects/flink/flink-docs-release-1.12/page/js/flink.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- Disqus -->
    
  </body>
</html>
